{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:34.873092Z",
     "iopub.status.busy": "2022-04-15T06:45:34.872633Z",
     "iopub.status.idle": "2022-04-15T06:45:35.634846Z",
     "shell.execute_reply": "2022-04-15T06:45:35.634090Z",
     "shell.execute_reply.started": "2022-04-15T06:45:34.873061Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991230</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.764677</td>\n",
       "      <td>0.618571</td>\n",
       "      <td>0.379632</td>\n",
       "      <td>0.190822</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.025995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971482</td>\n",
       "      <td>0.928969</td>\n",
       "      <td>0.572933</td>\n",
       "      <td>0.178457</td>\n",
       "      <td>0.122962</td>\n",
       "      <td>0.132360</td>\n",
       "      <td>0.094392</td>\n",
       "      <td>0.089575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959149</td>\n",
       "      <td>0.701378</td>\n",
       "      <td>0.231778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080698</td>\n",
       "      <td>0.128376</td>\n",
       "      <td>0.187448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975795</td>\n",
       "      <td>0.934088</td>\n",
       "      <td>0.659637</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>0.237116</td>\n",
       "      <td>0.281445</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055816</td>\n",
       "      <td>0.261294</td>\n",
       "      <td>0.359847</td>\n",
       "      <td>0.433143</td>\n",
       "      <td>0.453698</td>\n",
       "      <td>0.499004</td>\n",
       "      <td>0.542796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677705</td>\n",
       "      <td>0.222392</td>\n",
       "      <td>0.257158</td>\n",
       "      <td>0.204690</td>\n",
       "      <td>0.054665</td>\n",
       "      <td>0.026152</td>\n",
       "      <td>0.118181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.926857</td>\n",
       "      <td>0.906347</td>\n",
       "      <td>0.636993</td>\n",
       "      <td>0.415038</td>\n",
       "      <td>0.374745</td>\n",
       "      <td>0.382581</td>\n",
       "      <td>0.358943</td>\n",
       "      <td>0.341359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.925835</td>\n",
       "      <td>0.587384</td>\n",
       "      <td>0.633226</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.639283</td>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.599155</td>\n",
       "      <td>0.517632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994762</td>\n",
       "      <td>0.829702</td>\n",
       "      <td>0.458193</td>\n",
       "      <td>0.264162</td>\n",
       "      <td>0.240228</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.189291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>0.916477</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.385431</td>\n",
       "      <td>0.361067</td>\n",
       "      <td>0.332708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label         0         1         2         3         4  \\\n",
       "0          0    0.0  0.991230  0.943533  0.764677  0.618571  0.379632   \n",
       "1          1    0.0  0.971482  0.928969  0.572933  0.178457  0.122962   \n",
       "2          2    2.0  1.000000  0.959149  0.701378  0.231778  0.000000   \n",
       "3          3    0.0  0.975795  0.934088  0.659637  0.249921  0.237116   \n",
       "4          4    2.0  0.000000  0.055816  0.261294  0.359847  0.433143   \n",
       "...      ...    ...       ...       ...       ...       ...       ...   \n",
       "99995  99995    0.0  1.000000  0.677705  0.222392  0.257158  0.204690   \n",
       "99996  99996    2.0  0.926857  0.906347  0.636993  0.415038  0.374745   \n",
       "99997  99997    3.0  0.925835  0.587384  0.633226  0.632353  0.639283   \n",
       "99998  99998    2.0  1.000000  0.994762  0.829702  0.458193  0.264162   \n",
       "99999  99999    0.0  0.925999  0.916477  0.404290  0.000000  0.263034   \n",
       "\n",
       "              5         6         7  ...  195  196  197  198  199  200  201  \\\n",
       "0      0.190822  0.040237  0.025995  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.132360  0.094392  0.089575  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.080698  0.128376  0.187448  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.281445  0.249921  0.249921  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.453698  0.499004  0.542796  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "99995  0.054665  0.026152  0.118181  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99996  0.382581  0.358943  0.341359  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99997  0.614292  0.599155  0.517632  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99998  0.240228  0.213766  0.189291  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99999  0.385431  0.361067  0.332708  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       202  203  204  \n",
       "0      0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "99995  0.0  0.0  0.0  \n",
       "99996  0.0  0.0  0.0  \n",
       "99997  0.0  0.0  0.0  \n",
       "99998  0.0  0.0  0.0  \n",
       "99999  0.0  0.0  0.0  \n",
       "\n",
       "[100000 rows x 207 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# 数据加载\n",
    "with open('./train1.pkl', 'rb') as file:\n",
    "    train1 = pickle.load(file)\n",
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:35.636811Z",
     "iopub.status.busy": "2022-04-15T06:45:35.636388Z",
     "iopub.status.idle": "2022-04-15T06:45:35.740854Z",
     "shell.execute_reply": "2022-04-15T06:45:35.740175Z",
     "shell.execute_reply.started": "2022-04-15T06:45:35.636782Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.991571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631816</td>\n",
       "      <td>0.136230</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.102707</td>\n",
       "      <td>0.120854</td>\n",
       "      <td>0.123428</td>\n",
       "      <td>0.107915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0.607553</td>\n",
       "      <td>0.541708</td>\n",
       "      <td>0.340694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090708</td>\n",
       "      <td>0.164924</td>\n",
       "      <td>0.195034</td>\n",
       "      <td>0.168838</td>\n",
       "      <td>0.198844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38996</td>\n",
       "      <td>0.386932</td>\n",
       "      <td>0.367251</td>\n",
       "      <td>0.363917</td>\n",
       "      <td>0.360574</td>\n",
       "      <td>0.357245</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.350565</td>\n",
       "      <td>0.363874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>0.975273</td>\n",
       "      <td>0.671097</td>\n",
       "      <td>0.686759</td>\n",
       "      <td>0.708482</td>\n",
       "      <td>0.718660</td>\n",
       "      <td>0.716763</td>\n",
       "      <td>0.720548</td>\n",
       "      <td>0.701656</td>\n",
       "      <td>0.596579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>0.995635</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>0.521096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221770</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.490399</td>\n",
       "      <td>0.527158</td>\n",
       "      <td>0.518056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887949</td>\n",
       "      <td>0.745565</td>\n",
       "      <td>0.531720</td>\n",
       "      <td>0.380320</td>\n",
       "      <td>0.224631</td>\n",
       "      <td>0.091148</td>\n",
       "      <td>0.057639</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>119995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833028</td>\n",
       "      <td>0.634047</td>\n",
       "      <td>0.639118</td>\n",
       "      <td>0.623852</td>\n",
       "      <td>0.598042</td>\n",
       "      <td>0.613583</td>\n",
       "      <td>0.623852</td>\n",
       "      <td>0.628958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>119996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825971</td>\n",
       "      <td>0.452105</td>\n",
       "      <td>0.082227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137119</td>\n",
       "      <td>0.201082</td>\n",
       "      <td>0.165688</td>\n",
       "      <td>0.158125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>119997</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>0.916261</td>\n",
       "      <td>0.667525</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.255330</td>\n",
       "      <td>0.197383</td>\n",
       "      <td>0.173536</td>\n",
       "      <td>0.141934</td>\n",
       "      <td>0.134542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>119998</td>\n",
       "      <td>0.927669</td>\n",
       "      <td>0.677190</td>\n",
       "      <td>0.242906</td>\n",
       "      <td>0.055362</td>\n",
       "      <td>0.102120</td>\n",
       "      <td>0.072236</td>\n",
       "      <td>0.021010</td>\n",
       "      <td>0.038288</td>\n",
       "      <td>0.048557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>119999</td>\n",
       "      <td>0.665321</td>\n",
       "      <td>0.527064</td>\n",
       "      <td>0.516663</td>\n",
       "      <td>0.376442</td>\n",
       "      <td>0.489262</td>\n",
       "      <td>0.480725</td>\n",
       "      <td>0.459160</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>0.469983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         0         1         2         3         4         5  \\\n",
       "0      100000  0.991571  1.000000  0.631816  0.136230  0.041420  0.102707   \n",
       "1      100001  0.607553  0.541708  0.340694  0.000000  0.090708  0.164924   \n",
       "2      100002  0.975273  0.671097  0.686759  0.708482  0.718660  0.716763   \n",
       "3      100003  0.995635  0.917025  0.521096  0.000000  0.221770  0.404100   \n",
       "4      100004  1.000000  0.887949  0.745565  0.531720  0.380320  0.224631   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  119995  1.000000  0.833028  0.634047  0.639118  0.623852  0.598042   \n",
       "19996  119996  1.000000  0.825971  0.452105  0.082227  0.000000  0.137119   \n",
       "19997  119997  0.951745  0.916261  0.667525  0.351985  0.255330  0.197383   \n",
       "19998  119998  0.927669  0.677190  0.242906  0.055362  0.102120  0.072236   \n",
       "19999  119999  0.665321  0.527064  0.516663  0.376442  0.489262  0.480725   \n",
       "\n",
       "              6         7         8  ...      195       196       197  \\\n",
       "0      0.120854  0.123428  0.107915  ...  0.00000  0.000000  0.000000   \n",
       "1      0.195034  0.168838  0.198844  ...  0.38996  0.386932  0.367251   \n",
       "2      0.720548  0.701656  0.596579  ...  0.00000  0.000000  0.000000   \n",
       "3      0.490399  0.527158  0.518056  ...  0.00000  0.000000  0.000000   \n",
       "4      0.091148  0.057639  0.003915  ...  0.00000  0.000000  0.000000   \n",
       "...         ...       ...       ...  ...      ...       ...       ...   \n",
       "19995  0.613583  0.623852  0.628958  ...  0.00000  0.000000  0.000000   \n",
       "19996  0.201082  0.165688  0.158125  ...  0.00000  0.000000  0.000000   \n",
       "19997  0.173536  0.141934  0.134542  ...  0.00000  0.000000  0.000000   \n",
       "19998  0.021010  0.038288  0.048557  ...  0.00000  0.000000  0.000000   \n",
       "19999  0.459160  0.482864  0.469983  ...  0.00000  0.000000  0.000000   \n",
       "\n",
       "            198       199       200       201       202       203       204  \n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1      0.363917  0.360574  0.357245  0.350575  0.350575  0.350565  0.363874  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "19995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19997  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19998  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "19999  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[20000 rows x 206 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据加载\n",
    "with open('./test1.pkl', 'rb') as file:\n",
    "    test1 = pickle.load(file)\n",
    "test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构造-paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:35.742407Z",
     "iopub.status.busy": "2022-04-15T06:45:35.741898Z",
     "iopub.status.idle": "2022-04-15T06:45:36.960260Z",
     "shell.execute_reply": "2022-04-15T06:45:36.959360Z",
     "shell.execute_reply.started": "2022-04-15T06:45:35.742378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:36.962672Z",
     "iopub.status.busy": "2022-04-15T06:45:36.962338Z",
     "iopub.status.idle": "2022-04-15T06:45:36.969406Z",
     "shell.execute_reply": "2022-04-15T06:45:36.968671Z",
     "shell.execute_reply.started": "2022-04-15T06:45:36.962643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 自定义DataSet\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "    def __init__(self, data, is_train):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.is_train = is_train\n",
    "        # 训练集\n",
    "        \"\"\"if self.is_train:\n",
    "            train_x = data.drop(['id','label'], axis=1)\n",
    "            train_y = data['label']\n",
    "            self.data_x = paddle.to_tensor(train_x.values, dtype='float32')\n",
    "            self.data_y = paddle.to_tensor(train_y.values, dtype='int64')\n",
    "        else:\n",
    "            test_x = data.drop(['id'], axis=1)\n",
    "            self.data_x=paddle.to_tensor(test_x.values, dtype='float32')\"\"\"\n",
    "        self.data = paddle.to_tensor(data, dtype='float32')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据）\n",
    "        \"\"\"\n",
    "        tmp = self.data[index]\n",
    "        if self.is_train:\n",
    "            data = tmp[:205]\n",
    "            label = tmp[-1:][:,-1]\n",
    "            return data, label\n",
    "        else:\n",
    "            data = tmp[:205]\n",
    "            return [data]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现__len__方法，返回数据集总数目\n",
    "        \"\"\"\n",
    "        #return len(self.data_x)\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:36.970545Z",
     "iopub.status.busy": "2022-04-15T06:45:36.970210Z",
     "iopub.status.idle": "2022-04-15T06:45:36.984615Z",
     "shell.execute_reply": "2022-04-15T06:45:36.983955Z",
     "shell.execute_reply.started": "2022-04-15T06:45:36.970494Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTM in module paddle.nn.layer.rnn:\n",
      "\n",
      "class LSTM(RNNBase)\n",
      " |  LSTM(input_size, hidden_size, num_layers=1, direction='forward', time_major=False, dropout=0.0, weight_ih_attr=None, weight_hh_attr=None, bias_ih_attr=None, bias_hh_attr=None, name=None)\n",
      " |  \n",
      " |  Multilayer LSTM. It takes a sequence and an initial state as inputs, and \n",
      " |  returns the output sequences and the final states.\n",
      " |  \n",
      " |  Each layer inside the LSTM maps the input sequences and initial states \n",
      " |  to the output sequences and final states in the following manner: at each \n",
      " |  step, it takes step inputs(:math:`x_{t}`) and previous \n",
      " |  states(:math:`h_{t-1}, c_{t-1}`) as inputs, and returns step \n",
      " |  outputs(:math:`y_{t}`) and new states(:math:`h_{t}, c_{t}`).\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      i_{t} & = \\sigma(W_{ii}x_{t} + b_{ii} + W_{hi}h_{t-1} + b_{hi})\n",
      " |  \n",
      " |      f_{t} & = \\sigma(W_{if}x_{t} + b_{if} + W_{hf}h_{t-1} + b_{hf})\n",
      " |  \n",
      " |      o_{t} & = \\sigma(W_{io}x_{t} + b_{io} + W_{ho}h_{t-1} + b_{ho})\n",
      " |  \n",
      " |      \\widetilde{c}_{t} & = \\tanh (W_{ig}x_{t} + b_{ig} + W_{hg}h_{t-1} + b_{hg})\n",
      " |  \n",
      " |      c_{t} & = f_{t} * c_{t-1} + i_{t} * \\widetilde{c}_{t}\n",
      " |  \n",
      " |      h_{t} & = o_{t} * \\tanh(c_{t})\n",
      " |  \n",
      " |      y_{t} & = h_{t}\n",
      " |  \n",
      " |  where :math:`\\sigma` is the sigmoid fucntion, and * is the elemetwise \n",
      " |  multiplication operator.\n",
      " |  \n",
      " |  Using key word arguments to construct is recommended.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      input_size (int): The input size for the first layer's cell.\n",
      " |      hidden_size (int): The hidden size for each layer's cell.\n",
      " |      num_layers (int, optional): Number of layers. Defaults to 1.\n",
      " |      direction (str, optional): The direction of the network. It can be \"forward\"\n",
      " |          or \"bidirect\"(or \"bidirectional\"). When \"bidirect\", the way to merge\n",
      " |          outputs of forward and backward is concatenating. Defaults to \"forward\".\n",
      " |      time_major (bool, optional): Whether the first dimension of the input \n",
      " |          means the time steps. Defaults to False.\n",
      " |      dropout (float, optional): The droput probability. Dropout is applied \n",
      " |          to the input of each layer except for the first layer. Defaults to 0.\n",
      " |      weight_ih_attr (ParamAttr, optional): The parameter attribute for \n",
      " |          `weight_ih` of each cell. Default: None.\n",
      " |      weight_hh_attr (ParamAttr, optional): The parameter attribute for \n",
      " |          `weight_hh` of each cell. Default: None.\n",
      " |      bias_ih_attr (ParamAttr, optional): The parameter attribute for the \n",
      " |          `bias_ih` of each cells. Default: None.\n",
      " |      bias_hh_attr (ParamAttr, optional): The parameter attribute for the \n",
      " |          `bias_hh` of each cells. Default: None.\n",
      " |      name (str, optional): Name for the operation (optional, default is \n",
      " |          None). For more information, please refer to :ref:`api_guide_Name`.\n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **inputs** (Tensor): the input sequence. If `time_major` is True, the shape is `[time_steps, batch_size, input_size]`, else, the shape is `[batch_size, time_steps, hidden_size]`.\n",
      " |      - **initial_states** (list|tuple, optional): the initial state, a list/tuple of (h, c), the shape of each is `[num_layers * num_directions, batch_size, hidden_size]`. If initial_state is not given, zero initial states are used.\n",
      " |      - **sequence_length** (Tensor, optional): shape `[batch_size]`, dtype: int64 or int32. The valid lengths of input sequences. Defaults to None. If `sequence_length` is not None, the inputs are treated as padded sequences. In each input sequence, elements whos time step index are not less than the valid length are treated as paddings.\n",
      " |  \n",
      " |  Returns:\n",
      " |  \n",
      " |      - **outputs** (Tensor): the output sequence. If `time_major` is True, the shape is `[time_steps, batch_size, num_directions * hidden_size]`, If `time_major` is False, the shape is `[batch_size, time_steps, num_directions * hidden_size]`. Note that `num_directions` is 2 if direction is \"bidirectional\" else 1.\n",
      " |      \n",
      " |      - **final_states** (tuple): the final state, a tuple of two tensors, h and c. The shape of each is `[num_layers * num_directions, batch_size, hidden_size]`. Note that `num_directions` is 2 if direction is \"bidirectional\" (the index of forward states are 0, 2, 4, 6... and the index of backward states are 1, 3, 5, 7...), else 1.\n",
      " |  \n",
      " |  Variables:\n",
      " |      - **weight_ih_l[k]**: the learnable input-hidden weights of the k-th layer. If `k = 0`, the shape is `[hidden_size, input_size]`. Otherwise, the shape is `[hidden_size, num_directions * hidden_size]`.\n",
      " |      - **weight_hh_l[k]**: the learnable hidden-hidden weights of the k-th layer, with shape `[hidden_size, hidden_size]`.\n",
      " |      - **bias_ih_l[k]**: the learnable input-hidden bias of the k-th layer, with shape `[hidden_size]`.\n",
      " |      - **bias_hh_l[k]**: the learnable hidden-hidden bias of the k-th layer, swith shape `[hidden_size]`.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          import paddle\n",
      " |  \n",
      " |          rnn = paddle.nn.LSTM(16, 32, 2)\n",
      " |  \n",
      " |          x = paddle.randn((4, 23, 16))\n",
      " |          prev_h = paddle.randn((2, 4, 32))\n",
      " |          prev_c = paddle.randn((2, 4, 32))\n",
      " |          y, (h, c) = rnn(x, (prev_h, prev_c))\n",
      " |  \n",
      " |          print(y.shape)\n",
      " |          print(h.shape)\n",
      " |          print(c.shape)\n",
      " |  \n",
      " |          #[4,23,32]\n",
      " |          #[2,4,32]\n",
      " |          #[2,4,32]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNNBase\n",
      " |      paddle.fluid.dygraph.container.LayerList\n",
      " |      paddle.fluid.dygraph.layers.Layer\n",
      " |      paddle.fluid.core_avx.Layer\n",
      " |      pybind11_builtins.pybind11_object\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_size, hidden_size, num_layers=1, direction='forward', time_major=False, dropout=0.0, weight_ih_attr=None, weight_hh_attr=None, bias_ih_attr=None, bias_hh_attr=None, name=None)\n",
      " |      __init__(self: paddle.fluid.core_avx.Layer) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Extra representation of this layer, you can have custom implementation\n",
      " |      of your own layer.\n",
      " |  \n",
      " |  flatten_parameters(self)\n",
      " |      Resets parameter data pointer to address in continuous memory block for\n",
      " |      cudnn usage.\n",
      " |  \n",
      " |  forward(self, inputs, initial_states=None, sequence_length=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          *inputs(tuple): unpacked tuple arguments\n",
      " |          **kwargs(dict): unpacked dict arguments\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paddle.fluid.dygraph.container.LayerList:\n",
      " |  \n",
      " |  __delitem__(self, idx)\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setitem__(self, idx, sublayer)\n",
      " |  \n",
      " |  append(self, sublayer)\n",
      " |      Appends a sublayer to the end of the list.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          sublayer (Layer): sublayer to append\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])\n",
      " |              another = paddle.nn.Linear(10, 10)\n",
      " |              linears.append(another)\n",
      " |              print(len(linears))  # 11\n",
      " |  \n",
      " |  extend(self, sublayers)\n",
      " |      Appends sublayers to the end of the list.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          sublayers (iterable of Layer): iterable of sublayers to append\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])\n",
      " |              another_list = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(5)])\n",
      " |              linears.extend(another_list)\n",
      " |              print(len(linears))  # 15\n",
      " |              print(another_list[0] is linears[10])  # True\n",
      " |  \n",
      " |  insert(self, index, sublayer)\n",
      " |      Insert a sublayer before a given index in the list.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          index (int): index to insert.\n",
      " |          sublayer (Layer): sublayer to insert\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])\n",
      " |              another = paddle.nn.Linear(10, 10)\n",
      " |              linears.insert(3, another)\n",
      " |              print(linears[3] is another)  # True\n",
      " |              another = paddle.nn.Linear(10, 10)\n",
      " |              linears.insert(-1, another)\n",
      " |              print(linears[-2] is another) # True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from paddle.fluid.dygraph.layers.Layer:\n",
      " |  \n",
      " |  __call__(self, *inputs, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Return a list. Get all parameters, buffers(non-parameter tensors), sublayers, method and attr of Layer.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |              import paddle\n",
      " |              import numpy as np\n",
      " |      \n",
      " |              class Mylayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(Mylayer, self).__init__()\n",
      " |                      self.linear1 = paddle.nn.Linear(10, 10)\n",
      " |                      self.linear2 = paddle.nn.Linear(5, 5)\n",
      " |                      self.conv2d = paddle.nn.Conv2D(3, 2, 3)\n",
      " |                      self.embedding = paddle.nn.Embedding(128, 16)\n",
      " |                      self.h_0 = paddle.to_tensor(np.zeros([10, 10]).astype('float32'))\n",
      " |      \n",
      " |              mylayer = Mylayer()\n",
      " |              print(dir(mylayer))\n",
      " |              # only parts are shown, because of list have too much content\n",
      " |              # ['__call__', '__class__',  ... , 'conv2d', 'embedding', 'h_0', 'linear1', 'linear2', ... , 'sublayers', 'train']\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_parameter(self, name, parameter)\n",
      " |      Adds a Parameter instance.\n",
      " |      \n",
      " |      Added parameter can be accessed by self.name\n",
      " |      \n",
      " |      Parameters:\n",
      " |          name(str): name of this sublayer.\n",
      " |          parameter(Parameter): an instance of Parameter.\n",
      " |      Returns:\n",
      " |          Parameter: the parameter passed in.\n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(MyLayer, self).__init__()\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |                      w_tmp = self.create_parameter([1,1])\n",
      " |                      self.add_parameter(\"w_tmp\", w_tmp)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      return self._linear(input)\n",
      " |      \n",
      " |              mylayer = MyLayer()\n",
      " |              for name, param in mylayer.named_parameters():\n",
      " |                  print(name, param)      # will print w_tmp,_linear.weight,_linear.bias\n",
      " |  \n",
      " |  add_sublayer(self, name, sublayer)\n",
      " |      Adds a sub Layer instance.\n",
      " |      \n",
      " |      Added sublayer can be accessed by self.name\n",
      " |      \n",
      " |      Parameters:\n",
      " |          name(str): name of this sublayer.\n",
      " |          sublayer(Layer): an instance of Layer.\n",
      " |      Returns:\n",
      " |          Layer: the sublayer passed in.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MySequential(paddle.nn.Layer):\n",
      " |                  def __init__(self, *layers):\n",
      " |                      super(MySequential, self).__init__()\n",
      " |                      if len(layers) > 0 and isinstance(layers[0], tuple):\n",
      " |                          for name, layer in layers:\n",
      " |                              self.add_sublayer(name, layer)\n",
      " |                      else:\n",
      " |                          for idx, layer in enumerate(layers):\n",
      " |                              self.add_sublayer(str(idx), layer)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      for layer in self._sub_layers.values():\n",
      " |                          input = layer(input)\n",
      " |                      return input\n",
      " |      \n",
      " |              fc1 = paddle.nn.Linear(10, 3)\n",
      " |              fc2 = paddle.nn.Linear(3, 10, bias_attr=False)\n",
      " |              model = MySequential(fc1, fc2)\n",
      " |              for prefix, layer in model.named_sublayers():\n",
      " |                  print(prefix, layer)\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every sublayer (as returned by ``.sublayers()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          fn (function): a function to be applied to each sublayer\n",
      " |      \n",
      " |      Returns:\n",
      " |          Layer: self\n",
      " |      \n",
      " |      Example::\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |            import paddle\n",
      " |            import paddle.nn as nn\n",
      " |      \n",
      " |            net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |      \n",
      " |            def init_weights(layer):\n",
      " |                if type(layer) == nn.Linear:\n",
      " |                    print('before init weight:', layer.weight.numpy())\n",
      " |                    new_weight = paddle.full(shape=layer.weight.shape, dtype=layer.weight.dtype, fill_value=0.9)\n",
      " |                    layer.weight.set_value(new_weight)\n",
      " |                    print('after init weight:', layer.weight.numpy())\n",
      " |      \n",
      " |            net.apply(init_weights)\n",
      " |      \n",
      " |            print(net.state_dict())\n",
      " |  \n",
      " |  backward(self, *inputs)\n",
      " |  \n",
      " |  buffers(self, include_sublayers=True)\n",
      " |      Returns a list of all buffers from current layer and its sub-layers.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          include_sublayers(bool, optional): Whether include the buffers of sublayers. If True, also include the buffers from sublayers. Default: True\n",
      " |      \n",
      " |      Returns:\n",
      " |          list of Tensor : a list of buffers.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import numpy as np\n",
      " |              import paddle\n",
      " |      \n",
      " |              linear = paddle.nn.Linear(10, 3)\n",
      " |              value = np.array([0]).astype(\"float32\")\n",
      " |              buffer = paddle.to_tensor(value)\n",
      " |              linear.register_buffer(\"buf_name\", buffer, persistable=True)\n",
      " |      \n",
      " |              print(linear.buffers())     # == print([linear.buf_name])\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children layers.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Layer: a child layer\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              linear1 = paddle.nn.Linear(10, 3)\n",
      " |              linear2 = paddle.nn.Linear(3, 10, bias_attr=False)\n",
      " |              model = paddle.nn.Sequential(linear1, linear2)\n",
      " |      \n",
      " |              layer_list = list(model.children())\n",
      " |      \n",
      " |              print(layer_list)   # [<paddle.nn.layer.common.Linear object at 0x7f7b8113f830>, <paddle.nn.layer.common.Linear object at 0x7f7b8113f950>]\n",
      " |  \n",
      " |  clear_gradients(self)\n",
      " |      Clear the gradients of all parameters for this layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |              import numpy as np\n",
      " |      \n",
      " |              value = np.arange(26).reshape(2, 13).astype(\"float32\")\n",
      " |              a = paddle.to_tensor(value)\n",
      " |              linear = paddle.nn.Linear(13, 5)\n",
      " |              adam = paddle.optimizer.Adam(learning_rate=0.01,\n",
      " |                                          parameters=linear.parameters())\n",
      " |              out = linear(a)\n",
      " |              out.backward()\n",
      " |              adam.step()\n",
      " |              linear.clear_gradients()\n",
      " |  \n",
      " |  create_parameter(self, shape, attr=None, dtype=None, is_bias=False, default_initializer=None)\n",
      " |      Create parameters for this layer.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          shape(list): Shape of the parameter.\n",
      " |          attr(ParamAttr, optional): Parameter attribute of weight. Please refer to :ref:`api_paddle_ParamAttr`. Default: None.\n",
      " |          dtype(str, optional): Data type of this parameter.\n",
      " |              If set str, it can be \"bool\",  \"float16\", \"float32\", \"float64\",\n",
      " |              \"int8\", \"int16\", \"int32\", \"int64\", \"uint8\" or \"uint16\". Default: \"float32\".\n",
      " |          is_bias(bool, optional): if this is a bias parameter. Default: False.\n",
      " |          default_initializer(Initializer, optional): the default initializer for this parameter.\n",
      " |              If set None, default initializer will be set to paddle.nn.initializer.Xavier and paddle.nn.initializer.Constant\n",
      " |              for non-bias and bias parameter, respectively. Default: None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :Tensor, created parameter.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(MyLayer, self).__init__()\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |                      w_tmp = self.create_parameter([1,1])\n",
      " |                      self.add_parameter(\"w_tmp\", w_tmp)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      return self._linear(input)\n",
      " |      \n",
      " |              mylayer = MyLayer()\n",
      " |              for name, param in mylayer.named_parameters():\n",
      " |                  print(name, param)      # will print w_tmp,_linear.weight,_linear.bias\n",
      " |  \n",
      " |  create_tensor(self, name=None, persistable=None, dtype=None)\n",
      " |      Create Tensor for this layer.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          name(str, optional): name of the tensor. Please refer to :ref:`api_guide_Name` . Default: None\n",
      " |          persistable(bool, optional): if set this tensor persistable. Default: False\n",
      " |          dtype(str, optional): data type of this parameter.\n",
      " |              If set str, it can be \"bool\",  \"float16\", \"float32\", \"float64\",\n",
      " |              \"int8\", \"int16\", \"int32\", \"int64\", \"uint8\" or \"uint16\".\n",
      " |              If set None, it will be \"float32\". Default: None\n",
      " |      \n",
      " |      Returns:\n",
      " |          Tensor, created Tensor.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLinear(paddle.nn.Layer):\n",
      " |                  def __init__(self,\n",
      " |                              in_features,\n",
      " |                              out_features):\n",
      " |                      super(MyLinear, self).__init__()\n",
      " |                      self.linear = paddle.nn.Linear( 10, 10)\n",
      " |      \n",
      " |                      self.back_var = self.create_tensor(name = \"linear_tmp_0\", dtype=self._dtype)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      out = self.linear(input)\n",
      " |                      paddle.assign( out, self.back_var)\n",
      " |      \n",
      " |                      return out\n",
      " |  \n",
      " |  create_variable(self, name=None, persistable=None, dtype=None)\n",
      " |      Create Tensor for this layer.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          name(str, optional): name of the tensor. Please refer to :ref:`api_guide_Name` . Default: None\n",
      " |      \n",
      " |          persistable(bool, optional): if set this tensor persistable. Default: False\n",
      " |      \n",
      " |          dtype(str, optional): data type of this parameter. If set str, it can be \"bool\", \"float16\", \"float32\", \"float64\",\"int8\", \"int16\", \"int32\", \"int64\", \"uint8\" or \"uint16\". If set None, it will be \"float32\". Default: None\n",
      " |      \n",
      " |      Returns:\n",
      " |          Tensor, created Tensor.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLinear(paddle.nn.Layer):\n",
      " |                  def __init__(self,\n",
      " |                              in_features,\n",
      " |                              out_features):\n",
      " |                      super(MyLinear, self).__init__()\n",
      " |                      self.linear = paddle.nn.Linear( 10, 10)\n",
      " |      \n",
      " |                      self.back_var = self.create_variable(name = \"linear_tmp_0\", dtype=self._dtype)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      out = self.linear(input)\n",
      " |                      paddle.assign( out, self.back_var)\n",
      " |      \n",
      " |                      return out\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets this Layer and all its sublayers to evaluation mode.\n",
      " |      This only effects certain modules like `Dropout` and `BatchNorm`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Example::\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(MyLayer, self).__init__()\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |                      self._dropout = paddle.nn.Dropout(p=0.5)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      temp = self._linear(input)\n",
      " |                      temp = self._dropout(temp)\n",
      " |                      return temp\n",
      " |      \n",
      " |              x = paddle.randn([10, 1], 'float32')\n",
      " |              mylayer = MyLayer()\n",
      " |              mylayer.eval()  # set mylayer._dropout to eval mode\n",
      " |              out = mylayer(x)\n",
      " |              print(out)\n",
      " |  \n",
      " |  full_name(self)\n",
      " |      Full name for this layer, composed by name_scope + \"/\" + MyLayer.__class__.__name__\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: full name of this layer.\n",
      " |      \n",
      " |      Example::\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class LinearNet(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(LinearNet, self).__init__(name_scope = \"demo_linear_net\")\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |      \n",
      " |                  def forward(self, x):\n",
      " |                      return self._linear(x)\n",
      " |      \n",
      " |              linear_net = LinearNet()\n",
      " |              print(linear_net.full_name())   # demo_linear_net_0\n",
      " |  \n",
      " |  load_dict = set_state_dict(self, state_dict, use_structured_name=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', include_sublayers=True)\n",
      " |      Returns an iterator over all buffers in the Layer, yielding tuple of name and Tensor.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prefix(str, optional): Prefix to prepend to all buffer names. Default: ''.\n",
      " |          include_sublayers(bool, optional): Whether include the buffers of sublayers.\n",
      " |              If True, also include the named buffers from sublayers. Default: True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Tensor): Tuple of name and tensor\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import numpy as np\n",
      " |              import paddle\n",
      " |      \n",
      " |              fc1 = paddle.nn.Linear(10, 3)\n",
      " |              buffer1 = paddle.to_tensor(np.array([0]).astype(\"float32\"))\n",
      " |              # register a tensor as buffer by specific `persistable`\n",
      " |              fc1.register_buffer(\"buf_name_1\", buffer1, persistable=True)\n",
      " |      \n",
      " |              fc2 = paddle.nn.Linear(3, 10)\n",
      " |              buffer2 = paddle.to_tensor(np.array([1]).astype(\"float32\"))\n",
      " |              # register a buffer by assigning an attribute with Tensor.\n",
      " |              # The `persistable` can only be False by this way.\n",
      " |              fc2.buf_name_2 = buffer2\n",
      " |      \n",
      " |              model = paddle.nn.Sequential(fc1, fc2)\n",
      " |      \n",
      " |              # get all named buffers\n",
      " |              for name, buffer in model.named_buffers():\n",
      " |                  print(name, buffer)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children layers, yielding both\n",
      " |      the name of the layer as well as the layer itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Layer): Tuple containing a name and child layer\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              linear1 = paddle.nn.Linear(10, 3)\n",
      " |              linear2 = paddle.nn.Linear(3, 10, bias_attr=False)\n",
      " |              model = paddle.nn.Sequential(linear1, linear2)\n",
      " |              for prefix, layer in model.named_children():\n",
      " |                  print(prefix, layer)\n",
      " |                  # ('0', <paddle.nn.layer.common.Linear object at 0x7fb61ed85830>)\n",
      " |                  # ('1', <paddle.nn.layer.common.Linear object at 0x7fb61ed85950>)\n",
      " |  \n",
      " |  named_parameters(self, prefix='', include_sublayers=True)\n",
      " |      Returns an iterator over all parameters in the Layer, yielding tuple of name and parameter.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prefix(str, optional): Prefix to prepend to all parameter names. Default: ''.\n",
      " |          include_sublayers(bool, optional): Whether include the parameters of sublayers.\n",
      " |              If True, also include the named parameters from sublayers. Default: True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple of name and Parameter\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              fc1 = paddle.nn.Linear(10, 3)\n",
      " |              fc2 = paddle.nn.Linear(3, 10, bias_attr=False)\n",
      " |              model = paddle.nn.Sequential(fc1, fc2)\n",
      " |              for name, param in model.named_parameters():\n",
      " |                  print(name, param)\n",
      " |  \n",
      " |  named_sublayers(self, prefix='', include_self=False, layers_set=None)\n",
      " |      Returns an iterator over all sublayers in the Layer, yielding tuple of name and sublayer.\n",
      " |      The duplicate sublayer will only be yielded once.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prefix(str, optional): Prefix to prepend to all parameter names. Default: ''.\n",
      " |          include_self(bool, optional): Whether include the Layer itself. Default: False.\n",
      " |          layers_set(set, optional): The set to record duplicate sublayers. Default: None.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Layer): Tuple of name and Layer\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              fc1 = paddle.nn.Linear(10, 3)\n",
      " |              fc2 = paddle.nn.Linear(3, 10, bias_attr=False)\n",
      " |              model = paddle.nn.Sequential(fc1, fc2)\n",
      " |              for prefix, layer in model.named_sublayers():\n",
      " |                  print(prefix, layer)\n",
      " |  \n",
      " |  parameters(self, include_sublayers=True)\n",
      " |      Returns a list of all Parameters from current layer and its sub-layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list of Tensor : a list of Parameters.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |          import paddle\n",
      " |      \n",
      " |          linear = paddle.nn.Linear(1,1)\n",
      " |          print(linear.parameters())  # print linear_0.w_0 and linear_0.b_0\n",
      " |  \n",
      " |  register_buffer(self, name, tensor, persistable=True)\n",
      " |      Registers a tensor as buffer into the layer.\n",
      " |      \n",
      " |      `buffer` is a non-trainable tensor and will not be updated by optimizer,\n",
      " |      but is necessary for evaluation and inference. For example, the mean and variance in BatchNorm layers.\n",
      " |      The registered buffer is persistable by default, and will be saved into\n",
      " |      `state_dict` alongside parameters. If set persistable=False, it registers\n",
      " |      a non-persistable buffer, so that it will not be a part of `state_dict` .\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this layer using the given name\n",
      " |          tensor (Tensor): the tensor to be registered as buffer.\n",
      " |          persistable (bool): whether the buffer is part of this layer's\n",
      " |              state_dict.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import numpy as np\n",
      " |              import paddle\n",
      " |      \n",
      " |              linear = paddle.nn.Linear(10, 3)\n",
      " |              value = np.array([0]).astype(\"float32\")\n",
      " |              buffer = paddle.to_tensor(value)\n",
      " |              linear.register_buffer(\"buf_name\", buffer, persistable=True)\n",
      " |      \n",
      " |              # get the buffer by attribute.\n",
      " |              print(linear.buf_name)\n",
      " |  \n",
      " |  register_forward_post_hook(self, hook)\n",
      " |      Register a forward post-hook for Layer. The hook will be called after `forward` function has been computed.\n",
      " |      \n",
      " |      It should have the following form, `input` and `output` of the `hook` is `input` and `output` of the `Layer` respectively.\n",
      " |      User can use forward post-hook to change the output of the Layer or perform information statistics tasks on the Layer.\n",
      " |      \n",
      " |      hook(Layer, input, output) -> None or modified output\n",
      " |      \n",
      " |      Parameters:\n",
      " |          hook(function): a function registered as a forward post-hook\n",
      " |      \n",
      " |      Returns:\n",
      " |          HookRemoveHelper: a HookRemoveHelper object that can be used to remove the added hook by calling `hook_remove_helper.remove()` .\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |              import numpy as np\n",
      " |      \n",
      " |              # the forward_post_hook change the output of the layer: output = output * 2\n",
      " |              def forward_post_hook(layer, input, output):\n",
      " |                  # user can use layer, input and output for information statistis tasks\n",
      " |      \n",
      " |                  # change the output\n",
      " |                  return output * 2\n",
      " |      \n",
      " |              linear = paddle.nn.Linear(13, 5)\n",
      " |      \n",
      " |              # register the hook\n",
      " |              forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)\n",
      " |      \n",
      " |              value1 = np.arange(26).reshape(2, 13).astype(\"float32\")\n",
      " |              in1 = paddle.to_tensor(value1)\n",
      " |      \n",
      " |              out0 = linear(in1)\n",
      " |      \n",
      " |              # remove the hook\n",
      " |              forward_post_hook_handle.remove()\n",
      " |      \n",
      " |              out1 = linear(in1)\n",
      " |      \n",
      " |              # hook change the linear's output to output * 2, so out0 is equal to out1 * 2.\n",
      " |              assert (out0.numpy() == (out1.numpy()) * 2).any()\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Register a forward pre-hook for Layer. The hook will be called before `forward` function has been computed.\n",
      " |      \n",
      " |      It should have the following form, `input` of the `hook` is `input` of the `Layer`,\n",
      " |      hook can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if\n",
      " |      a single value is returned(unless that value is already a tuple).\n",
      " |      User can use forward pre-hook to change the input of the Layer or perform information statistics tasks on the Layer.\n",
      " |      \n",
      " |      hook(Layer, input) -> None or modified input\n",
      " |      \n",
      " |      Parameters:\n",
      " |          hook(function): a function registered as a forward pre-hook\n",
      " |      \n",
      " |      Returns:\n",
      " |          HookRemoveHelper: a HookRemoveHelper object that can be used to remove the added hook by calling `hook_remove_helper.remove()` .\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |              import numpy as np\n",
      " |      \n",
      " |              # the forward_post_hook change the input of the layer: input = input * 2\n",
      " |              def forward_pre_hook(layer, input):\n",
      " |                  # user can use layer and input for information statistis tasks\n",
      " |      \n",
      " |                  # change the input\n",
      " |                  input_return = (input[0] * 2)\n",
      " |                  return input_return\n",
      " |      \n",
      " |              linear = paddle.nn.Linear(13, 5)\n",
      " |      \n",
      " |              # register the hook\n",
      " |              forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)\n",
      " |      \n",
      " |              value0 = np.arange(26).reshape(2, 13).astype(\"float32\")\n",
      " |              in0 = paddle.to_tensor(value0)\n",
      " |              out0 = linear(in0)\n",
      " |      \n",
      " |              # remove the hook\n",
      " |              forward_pre_hook_handle.remove()\n",
      " |      \n",
      " |              value1 = value0 * 2\n",
      " |              in1 = paddle.to_tensor(value1)\n",
      " |              out1 = linear(in1)\n",
      " |      \n",
      " |              # hook change the linear's input to input * 2, so out0 is equal to out1.\n",
      " |              assert (out0.numpy() == out1.numpy()).any()\n",
      " |  \n",
      " |  register_state_dict_hook(self, hook)\n",
      " |  \n",
      " |  set_dict = set_state_dict(self, state_dict, use_structured_name=True)\n",
      " |  \n",
      " |  set_state_dict(self, state_dict, use_structured_name=True)\n",
      " |      Set parameters and persistable buffers from state_dict. All the parameters and buffers will be reset by the tensor in the state_dict\n",
      " |      \n",
      " |      Parameters:\n",
      " |          state_dict(dict) : Dict contains all the parameters and persistable buffers.\n",
      " |          use_structured_name(bool, optional) : If true, use structured name as key, otherwise, use parameter or buffer name as key.\n",
      " |                                                Default: True\n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              emb = paddle.nn.Embedding(10, 10)\n",
      " |      \n",
      " |              state_dict = emb.state_dict()\n",
      " |              paddle.save(state_dict, \"paddle_dy.pdparams\")\n",
      " |              para_state_dict = paddle.load(\"paddle_dy.pdparams\")\n",
      " |              emb.set_state_dict(para_state_dict)\n",
      " |  \n",
      " |  state_dict(self, destination=None, include_sublayers=True, structured_name_prefix='')\n",
      " |      Get all parameters and persistable buffers of current layer and its sub-layers. And set them into a dict\n",
      " |      \n",
      " |      Parameters:\n",
      " |          destination(dict, optional) : If provide, all the parameters and persistable buffers will be set to this dict . Default: None\n",
      " |          include_sublayers(bool, optional) : If true, also include the parameters and persistable buffers from sublayers. Default: True\n",
      " |      \n",
      " |      Retruns:\n",
      " |          dict: a dict contains all the parameters and persistable buffers.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              emb = paddle.nn.Embedding(10, 10)\n",
      " |      \n",
      " |              state_dict = emb.state_dict()\n",
      " |              paddle.save( state_dict, \"paddle_dy.pdparams\")\n",
      " |  \n",
      " |  sublayers(self, include_self=False)\n",
      " |      Returns a list of sub layers.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          include_self(bool, optional): Whether return self as sublayers. Default: False\n",
      " |      \n",
      " |      Returns:\n",
      " |          list of Layer : a list of sub layers.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(MyLayer, self).__init__()\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |                      self._dropout = paddle.nn.Dropout(p=0.5)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      temp = self._linear(input)\n",
      " |                      temp = self._dropout(temp)\n",
      " |                      return temp\n",
      " |      \n",
      " |              mylayer = MyLayer()\n",
      " |              print(mylayer.sublayers())  # [<paddle.nn.layer.common.Linear object at 0x7f44b58977d0>, <paddle.nn.layer.common.Dropout object at 0x7f44b58978f0>]\n",
      " |  \n",
      " |  to(self, device=None, dtype=None, blocking=None)\n",
      " |      Cast the parameters and buffers of Layer by the give device, dtype and blocking.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          device(str|paddle.CPUPlace()|paddle.CUDAPlace()|paddle.CUDAPinnedPlace()|paddle.XPUPlace()|None, optional): The device of the Layer which want to be stored.\n",
      " |          If None, the device is the same with the original Tensor. If device is string, it can be ``cpu``, ``gpu:x`` and ``xpu:x``, where ``x`` is the\n",
      " |          index of the GPUs or XPUs. Default: None.\n",
      " |      \n",
      " |          dtype(str|numpy.dtype|paddle.dtype|None, optional): The type of the data. If None, the dtype is the same with the original Tensor. Default: None.\n",
      " |      \n",
      " |          blocking(bool|None, optional): If False and the source is in pinned memory, the copy will be\n",
      " |            asynchronous with respect to the host. Otherwise, the argument has no effect. If None, the blocking is set True. Default: None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          self\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # required: skip\n",
      " |              import paddle\n",
      " |      \n",
      " |              linear=paddle.nn.Linear(2, 2)\n",
      " |              linear.weight\n",
      " |              #Parameter containing:\n",
      " |              #Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      " |              #       [[-0.32770029,  0.38653070],\n",
      " |              #        [ 0.46030545,  0.08158520]])\n",
      " |      \n",
      " |              linear.to(dtype='float64')\n",
      " |              linear.weight\n",
      " |              #Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,\n",
      " |              #       [[-0.32770029,  0.38653070],\n",
      " |              #        [ 0.46030545,  0.08158520]])\n",
      " |      \n",
      " |              linear.to(device='cpu')\n",
      " |              linear.weight\n",
      " |              #Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,\n",
      " |              #       [[-0.32770029,  0.38653070],\n",
      " |              #        [ 0.46030545,  0.08158520]])\n",
      " |              linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)\n",
      " |              linear.weight\n",
      " |              #Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,\n",
      " |              #       [[-0.04989364, -0.56889004],\n",
      " |              #        [ 0.33960250,  0.96878713]])\n",
      " |  \n",
      " |  to_static_state_dict(self, destination=None, include_sublayers=True, structured_name_prefix='')\n",
      " |      Get all parameters and buffers of current layer and its sub-layers. And set them into a dict\n",
      " |      \n",
      " |      Parameters:\n",
      " |          destination(dict, optional) : If provide, all the parameters and persistable buffers will be set to this dict . Default: None\n",
      " |          include_sublayers(bool, optional) : If true, also include the parameters and persistable buffers from sublayers. Default: True\n",
      " |      \n",
      " |      Retruns:\n",
      " |          dict: a dict contains all the parameters and persistable buffers.\n",
      " |      \n",
      " |      Examples:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              emb = paddle.nn.Embedding(10, 10)\n",
      " |      \n",
      " |              state_dict = emb.to_static_state_dict()\n",
      " |              paddle.save( state_dict, \"paddle_dy.pdparams\")\n",
      " |  \n",
      " |  train(self)\n",
      " |      Sets this Layer and all its sublayers to training mode.\n",
      " |      This only effects certain modules like `Dropout` and `BatchNorm`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None\n",
      " |      \n",
      " |      Example::\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import paddle\n",
      " |      \n",
      " |              class MyLayer(paddle.nn.Layer):\n",
      " |                  def __init__(self):\n",
      " |                      super(MyLayer, self).__init__()\n",
      " |                      self._linear = paddle.nn.Linear(1, 1)\n",
      " |                      self._dropout = paddle.nn.Dropout(p=0.5)\n",
      " |      \n",
      " |                  def forward(self, input):\n",
      " |                      temp = self._linear(input)\n",
      " |                      temp = self._dropout(temp)\n",
      " |                      return temp\n",
      " |      \n",
      " |              x = paddle.randn([10, 1], 'float32')\n",
      " |              mylayer = MyLayer()\n",
      " |              mylayer.eval()  # set mylayer._dropout to eval mode\n",
      " |              out = mylayer(x)\n",
      " |              mylayer.train()  # set mylayer._dropout to train mode\n",
      " |              out = mylayer(x)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from paddle.fluid.dygraph.layers.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pybind11_builtins.pybind11_object:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from pybind11_builtins.pybind11_type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(paddle.nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:36.985645Z",
     "iopub.status.busy": "2022-04-15T06:45:36.985472Z",
     "iopub.status.idle": "2022-04-15T06:45:39.943616Z",
     "shell.execute_reply": "2022-04-15T06:45:39.942699Z",
     "shell.execute_reply.started": "2022-04-15T06:45:36.985625Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0415 14:45:36.987524  4223 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0415 14:45:36.991968  4223 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM(3, 16\n",
       "  (0): RNN(\n",
       "    (cell): LSTMCell(3, 16)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.nn.LSTM(input_size=3, hidden_size=16, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:39.946088Z",
     "iopub.status.busy": "2022-04-15T06:45:39.945442Z",
     "iopub.status.idle": "2022-04-15T06:45:39.957562Z",
     "shell.execute_reply": "2022-04-15T06:45:39.956568Z",
     "shell.execute_reply.started": "2022-04-15T06:45:39.946041Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddle.vision.transforms import Transpose\n",
    "\n",
    "# CNN模型 + LSTM\n",
    "class CNNLSTMModel(paddle.nn.Layer):\n",
    "    # 定义模型结构\n",
    "    def __init__(self):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        # 定义CNN网络\n",
    "        self.conv1 = paddle.nn.Conv1D(in_channels=1, out_channels=32, kernel_size=5, padding='SAME', )\n",
    "        self.conv2 = paddle.nn.Conv1D(in_channels=32, out_channels=64, kernel_size=5, padding='SAME')\n",
    "        self.conv3 = paddle.nn.Conv1D(in_channels=64, out_channels=128, kernel_size=5, padding='SAME')\n",
    "        self.max_pool1 = paddle.nn.MaxPool1D(kernel_size=5, stride=2, padding='SAME')\n",
    "        self.dropout = paddle.nn.Dropout(0.5)\n",
    "        \n",
    "        self.lstm = paddle.nn.LSTM(input_size=128, hidden_size=16, num_layers=1)\n",
    "        self.flatten = paddle.nn.Flatten()\n",
    "        #self.fc1 = paddle.nn.Linear(sequence_length*14, 512)\n",
    "        self.fc1 = paddle.nn.Linear(in_features=1648, out_features=512)\n",
    "        self.fc2 = paddle.nn.Linear(512,4)\n",
    "        # 使用多分类器\n",
    "        self.softmax = paddle.nn.Softmax()\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, input):\n",
    "        '''前向计算'''\n",
    "        #print('input', input.shape)\n",
    "        #input = input.values\n",
    "        #input = input.reshape(input.shape[0], 205, 1)\n",
    "        #np.array(data_y.numpy()).reshape(-1, 1)\n",
    "        input = paddle.reshape(input, [input.shape[0], 205, 1])\n",
    "        # 转置\n",
    "        transform_cnn = Transpose(order=(0,2,1))\n",
    "        transform_LSTM = Transpose(order=(0,2,1))\n",
    "        input = transform_cnn(input)\n",
    "        # 前向传播\n",
    "        x = self.conv1(input)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout(x)\n",
    "        # LSTM\n",
    "        x = transform_LSTM(x)\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        x = self.flatten(x)\n",
    "        # 全连接层        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        out = self.softmax(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:39.958997Z",
     "iopub.status.busy": "2022-04-15T06:45:39.958721Z",
     "iopub.status.idle": "2022-04-15T06:45:39.963805Z",
     "shell.execute_reply": "2022-04-15T06:45:39.962847Z",
     "shell.execute_reply.started": "2022-04-15T06:45:39.958958Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义评价函数\n",
    "def mae_loss(y_pred,y_true):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    loss = sum(sum(abs(y_pred-y_true)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:39.965691Z",
     "iopub.status.busy": "2022-04-15T06:45:39.965050Z",
     "iopub.status.idle": "2022-04-15T06:45:40.588708Z",
     "shell.execute_reply": "2022-04-15T06:45:40.587940Z",
     "shell.execute_reply.started": "2022-04-15T06:45:39.965648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.991230</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.764677</td>\n",
       "      <td>0.618571</td>\n",
       "      <td>0.379632</td>\n",
       "      <td>0.190822</td>\n",
       "      <td>0.040237</td>\n",
       "      <td>0.025995</td>\n",
       "      <td>0.031709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.971482</td>\n",
       "      <td>0.928969</td>\n",
       "      <td>0.572933</td>\n",
       "      <td>0.178457</td>\n",
       "      <td>0.122962</td>\n",
       "      <td>0.132360</td>\n",
       "      <td>0.094392</td>\n",
       "      <td>0.089575</td>\n",
       "      <td>0.030481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959149</td>\n",
       "      <td>0.701378</td>\n",
       "      <td>0.231778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080698</td>\n",
       "      <td>0.128376</td>\n",
       "      <td>0.187448</td>\n",
       "      <td>0.280826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.975795</td>\n",
       "      <td>0.934088</td>\n",
       "      <td>0.659637</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>0.237116</td>\n",
       "      <td>0.281445</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>0.249921</td>\n",
       "      <td>0.241397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055816</td>\n",
       "      <td>0.261294</td>\n",
       "      <td>0.359847</td>\n",
       "      <td>0.433143</td>\n",
       "      <td>0.453698</td>\n",
       "      <td>0.499004</td>\n",
       "      <td>0.542796</td>\n",
       "      <td>0.616904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677705</td>\n",
       "      <td>0.222392</td>\n",
       "      <td>0.257158</td>\n",
       "      <td>0.204690</td>\n",
       "      <td>0.054665</td>\n",
       "      <td>0.026152</td>\n",
       "      <td>0.118181</td>\n",
       "      <td>0.244838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>0.926857</td>\n",
       "      <td>0.906347</td>\n",
       "      <td>0.636993</td>\n",
       "      <td>0.415038</td>\n",
       "      <td>0.374745</td>\n",
       "      <td>0.382581</td>\n",
       "      <td>0.358943</td>\n",
       "      <td>0.341359</td>\n",
       "      <td>0.336525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>0.925835</td>\n",
       "      <td>0.587384</td>\n",
       "      <td>0.633226</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.639283</td>\n",
       "      <td>0.614292</td>\n",
       "      <td>0.599155</td>\n",
       "      <td>0.517632</td>\n",
       "      <td>0.403803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994762</td>\n",
       "      <td>0.829702</td>\n",
       "      <td>0.458193</td>\n",
       "      <td>0.264162</td>\n",
       "      <td>0.240228</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.189291</td>\n",
       "      <td>0.203816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>0.916477</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263034</td>\n",
       "      <td>0.385431</td>\n",
       "      <td>0.361067</td>\n",
       "      <td>0.332708</td>\n",
       "      <td>0.339850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id         0         1         2         3         4         5  \\\n",
       "0          0  0.991230  0.943533  0.764677  0.618571  0.379632  0.190822   \n",
       "1          1  0.971482  0.928969  0.572933  0.178457  0.122962  0.132360   \n",
       "2          2  1.000000  0.959149  0.701378  0.231778  0.000000  0.080698   \n",
       "3          3  0.975795  0.934088  0.659637  0.249921  0.237116  0.281445   \n",
       "4          4  0.000000  0.055816  0.261294  0.359847  0.433143  0.453698   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  99995  1.000000  0.677705  0.222392  0.257158  0.204690  0.054665   \n",
       "99996  99996  0.926857  0.906347  0.636993  0.415038  0.374745  0.382581   \n",
       "99997  99997  0.925835  0.587384  0.633226  0.632353  0.639283  0.614292   \n",
       "99998  99998  1.000000  0.994762  0.829702  0.458193  0.264162  0.240228   \n",
       "99999  99999  0.925999  0.916477  0.404290  0.000000  0.263034  0.385431   \n",
       "\n",
       "              6         7         8  ...  196  197  198  199  200  201  202  \\\n",
       "0      0.040237  0.025995  0.031709  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1      0.094392  0.089575  0.030481  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2      0.128376  0.187448  0.280826  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3      0.249921  0.249921  0.241397  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4      0.499004  0.542796  0.616904  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "99995  0.026152  0.118181  0.244838  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99996  0.358943  0.341359  0.336525  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99997  0.599155  0.517632  0.403803  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99998  0.213766  0.189291  0.203816  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "99999  0.361067  0.332708  0.339850  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       203  204  label  \n",
       "0      0.0  0.0    0.0  \n",
       "1      0.0  0.0    0.0  \n",
       "2      0.0  0.0    2.0  \n",
       "3      0.0  0.0    0.0  \n",
       "4      0.0  0.0    2.0  \n",
       "...    ...  ...    ...  \n",
       "99995  0.0  0.0    0.0  \n",
       "99996  0.0  0.0    2.0  \n",
       "99997  0.0  0.0    3.0  \n",
       "99998  0.0  0.0    2.0  \n",
       "99999  0.0  0.0    0.0  \n",
       "\n",
       "[100000 rows x 207 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train1.drop(['label'], axis=1)\n",
    "train_y = train1['label']\n",
    "train_df = pd.concat([train_x, train_y], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:40.591845Z",
     "iopub.status.busy": "2022-04-15T06:45:40.591054Z",
     "iopub.status.idle": "2022-04-15T06:45:41.120481Z",
     "shell.execute_reply": "2022-04-15T06:45:41.119801Z",
     "shell.execute_reply.started": "2022-04-15T06:45:40.591811Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9912298 ],\n",
       "        [0.94353304],\n",
       "        [0.7646773 ],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.9714822 ],\n",
       "        [0.92896875],\n",
       "        [0.57293281],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[1.        ],\n",
       "        [0.95914876],\n",
       "        [0.70137828],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [2.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.92583516],\n",
       "        [0.5873839 ],\n",
       "        [0.63322617],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [3.        ]],\n",
       "\n",
       "       [[1.        ],\n",
       "        [0.99476217],\n",
       "        [0.82970177],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [2.        ]],\n",
       "\n",
       "       [[0.9259994 ],\n",
       "        [0.91647664],\n",
       "        [0.40429008],\n",
       "        ...,\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np = train_df.iloc[:, 1:].values[:, :, np.newaxis]\n",
    "test_np = test1.iloc[:, 1:].values[:, :, np.newaxis]\n",
    "train_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:41.122283Z",
     "iopub.status.busy": "2022-04-15T06:45:41.121596Z",
     "iopub.status.idle": "2022-04-15T06:45:41.330145Z",
     "shell.execute_reply": "2022-04-15T06:45:41.329535Z",
     "shell.execute_reply.started": "2022-04-15T06:45:41.122252Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyDataset at 0x7fb551064d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_dataset = MyDataset(train_df, is_train=True)\n",
    "#test_dataset = MyDataset(test1, is_train=False)\n",
    "train_dataset = MyDataset(train_np, is_train=True)\n",
    "test_dataset = MyDataset(test_np, is_train=False)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:41.331865Z",
     "iopub.status.busy": "2022-04-15T06:45:41.331168Z",
     "iopub.status.idle": "2022-04-15T06:45:41.335218Z",
     "shell.execute_reply": "2022-04-15T06:45:41.334576Z",
     "shell.execute_reply.started": "2022-04-15T06:45:41.331835Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1280\n",
    "train_reader = paddle.io.DataLoader(train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True,\n",
    "                    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:41.336467Z",
     "iopub.status.busy": "2022-04-15T06:45:41.336201Z",
     "iopub.status.idle": "2022-04-15T06:45:41.682656Z",
     "shell.execute_reply": "2022-04-15T06:45:41.681972Z",
     "shell.execute_reply.started": "2022-04-15T06:45:41.336443Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(sparse=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建CNN模型\n",
    "model = CNNLSTMModel()\n",
    "#param_dict = paddle.load('work/model/cnn.model')   #读取保存的参数\n",
    "#model.load_dict(param_dict)    #加载参数\n",
    "# 训练模式\n",
    "model.train() \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False) # 数据转换\n",
    "yy = train1['label'].values.reshape(-1, 1)\n",
    "onehot_encoder.fit(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:45:41.684266Z",
     "iopub.status.busy": "2022-04-15T06:45:41.683721Z",
     "iopub.status.idle": "2022-04-15T06:56:57.111385Z",
     "shell.execute_reply": "2022-04-15T06:56:57.110629Z",
     "shell.execute_reply.started": "2022-04-15T06:45:41.684237Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:0 loss:1.4273521900177002\n",
      "epoch:0,batch:1 loss:1.1258856058120728\n",
      "epoch:0,batch:2 loss:1.0889809131622314\n",
      "epoch:0,batch:3 loss:1.1116371154785156\n",
      "epoch:0,batch:4 loss:1.1014807224273682\n",
      "epoch:0,batch:5 loss:1.1092934608459473\n",
      "epoch:0,batch:6 loss:1.0975745916366577\n",
      "epoch:0,batch:7 loss:1.0796058177947998\n",
      "epoch:0,batch:8 loss:1.094449520111084\n",
      "epoch:0,batch:9 loss:1.0850746631622314\n",
      "epoch:0,batch:10 loss:1.1178871393203735\n",
      "epoch:0,batch:11 loss:1.1022621393203735\n",
      "epoch:0,batch:12 loss:1.0835120677947998\n",
      "epoch:0,batch:13 loss:1.1022621393203735\n",
      "epoch:0,batch:14 loss:1.1139808893203735\n",
      "epoch:0,batch:15 loss:1.0858557224273682\n",
      "epoch:0,batch:16 loss:1.0733559131622314\n",
      "epoch:0,batch:17 loss:1.0952308177947998\n",
      "epoch:0,batch:18 loss:1.0913245677947998\n",
      "epoch:0,batch:19 loss:1.0991370677947998\n",
      "epoch:0,batch:20 loss:1.0819495916366577\n",
      "epoch:0,batch:21 loss:1.1225745677947998\n",
      "epoch:0,batch:22 loss:1.1186683177947998\n",
      "epoch:0,batch:23 loss:1.1108558177947998\n",
      "epoch:0,batch:24 loss:1.1006996631622314\n",
      "epoch:0,batch:25 loss:1.0921058654785156\n",
      "epoch:0,batch:26 loss:1.0842933654785156\n",
      "epoch:0,batch:27 loss:1.0913245677947998\n",
      "epoch:0,batch:28 loss:1.1217933893203735\n",
      "epoch:0,batch:29 loss:1.1116371154785156\n",
      "epoch:0,batch:30 loss:1.0975745916366577\n",
      "epoch:0,batch:31 loss:1.120230793952942\n",
      "epoch:0,batch:32 loss:1.1069495677947998\n",
      "epoch:0,batch:33 loss:1.0913245677947998\n",
      "epoch:0,batch:34 loss:1.1069495677947998\n",
      "epoch:0,batch:35 loss:1.0858558416366577\n",
      "epoch:0,batch:36 loss:1.0889809131622314\n",
      "epoch:0,batch:37 loss:1.098355770111084\n",
      "epoch:0,batch:38 loss:1.1147620677947998\n",
      "epoch:0,batch:39 loss:1.1092934608459473\n",
      "epoch:0,batch:40 loss:1.1030433177947998\n",
      "epoch:0,batch:41 loss:1.1116371154785156\n",
      "epoch:0,batch:42 loss:1.0913245677947998\n",
      "epoch:0,batch:43 loss:1.103824496269226\n",
      "epoch:0,batch:44 loss:1.1092934608459473\n",
      "epoch:0,batch:45 loss:1.100699543952942\n",
      "epoch:0,batch:46 loss:1.1077308654785156\n",
      "epoch:0,batch:47 loss:1.0944496393203735\n",
      "epoch:0,batch:48 loss:1.082730770111084\n",
      "epoch:0,batch:49 loss:1.096793293952942\n",
      "epoch:0,batch:50 loss:1.124137043952942\n",
      "epoch:0,batch:51 loss:1.106168270111084\n",
      "epoch:0,batch:52 loss:1.112418293952942\n",
      "epoch:0,batch:53 loss:1.0881996154785156\n",
      "epoch:0,batch:54 loss:1.0905433893203735\n",
      "epoch:0,batch:55 loss:1.0866371393203735\n",
      "epoch:0,batch:56 loss:1.0905433893203735\n",
      "epoch:0,batch:57 loss:1.0842933654785156\n",
      "epoch:0,batch:58 loss:1.1077308654785156\n",
      "epoch:0,batch:59 loss:1.1350746154785156\n",
      "epoch:0,batch:60 loss:1.0936683416366577\n",
      "epoch:0,batch:61 loss:1.1311683654785156\n",
      "epoch:0,batch:62 loss:1.0967934131622314\n",
      "epoch:0,batch:63 loss:1.1006996631622314\n",
      "epoch:0,batch:64 loss:1.0967934131622314\n",
      "epoch:0,batch:65 loss:1.1069495677947998\n",
      "epoch:0,batch:66 loss:1.0803871154785156\n",
      "epoch:0,batch:67 loss:1.1225745677947998\n",
      "epoch:0,batch:68 loss:1.0803871154785156\n",
      "epoch:0,batch:69 loss:1.1053870916366577\n",
      "epoch:0,batch:70 loss:1.0905433893203735\n",
      "epoch:0,batch:71 loss:1.0928871631622314\n",
      "epoch:0,batch:72 loss:1.100699543952942\n",
      "epoch:0,batch:73 loss:1.1124184131622314\n",
      "epoch:0,batch:74 loss:1.1038246154785156\n",
      "epoch:0,batch:75 loss:1.0913245677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:13<10:50, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,batch:76 loss:1.1108559370040894\n",
      "epoch:0,batch:77 loss:1.0960121154785156\n",
      "epoch:0, train_loss:1.0960121154785156\n",
      "epoch:1,batch:0 loss:1.1030433177947998\n",
      "epoch:1,batch:1 loss:1.0936684608459473\n",
      "epoch:1,batch:2 loss:1.1288245916366577\n",
      "epoch:1,batch:3 loss:1.106168270111084\n",
      "epoch:1,batch:4 loss:1.1131995916366577\n",
      "epoch:1,batch:5 loss:1.1092933416366577\n",
      "epoch:1,batch:6 loss:1.0983558893203735\n",
      "epoch:1,batch:7 loss:1.1014808416366577\n",
      "epoch:1,batch:8 loss:1.0850746631622314\n",
      "epoch:1,batch:9 loss:1.1069496870040894\n",
      "epoch:1,batch:10 loss:1.0881996154785156\n",
      "epoch:1,batch:11 loss:1.1131995916366577\n",
      "epoch:1,batch:12 loss:1.1053870916366577\n",
      "epoch:1,batch:13 loss:1.0710121393203735\n",
      "epoch:1,batch:14 loss:1.1022621393203735\n",
      "epoch:1,batch:15 loss:1.0881996154785156\n",
      "epoch:1,batch:16 loss:1.0967934131622314\n",
      "epoch:1,batch:17 loss:1.1100746393203735\n",
      "epoch:1,batch:18 loss:1.090543270111084\n",
      "epoch:1,batch:19 loss:1.0975745916366577\n",
      "epoch:1,batch:20 loss:1.125699520111084\n",
      "epoch:1,batch:21 loss:1.1311683654785156\n",
      "epoch:1,batch:22 loss:1.1171058416366577\n",
      "epoch:1,batch:23 loss:1.0952308177947998\n",
      "epoch:1,batch:24 loss:1.0991370677947998\n",
      "epoch:1,batch:25 loss:1.0975747108459473\n",
      "epoch:1,batch:26 loss:1.0928871631622314\n",
      "epoch:1,batch:27 loss:1.0678870677947998\n",
      "epoch:1,batch:28 loss:1.1100746393203735\n",
      "epoch:1,batch:29 loss:1.1061683893203735\n",
      "epoch:1,batch:30 loss:1.0928871631622314\n",
      "epoch:1,batch:31 loss:1.0897620916366577\n",
      "epoch:1,batch:32 loss:1.1092933416366577\n",
      "epoch:1,batch:33 loss:1.082730770111084\n",
      "epoch:1,batch:34 loss:1.1014808416366577\n",
      "epoch:1,batch:35 loss:1.107730746269226\n",
      "epoch:1,batch:36 loss:1.0913245677947998\n",
      "epoch:1,batch:37 loss:1.0921058654785156\n",
      "epoch:1,batch:38 loss:1.1022621393203735\n",
      "epoch:1,batch:39 loss:1.113980770111084\n",
      "epoch:1,batch:40 loss:1.0999183654785156\n",
      "epoch:1,batch:41 loss:1.1061683893203735\n",
      "epoch:1,batch:42 loss:1.1116371154785156\n",
      "epoch:1,batch:43 loss:1.0803871154785156\n",
      "epoch:1,batch:44 loss:1.0874183177947998\n",
      "epoch:1,batch:45 loss:1.117887020111084\n",
      "epoch:1,batch:46 loss:1.1069495677947998\n",
      "epoch:1,batch:47 loss:1.100699543952942\n",
      "epoch:1,batch:48 loss:1.106168270111084\n",
      "epoch:1,batch:49 loss:1.100699543952942\n",
      "epoch:1,batch:50 loss:1.1311683654785156\n",
      "epoch:1,batch:51 loss:1.0960121154785156\n",
      "epoch:1,batch:52 loss:1.112418293952942\n",
      "epoch:1,batch:53 loss:1.121793270111084\n",
      "epoch:1,batch:54 loss:1.0780433416366577\n",
      "epoch:1,batch:55 loss:1.1038246154785156\n",
      "epoch:1,batch:56 loss:1.096793293952942\n",
      "epoch:1,batch:57 loss:1.1053870916366577\n",
      "epoch:1,batch:58 loss:1.0858558416366577\n",
      "epoch:1,batch:59 loss:1.0678870677947998\n",
      "epoch:1,batch:60 loss:1.0881996154785156\n",
      "epoch:1,batch:61 loss:1.0913245677947998\n",
      "epoch:1,batch:62 loss:1.0897620916366577\n",
      "epoch:1,batch:63 loss:1.0913245677947998\n",
      "epoch:1,batch:64 loss:1.107730746269226\n",
      "epoch:1,batch:65 loss:1.1069495677947998\n",
      "epoch:1,batch:66 loss:1.106168270111084\n",
      "epoch:1,batch:67 loss:1.1217933893203735\n",
      "epoch:1,batch:68 loss:1.0850746631622314\n",
      "epoch:1,batch:69 loss:1.106168270111084\n",
      "epoch:1,batch:70 loss:1.1100746393203735\n",
      "epoch:1,batch:71 loss:1.0881996154785156\n",
      "epoch:1,batch:72 loss:1.1194496154785156\n",
      "epoch:1,batch:73 loss:1.0952308177947998\n",
      "epoch:1,batch:74 loss:1.096793293952942\n",
      "epoch:1,batch:75 loss:1.0944496393203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:26<10:38, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,batch:76 loss:1.0827308893203735\n",
      "epoch:1,batch:77 loss:1.0999183654785156\n",
      "epoch:1, train_loss:1.0999183654785156\n",
      "epoch:2,batch:0 loss:1.0999183654785156\n",
      "epoch:2,batch:1 loss:1.0881996154785156\n",
      "epoch:2,batch:2 loss:1.1053870916366577\n",
      "epoch:2,batch:3 loss:1.0874183177947998\n",
      "epoch:2,batch:4 loss:1.1069495677947998\n",
      "epoch:2,batch:5 loss:1.1092932224273682\n",
      "epoch:2,batch:6 loss:1.1327309608459473\n",
      "epoch:2,batch:7 loss:1.1069495677947998\n",
      "epoch:2,batch:8 loss:1.1264808177947998\n",
      "epoch:2,batch:9 loss:1.0858558416366577\n",
      "epoch:2,batch:10 loss:1.085074543952942\n",
      "epoch:2,batch:11 loss:1.1085121631622314\n",
      "epoch:2,batch:12 loss:1.0764808654785156\n",
      "epoch:2,batch:13 loss:1.102262020111084\n",
      "epoch:2,batch:14 loss:1.088980793952942\n",
      "epoch:2,batch:15 loss:1.125699520111084\n",
      "epoch:2,batch:16 loss:1.0913245677947998\n",
      "epoch:2,batch:17 loss:1.1069495677947998\n",
      "epoch:2,batch:18 loss:1.0952308177947998\n",
      "epoch:2,batch:19 loss:1.1069495677947998\n",
      "epoch:2,batch:20 loss:1.1139808893203735\n",
      "epoch:2,batch:21 loss:1.113980770111084\n",
      "epoch:2,batch:22 loss:1.117887020111084\n",
      "epoch:2,batch:23 loss:1.1241371631622314\n",
      "epoch:2,batch:24 loss:1.0881996154785156\n",
      "epoch:2,batch:25 loss:1.0874183177947998\n",
      "epoch:2,batch:26 loss:1.0921058654785156\n",
      "epoch:2,batch:27 loss:1.107730746269226\n",
      "epoch:2,batch:28 loss:1.0717933177947998\n",
      "epoch:2,batch:29 loss:1.0999183654785156\n",
      "epoch:2,batch:30 loss:1.124137043952942\n",
      "epoch:2,batch:31 loss:1.0921058654785156\n",
      "epoch:2,batch:32 loss:1.100699543952942\n",
      "epoch:2,batch:33 loss:1.1272621154785156\n",
      "epoch:2,batch:34 loss:1.0866371393203735\n",
      "epoch:2,batch:35 loss:1.0819497108459473\n",
      "epoch:2,batch:36 loss:1.102262258529663\n",
      "epoch:2,batch:37 loss:1.0936683416366577\n",
      "epoch:2,batch:38 loss:1.1038246154785156\n",
      "epoch:2,batch:39 loss:1.0819495916366577\n",
      "epoch:2,batch:40 loss:1.1155433654785156\n",
      "epoch:2,batch:41 loss:1.0991370677947998\n",
      "epoch:2,batch:42 loss:1.1085121631622314\n",
      "epoch:2,batch:43 loss:1.1092933416366577\n",
      "epoch:2,batch:44 loss:1.1147620677947998\n",
      "epoch:2,batch:45 loss:1.0936683416366577\n",
      "epoch:2,batch:46 loss:1.0819494724273682\n",
      "epoch:2,batch:47 loss:1.100699543952942\n",
      "epoch:2,batch:48 loss:1.0803871154785156\n",
      "epoch:2,batch:49 loss:1.092887043952942\n",
      "epoch:2,batch:50 loss:1.1077308654785156\n",
      "epoch:2,batch:51 loss:1.0835120677947998\n",
      "epoch:2,batch:52 loss:1.094449520111084\n",
      "epoch:2,batch:53 loss:1.090543270111084\n",
      "epoch:2,batch:54 loss:1.1046059131622314\n",
      "epoch:2,batch:55 loss:1.1108558177947998\n",
      "epoch:2,batch:56 loss:1.108512043952942\n",
      "epoch:2,batch:57 loss:1.0921058654785156\n",
      "epoch:2,batch:58 loss:1.0944496393203735\n",
      "epoch:2,batch:59 loss:1.110074520111084\n",
      "epoch:2,batch:60 loss:1.1147620677947998\n",
      "epoch:2,batch:61 loss:1.0960121154785156\n",
      "epoch:2,batch:62 loss:1.0874183177947998\n",
      "epoch:2,batch:63 loss:1.0842933654785156\n",
      "epoch:2,batch:64 loss:1.1288244724273682\n",
      "epoch:2,batch:65 loss:1.0921058654785156\n",
      "epoch:2,batch:66 loss:1.112418293952942\n",
      "epoch:2,batch:67 loss:1.100699543952942\n",
      "epoch:2,batch:68 loss:1.110074520111084\n",
      "epoch:2,batch:69 loss:1.1233558654785156\n",
      "epoch:2,batch:70 loss:1.0725746154785156\n",
      "epoch:2,batch:71 loss:1.0866371393203735\n",
      "epoch:2,batch:72 loss:1.0975744724273682\n",
      "epoch:2,batch:73 loss:1.0874183177947998\n",
      "epoch:2,batch:74 loss:1.1038246154785156\n",
      "epoch:2,batch:75 loss:1.088980793952942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:39<10:23, 13.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2,batch:76 loss:1.1131995916366577\n",
      "epoch:2,batch:77 loss:1.0858558416366577\n",
      "epoch:2, train_loss:1.0858558416366577\n",
      "epoch:3,batch:0 loss:1.0811684131622314\n",
      "epoch:3,batch:1 loss:1.128043293952942\n",
      "epoch:3,batch:2 loss:1.0967934131622314\n",
      "epoch:3,batch:3 loss:1.1022621393203735\n",
      "epoch:3,batch:4 loss:1.0999183654785156\n",
      "epoch:3,batch:5 loss:1.1319496631622314\n",
      "epoch:3,batch:6 loss:1.1249184608459473\n",
      "epoch:3,batch:7 loss:1.0921058654785156\n",
      "epoch:3,batch:8 loss:1.0678870677947998\n",
      "epoch:3,batch:9 loss:1.0936683416366577\n",
      "epoch:3,batch:10 loss:1.092887043952942\n",
      "epoch:3,batch:11 loss:1.112418293952942\n",
      "epoch:3,batch:12 loss:1.096793293952942\n",
      "epoch:3,batch:13 loss:1.085074543952942\n",
      "epoch:3,batch:14 loss:1.1077308654785156\n",
      "epoch:3,batch:15 loss:1.0960121154785156\n",
      "epoch:3,batch:16 loss:1.1092932224273682\n",
      "epoch:3,batch:17 loss:1.0889809131622314\n",
      "epoch:3,batch:18 loss:1.1186683177947998\n",
      "epoch:3,batch:19 loss:1.0850746631622314\n",
      "epoch:3,batch:20 loss:1.1147620677947998\n",
      "epoch:3,batch:21 loss:1.1061683893203735\n",
      "epoch:3,batch:22 loss:1.1085121631622314\n",
      "epoch:3,batch:23 loss:1.1061683893203735\n",
      "epoch:3,batch:24 loss:1.1116371154785156\n",
      "epoch:3,batch:25 loss:1.1046059131622314\n",
      "epoch:3,batch:26 loss:1.0991369485855103\n",
      "epoch:3,batch:27 loss:1.0897619724273682\n",
      "epoch:3,batch:28 loss:1.0827308893203735\n",
      "epoch:3,batch:29 loss:1.0897620916366577\n",
      "epoch:3,batch:30 loss:1.1053870916366577\n",
      "epoch:3,batch:31 loss:1.0975745916366577\n",
      "epoch:3,batch:32 loss:1.1131994724273682\n",
      "epoch:3,batch:33 loss:1.1006996631622314\n",
      "epoch:3,batch:34 loss:1.1038246154785156\n",
      "epoch:3,batch:35 loss:1.086637020111084\n",
      "epoch:3,batch:36 loss:1.1100746393203735\n",
      "epoch:3,batch:37 loss:1.1046059131622314\n",
      "epoch:3,batch:38 loss:1.0913246870040894\n",
      "epoch:3,batch:39 loss:1.0796059370040894\n",
      "epoch:3,batch:40 loss:1.0905433893203735\n",
      "epoch:3,batch:41 loss:1.0835121870040894\n",
      "epoch:3,batch:42 loss:1.1163246631622314\n",
      "epoch:3,batch:43 loss:1.1022621393203735\n",
      "epoch:3,batch:44 loss:1.1053870916366577\n",
      "epoch:3,batch:45 loss:1.1147620677947998\n",
      "epoch:3,batch:46 loss:1.111636996269226\n",
      "epoch:3,batch:47 loss:1.1155433654785156\n",
      "epoch:3,batch:48 loss:1.1022621393203735\n",
      "epoch:3,batch:49 loss:1.121793270111084\n",
      "epoch:3,batch:50 loss:1.1296058893203735\n",
      "epoch:3,batch:51 loss:1.0921058654785156\n",
      "epoch:3,batch:52 loss:1.092887043952942\n",
      "epoch:3,batch:53 loss:1.0827308893203735\n",
      "epoch:3,batch:54 loss:1.1046059131622314\n",
      "epoch:3,batch:55 loss:1.0991370677947998\n",
      "epoch:3,batch:56 loss:1.1085121631622314\n",
      "epoch:3,batch:57 loss:1.0921058654785156\n",
      "epoch:3,batch:58 loss:1.0796058177947998\n",
      "epoch:3,batch:59 loss:1.0952308177947998\n",
      "epoch:3,batch:60 loss:1.1092934608459473\n",
      "epoch:3,batch:61 loss:1.1006996631622314\n",
      "epoch:3,batch:62 loss:1.1030433177947998\n",
      "epoch:3,batch:63 loss:1.1046059131622314\n",
      "epoch:3,batch:64 loss:1.0921058654785156\n",
      "epoch:3,batch:65 loss:1.110074520111084\n",
      "epoch:3,batch:66 loss:1.0975747108459473\n",
      "epoch:3,batch:67 loss:1.0764808654785156\n",
      "epoch:3,batch:68 loss:1.0874184370040894\n",
      "epoch:3,batch:69 loss:1.092105746269226\n",
      "epoch:3,batch:70 loss:1.115543246269226\n",
      "epoch:3,batch:71 loss:1.1131994724273682\n",
      "epoch:3,batch:72 loss:1.0991370677947998\n",
      "epoch:3,batch:73 loss:1.1038246154785156\n",
      "epoch:3,batch:74 loss:1.0921058654785156\n",
      "epoch:3,batch:75 loss:1.0967934131622314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:54<10:30, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3,batch:76 loss:1.1014808416366577\n",
      "epoch:3,batch:77 loss:1.0819495916366577\n",
      "epoch:3, train_loss:1.0819495916366577\n",
      "epoch:4,batch:0 loss:1.0960121154785156\n",
      "epoch:4,batch:1 loss:1.0944496393203735\n",
      "epoch:4,batch:2 loss:1.1210120916366577\n",
      "epoch:4,batch:3 loss:1.1077308654785156\n",
      "epoch:4,batch:4 loss:1.117887020111084\n",
      "epoch:4,batch:5 loss:1.0928871631622314\n",
      "epoch:4,batch:6 loss:1.092887043952942\n",
      "epoch:4,batch:7 loss:1.0889809131622314\n",
      "epoch:4,batch:8 loss:1.1077308654785156\n",
      "epoch:4,batch:9 loss:1.0889809131622314\n",
      "epoch:4,batch:10 loss:1.0764808654785156\n",
      "epoch:4,batch:11 loss:1.1030433177947998\n",
      "epoch:4,batch:12 loss:1.1022621393203735\n",
      "epoch:4,batch:13 loss:1.1100746393203735\n",
      "epoch:4,batch:14 loss:1.1014808416366577\n",
      "epoch:4,batch:15 loss:1.1038247346878052\n",
      "epoch:4,batch:16 loss:1.1077308654785156\n",
      "epoch:4,batch:17 loss:1.1077308654785156\n",
      "epoch:4,batch:18 loss:1.1092933416366577\n",
      "epoch:4,batch:19 loss:1.1006996631622314\n",
      "epoch:4,batch:20 loss:1.1022621393203735\n",
      "epoch:4,batch:21 loss:1.0960121154785156\n",
      "epoch:4,batch:22 loss:1.1194496154785156\n",
      "epoch:4,batch:23 loss:1.1131994724273682\n",
      "epoch:4,batch:24 loss:1.0975744724273682\n",
      "epoch:4,batch:25 loss:1.1186683177947998\n",
      "epoch:4,batch:26 loss:1.0983558893203735\n",
      "epoch:4,batch:27 loss:1.0991370677947998\n",
      "epoch:4,batch:28 loss:1.107730746269226\n",
      "epoch:4,batch:29 loss:1.1139808893203735\n",
      "epoch:4,batch:30 loss:1.1085121631622314\n",
      "epoch:4,batch:31 loss:1.1124184131622314\n",
      "epoch:4,batch:32 loss:1.113980770111084\n",
      "epoch:4,batch:33 loss:1.0858558416366577\n",
      "epoch:4,batch:34 loss:1.092105746269226\n",
      "epoch:4,batch:35 loss:1.0921058654785156\n",
      "epoch:4,batch:36 loss:1.0874183177947998\n",
      "epoch:4,batch:37 loss:1.0819494724273682\n",
      "epoch:4,batch:38 loss:1.1210119724273682\n",
      "epoch:4,batch:39 loss:1.094449520111084\n",
      "epoch:4,batch:40 loss:1.103824496269226\n",
      "epoch:4,batch:41 loss:1.1131994724273682\n",
      "epoch:4,batch:42 loss:1.1038246154785156\n",
      "epoch:4,batch:43 loss:1.0936682224273682\n",
      "epoch:4,batch:44 loss:1.067105770111084\n",
      "epoch:4,batch:45 loss:1.094449520111084\n",
      "epoch:4,batch:46 loss:1.0749183893203735\n",
      "epoch:4,batch:47 loss:1.0960121154785156\n",
      "epoch:4,batch:48 loss:1.0944496393203735\n",
      "epoch:4,batch:49 loss:1.1053870916366577\n",
      "epoch:4,batch:50 loss:1.0858558416366577\n",
      "epoch:4,batch:51 loss:1.092887043952942\n",
      "epoch:4,batch:52 loss:1.102262020111084\n",
      "epoch:4,batch:53 loss:1.0936684608459473\n",
      "epoch:4,batch:54 loss:1.102262020111084\n",
      "epoch:4,batch:55 loss:1.1038246154785156\n",
      "epoch:4,batch:56 loss:1.0772621631622314\n",
      "epoch:4,batch:57 loss:1.0983558893203735\n",
      "epoch:4,batch:58 loss:1.1053872108459473\n",
      "epoch:4,batch:59 loss:1.106168270111084\n",
      "epoch:4,batch:60 loss:1.0858558416366577\n",
      "epoch:4,batch:61 loss:1.0913246870040894\n",
      "epoch:4,batch:62 loss:1.1030433177947998\n",
      "epoch:4,batch:63 loss:1.0913245677947998\n",
      "epoch:4,batch:64 loss:1.1038246154785156\n",
      "epoch:4,batch:65 loss:1.1210120916366577\n",
      "epoch:4,batch:66 loss:1.1155433654785156\n",
      "epoch:4,batch:67 loss:1.0991370677947998\n",
      "epoch:4,batch:68 loss:1.104605793952942\n",
      "epoch:4,batch:69 loss:1.1225745677947998\n",
      "epoch:4,batch:70 loss:1.0967934131622314\n",
      "epoch:4,batch:71 loss:1.077262043952942\n",
      "epoch:4,batch:72 loss:1.1202309131622314\n",
      "epoch:4,batch:73 loss:1.090543270111084\n",
      "epoch:4,batch:74 loss:1.0749183893203735\n",
      "epoch:4,batch:75 loss:1.102262020111084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:07<10:06, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4,batch:76 loss:1.131949543952942\n",
      "epoch:4,batch:77 loss:1.0975745916366577\n",
      "epoch:4, train_loss:1.0975745916366577\n",
      "epoch:5,batch:0 loss:1.0944496393203735\n",
      "epoch:5,batch:1 loss:1.0975745916366577\n",
      "epoch:5,batch:2 loss:1.094449520111084\n",
      "epoch:5,batch:3 loss:1.081168293952942\n",
      "epoch:5,batch:4 loss:1.0850746631622314\n",
      "epoch:5,batch:5 loss:1.074918270111084\n",
      "epoch:5,batch:6 loss:1.1131997108459473\n",
      "epoch:5,batch:7 loss:1.1155433654785156\n",
      "epoch:5,batch:8 loss:1.1038246154785156\n",
      "epoch:5,batch:9 loss:1.1053870916366577\n",
      "epoch:5,batch:10 loss:1.117887020111084\n",
      "epoch:5,batch:11 loss:1.0874183177947998\n",
      "epoch:5,batch:12 loss:1.0858557224273682\n",
      "epoch:5,batch:13 loss:1.0936683416366577\n",
      "epoch:5,batch:14 loss:1.102262020111084\n",
      "epoch:5,batch:15 loss:1.1210120916366577\n",
      "epoch:5,batch:16 loss:1.116324543952942\n",
      "epoch:5,batch:17 loss:1.104605793952942\n",
      "epoch:5,batch:18 loss:1.0905433893203735\n",
      "epoch:5,batch:19 loss:1.1366370916366577\n",
      "epoch:5,batch:20 loss:1.0835121870040894\n",
      "epoch:5,batch:21 loss:1.1131994724273682\n",
      "epoch:5,batch:22 loss:1.0819494724273682\n",
      "epoch:5,batch:23 loss:1.0991370677947998\n",
      "epoch:5,batch:24 loss:1.0975745916366577\n",
      "epoch:5,batch:25 loss:1.0967934131622314\n",
      "epoch:5,batch:26 loss:1.0788246393203735\n",
      "epoch:5,batch:27 loss:1.0842933654785156\n",
      "epoch:5,batch:28 loss:1.1233558654785156\n",
      "epoch:5,batch:29 loss:1.116324543952942\n",
      "epoch:5,batch:30 loss:1.098355770111084\n",
      "epoch:5,batch:31 loss:1.0803871154785156\n",
      "epoch:5,batch:32 loss:1.0983558893203735\n",
      "epoch:5,batch:33 loss:1.110074520111084\n",
      "epoch:5,batch:34 loss:1.1186683177947998\n",
      "epoch:5,batch:35 loss:1.102262020111084\n",
      "epoch:5,batch:36 loss:1.0897620916366577\n",
      "epoch:5,batch:37 loss:1.100699543952942\n",
      "epoch:5,batch:38 loss:1.0991371870040894\n",
      "epoch:5,batch:39 loss:1.0749183893203735\n",
      "epoch:5,batch:40 loss:1.068668246269226\n",
      "epoch:5,batch:41 loss:1.1053870916366577\n",
      "epoch:5,batch:42 loss:1.1171058416366577\n",
      "epoch:5,batch:43 loss:1.098355770111084\n",
      "epoch:5,batch:44 loss:1.0897620916366577\n",
      "epoch:5,batch:45 loss:1.0952308177947998\n",
      "epoch:5,batch:46 loss:1.1178871393203735\n",
      "epoch:5,batch:47 loss:1.1217933893203735\n",
      "epoch:5,batch:48 loss:1.094449520111084\n",
      "epoch:5,batch:49 loss:1.129605770111084\n",
      "epoch:5,batch:50 loss:1.0756995677947998\n",
      "epoch:5,batch:51 loss:1.0889809131622314\n",
      "epoch:5,batch:52 loss:1.0991370677947998\n",
      "epoch:5,batch:53 loss:1.0999183654785156\n",
      "epoch:5,batch:54 loss:1.100699543952942\n",
      "epoch:5,batch:55 loss:1.116324543952942\n",
      "epoch:5,batch:56 loss:1.0835120677947998\n",
      "epoch:5,batch:57 loss:1.125699520111084\n",
      "epoch:5,batch:58 loss:1.108512043952942\n",
      "epoch:5,batch:59 loss:1.1124184131622314\n",
      "epoch:5,batch:60 loss:1.0921058654785156\n",
      "epoch:5,batch:61 loss:1.081168293952942\n",
      "epoch:5,batch:62 loss:1.102262020111084\n",
      "epoch:5,batch:63 loss:1.0999183654785156\n",
      "epoch:5,batch:64 loss:1.0960121154785156\n",
      "epoch:5,batch:65 loss:1.0913245677947998\n",
      "epoch:5,batch:66 loss:1.0999183654785156\n",
      "epoch:5,batch:67 loss:1.1131997108459473\n",
      "epoch:5,batch:68 loss:1.0881996154785156\n",
      "epoch:5,batch:69 loss:1.1069495677947998\n",
      "epoch:5,batch:70 loss:1.104605793952942\n",
      "epoch:5,batch:71 loss:1.1155433654785156\n",
      "epoch:5,batch:72 loss:1.0624183416366577\n",
      "epoch:5,batch:73 loss:1.1085121631622314\n",
      "epoch:5,batch:74 loss:1.1053870916366577\n",
      "epoch:5,batch:75 loss:1.1194496154785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:20<09:50, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5,batch:76 loss:1.1210120916366577\n",
      "epoch:5,batch:77 loss:1.099918246269226\n",
      "epoch:5, train_loss:1.099918246269226\n",
      "epoch:6,batch:0 loss:1.1256996393203735\n",
      "epoch:6,batch:1 loss:1.1085121631622314\n",
      "epoch:6,batch:2 loss:1.1085121631622314\n",
      "epoch:6,batch:3 loss:1.1116371154785156\n",
      "epoch:6,batch:4 loss:1.0874184370040894\n",
      "epoch:6,batch:5 loss:1.1061683893203735\n",
      "epoch:6,batch:6 loss:1.1030433177947998\n",
      "epoch:6,batch:7 loss:1.0858558416366577\n",
      "epoch:6,batch:8 loss:1.0944496393203735\n",
      "epoch:6,batch:9 loss:1.0881996154785156\n",
      "epoch:6,batch:10 loss:1.110074520111084\n",
      "epoch:6,batch:11 loss:1.1014808416366577\n",
      "epoch:6,batch:12 loss:1.0921058654785156\n",
      "epoch:6,batch:13 loss:1.1186683177947998\n",
      "epoch:6,batch:14 loss:1.1085121631622314\n",
      "epoch:6,batch:15 loss:1.094449520111084\n",
      "epoch:6,batch:16 loss:1.0881996154785156\n",
      "epoch:6,batch:17 loss:1.1249182224273682\n",
      "epoch:6,batch:18 loss:1.0960121154785156\n",
      "epoch:6,batch:19 loss:1.098355770111084\n",
      "epoch:6,batch:20 loss:1.084293246269226\n",
      "epoch:6,batch:21 loss:1.0796058177947998\n",
      "epoch:6,batch:22 loss:1.094449520111084\n",
      "epoch:6,batch:23 loss:1.090543270111084\n",
      "epoch:6,batch:24 loss:1.1030434370040894\n",
      "epoch:6,batch:25 loss:1.0913245677947998\n",
      "epoch:6,batch:26 loss:1.1210122108459473\n",
      "epoch:6,batch:27 loss:1.0960121154785156\n",
      "epoch:6,batch:28 loss:1.0811684131622314\n",
      "epoch:6,batch:29 loss:1.0991370677947998\n",
      "epoch:6,batch:30 loss:1.0999183654785156\n",
      "epoch:6,batch:31 loss:1.0975745916366577\n",
      "epoch:6,batch:32 loss:1.1069496870040894\n",
      "epoch:6,batch:33 loss:1.1194496154785156\n",
      "epoch:6,batch:34 loss:1.1022621393203735\n",
      "epoch:6,batch:35 loss:1.119449496269226\n",
      "epoch:6,batch:36 loss:1.0881996154785156\n",
      "epoch:6,batch:37 loss:1.100699543952942\n",
      "epoch:6,batch:38 loss:1.078824520111084\n",
      "epoch:6,batch:39 loss:1.0913245677947998\n",
      "epoch:6,batch:40 loss:1.0858557224273682\n",
      "epoch:6,batch:41 loss:1.1092933416366577\n",
      "epoch:6,batch:42 loss:1.0905433893203735\n",
      "epoch:6,batch:43 loss:1.077262043952942\n",
      "epoch:6,batch:44 loss:1.1264808177947998\n",
      "epoch:6,batch:45 loss:1.1342933177947998\n",
      "epoch:6,batch:46 loss:1.0819494724273682\n",
      "epoch:6,batch:47 loss:1.1014808416366577\n",
      "epoch:6,batch:48 loss:1.112418293952942\n",
      "epoch:6,batch:49 loss:1.1225745677947998\n",
      "epoch:6,batch:50 loss:1.104605793952942\n",
      "epoch:6,batch:51 loss:1.1186683177947998\n",
      "epoch:6,batch:52 loss:1.092887043952942\n",
      "epoch:6,batch:53 loss:1.0881996154785156\n",
      "epoch:6,batch:54 loss:1.088980793952942\n",
      "epoch:6,batch:55 loss:1.0780433416366577\n",
      "epoch:6,batch:56 loss:1.0889809131622314\n",
      "epoch:6,batch:57 loss:1.125699758529663\n",
      "epoch:6,batch:58 loss:1.0913245677947998\n",
      "epoch:6,batch:59 loss:1.1006996631622314\n",
      "epoch:6,batch:60 loss:1.1092933416366577\n",
      "epoch:6,batch:61 loss:1.1272621154785156\n",
      "epoch:6,batch:62 loss:1.1147620677947998\n",
      "epoch:6,batch:63 loss:1.0905433893203735\n",
      "epoch:6,batch:64 loss:1.0983558893203735\n",
      "epoch:6,batch:65 loss:1.1131995916366577\n",
      "epoch:6,batch:66 loss:1.0983558893203735\n",
      "epoch:6,batch:67 loss:1.0803871154785156\n",
      "epoch:6,batch:68 loss:1.0842933654785156\n",
      "epoch:6,batch:69 loss:1.092887043952942\n",
      "epoch:6,batch:70 loss:1.1014808416366577\n",
      "epoch:6,batch:71 loss:1.1092933416366577\n",
      "epoch:6,batch:72 loss:1.113980770111084\n",
      "epoch:6,batch:73 loss:1.0913245677947998\n",
      "epoch:6,batch:74 loss:1.1022621393203735\n",
      "epoch:6,batch:75 loss:1.0835120677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:34<09:34, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6,batch:76 loss:1.113981008529663\n",
      "epoch:6,batch:77 loss:1.0889809131622314\n",
      "epoch:6, train_loss:1.0889809131622314\n",
      "epoch:7,batch:0 loss:1.094449520111084\n",
      "epoch:7,batch:1 loss:1.0819495916366577\n",
      "epoch:7,batch:2 loss:1.096011996269226\n",
      "epoch:7,batch:3 loss:1.1069496870040894\n",
      "epoch:7,batch:4 loss:1.1069495677947998\n",
      "epoch:7,batch:5 loss:1.102262020111084\n",
      "epoch:7,batch:6 loss:1.102262020111084\n",
      "epoch:7,batch:7 loss:1.1116371154785156\n",
      "epoch:7,batch:8 loss:1.0850746631622314\n",
      "epoch:7,batch:9 loss:1.1061683893203735\n",
      "epoch:7,batch:10 loss:1.100699543952942\n",
      "epoch:7,batch:11 loss:1.1046059131622314\n",
      "epoch:7,batch:12 loss:1.1085121631622314\n",
      "epoch:7,batch:13 loss:1.1249184608459473\n",
      "epoch:7,batch:14 loss:1.0928869247436523\n",
      "epoch:7,batch:15 loss:1.1006996631622314\n",
      "epoch:7,batch:16 loss:1.112418293952942\n",
      "epoch:7,batch:17 loss:1.117887020111084\n",
      "epoch:7,batch:18 loss:1.0741369724273682\n",
      "epoch:7,batch:19 loss:1.0803871154785156\n",
      "epoch:7,batch:20 loss:1.0928871631622314\n",
      "epoch:7,batch:21 loss:1.1155433654785156\n",
      "epoch:7,batch:22 loss:1.1077308654785156\n",
      "epoch:7,batch:23 loss:1.110074520111084\n",
      "epoch:7,batch:24 loss:1.0874183177947998\n",
      "epoch:7,batch:25 loss:1.1030433177947998\n",
      "epoch:7,batch:26 loss:1.111636996269226\n",
      "epoch:7,batch:27 loss:1.1092933416366577\n",
      "epoch:7,batch:28 loss:1.1014808416366577\n",
      "epoch:7,batch:29 loss:1.1100746393203735\n",
      "epoch:7,batch:30 loss:1.0756995677947998\n",
      "epoch:7,batch:31 loss:1.1030434370040894\n",
      "epoch:7,batch:32 loss:1.0624183416366577\n",
      "epoch:7,batch:33 loss:1.0858559608459473\n",
      "epoch:7,batch:34 loss:1.1006996631622314\n",
      "epoch:7,batch:35 loss:1.0897619724273682\n",
      "epoch:7,batch:36 loss:1.0960122346878052\n",
      "epoch:7,batch:37 loss:1.0913245677947998\n",
      "epoch:7,batch:38 loss:1.0866371393203735\n",
      "epoch:7,batch:39 loss:1.1030433177947998\n",
      "epoch:7,batch:40 loss:1.113980770111084\n",
      "epoch:7,batch:41 loss:1.1100746393203735\n",
      "epoch:7,batch:42 loss:1.1131997108459473\n",
      "epoch:7,batch:43 loss:1.1085119247436523\n",
      "epoch:7,batch:44 loss:1.1100746393203735\n",
      "epoch:7,batch:45 loss:1.100699543952942\n",
      "epoch:7,batch:46 loss:1.086637020111084\n",
      "epoch:7,batch:47 loss:1.1194496154785156\n",
      "epoch:7,batch:48 loss:1.0991370677947998\n",
      "epoch:7,batch:49 loss:1.0881996154785156\n",
      "epoch:7,batch:50 loss:1.0897620916366577\n",
      "epoch:7,batch:51 loss:1.1108558177947998\n",
      "epoch:7,batch:52 loss:1.1030433177947998\n",
      "epoch:7,batch:53 loss:1.1092933416366577\n",
      "epoch:7,batch:54 loss:1.0975745916366577\n",
      "epoch:7,batch:55 loss:1.0842933654785156\n",
      "epoch:7,batch:56 loss:1.0967934131622314\n",
      "epoch:7,batch:57 loss:1.102262258529663\n",
      "epoch:7,batch:58 loss:1.124137043952942\n",
      "epoch:7,batch:59 loss:1.1077308654785156\n",
      "epoch:7,batch:60 loss:1.1210120916366577\n",
      "epoch:7,batch:61 loss:1.1077308654785156\n",
      "epoch:7,batch:62 loss:1.1053870916366577\n",
      "epoch:7,batch:63 loss:1.0999183654785156\n",
      "epoch:7,batch:64 loss:1.0819495916366577\n",
      "epoch:7,batch:65 loss:1.0952308177947998\n",
      "epoch:7,batch:66 loss:1.1038246154785156\n",
      "epoch:7,batch:67 loss:1.0897620916366577\n",
      "epoch:7,batch:68 loss:1.1225745677947998\n",
      "epoch:7,batch:69 loss:1.1014808416366577\n",
      "epoch:7,batch:70 loss:1.094449520111084\n",
      "epoch:7,batch:71 loss:1.1030433177947998\n",
      "epoch:7,batch:72 loss:1.0663245916366577\n",
      "epoch:7,batch:73 loss:1.1100746393203735\n",
      "epoch:7,batch:74 loss:1.1108558177947998\n",
      "epoch:7,batch:75 loss:1.0881996154785156\n",
      "epoch:7,batch:76 loss:1.0991370677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:47<09:28, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7,batch:77 loss:1.1014809608459473\n",
      "epoch:7, train_loss:1.1014809608459473\n",
      "epoch:8,batch:0 loss:1.1225746870040894\n",
      "epoch:8,batch:1 loss:1.1116371154785156\n",
      "epoch:8,batch:2 loss:1.110074520111084\n",
      "epoch:8,batch:3 loss:1.0999183654785156\n",
      "epoch:8,batch:4 loss:1.1085121631622314\n",
      "epoch:8,batch:5 loss:1.0842933654785156\n",
      "epoch:8,batch:6 loss:1.1210120916366577\n",
      "epoch:8,batch:7 loss:1.1038246154785156\n",
      "epoch:8,batch:8 loss:1.0921058654785156\n",
      "epoch:8,batch:9 loss:1.110074520111084\n",
      "epoch:8,batch:10 loss:1.0881996154785156\n",
      "epoch:8,batch:11 loss:1.096011996269226\n",
      "epoch:8,batch:12 loss:1.0678870677947998\n",
      "epoch:8,batch:13 loss:1.0975745916366577\n",
      "epoch:8,batch:14 loss:1.096793293952942\n",
      "epoch:8,batch:15 loss:1.0858557224273682\n",
      "epoch:8,batch:16 loss:1.1030433177947998\n",
      "epoch:8,batch:17 loss:1.1092934608459473\n",
      "epoch:8,batch:18 loss:1.0921058654785156\n",
      "epoch:8,batch:19 loss:1.0897620916366577\n",
      "epoch:8,batch:20 loss:1.1077308654785156\n",
      "epoch:8,batch:21 loss:1.0858558416366577\n",
      "epoch:8,batch:22 loss:1.0874183177947998\n",
      "epoch:8,batch:23 loss:1.113980770111084\n",
      "epoch:8,batch:24 loss:1.092105746269226\n",
      "epoch:8,batch:25 loss:1.0678871870040894\n",
      "epoch:8,batch:26 loss:1.107730746269226\n",
      "epoch:8,batch:27 loss:1.112418293952942\n",
      "epoch:8,batch:28 loss:1.1038246154785156\n",
      "epoch:8,batch:29 loss:1.1069495677947998\n",
      "epoch:8,batch:30 loss:1.1030433177947998\n",
      "epoch:8,batch:31 loss:1.113980770111084\n",
      "epoch:8,batch:32 loss:1.1116371154785156\n",
      "epoch:8,batch:33 loss:1.0913245677947998\n",
      "epoch:8,batch:34 loss:1.1131994724273682\n",
      "epoch:8,batch:35 loss:1.102262020111084\n",
      "epoch:8,batch:36 loss:1.1131995916366577\n",
      "epoch:8,batch:37 loss:1.0803871154785156\n",
      "epoch:8,batch:38 loss:1.1030434370040894\n",
      "epoch:8,batch:39 loss:1.1069495677947998\n",
      "epoch:8,batch:40 loss:1.0858558416366577\n",
      "epoch:8,batch:41 loss:1.0913245677947998\n",
      "epoch:8,batch:42 loss:1.099918246269226\n",
      "epoch:8,batch:43 loss:1.1038246154785156\n",
      "epoch:8,batch:44 loss:1.1038246154785156\n",
      "epoch:8,batch:45 loss:1.102262020111084\n",
      "epoch:8,batch:46 loss:1.113980770111084\n",
      "epoch:8,batch:47 loss:1.0905433893203735\n",
      "epoch:8,batch:48 loss:1.1202309131622314\n",
      "epoch:8,batch:49 loss:1.096793293952942\n",
      "epoch:8,batch:50 loss:1.0960121154785156\n",
      "epoch:8,batch:51 loss:1.1131994724273682\n",
      "epoch:8,batch:52 loss:1.1038246154785156\n",
      "epoch:8,batch:53 loss:1.1006996631622314\n",
      "epoch:8,batch:54 loss:1.092887043952942\n",
      "epoch:8,batch:55 loss:1.1053872108459473\n",
      "epoch:8,batch:56 loss:1.1264808177947998\n",
      "epoch:8,batch:57 loss:1.1108558177947998\n",
      "epoch:8,batch:58 loss:1.0960121154785156\n",
      "epoch:8,batch:59 loss:1.1186683177947998\n",
      "epoch:8,batch:60 loss:1.1186683177947998\n",
      "epoch:8,batch:61 loss:1.0897620916366577\n",
      "epoch:8,batch:62 loss:1.0913245677947998\n",
      "epoch:8,batch:63 loss:1.1061683893203735\n",
      "epoch:8,batch:64 loss:1.102262020111084\n",
      "epoch:8,batch:65 loss:1.092887043952942\n",
      "epoch:8,batch:66 loss:1.1069495677947998\n",
      "epoch:8,batch:67 loss:1.0921058654785156\n",
      "epoch:8,batch:68 loss:1.0897619724273682\n",
      "epoch:8,batch:69 loss:1.1131995916366577\n",
      "epoch:8,batch:70 loss:1.0592933893203735\n",
      "epoch:8,batch:71 loss:1.1038246154785156\n",
      "epoch:8,batch:72 loss:1.096793293952942\n",
      "epoch:8,batch:73 loss:1.098355770111084\n",
      "epoch:8,batch:74 loss:1.094449520111084\n",
      "epoch:8,batch:75 loss:1.1155433654785156\n",
      "epoch:8,batch:76 loss:1.0913245677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:00<09:07, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8,batch:77 loss:1.0874184370040894\n",
      "epoch:8, train_loss:1.0874184370040894\n",
      "epoch:9,batch:0 loss:1.1022621393203735\n",
      "epoch:9,batch:1 loss:1.094449520111084\n",
      "epoch:9,batch:2 loss:1.1038246154785156\n",
      "epoch:9,batch:3 loss:1.0999183654785156\n",
      "epoch:9,batch:4 loss:1.1210122108459473\n",
      "epoch:9,batch:5 loss:1.0991370677947998\n",
      "epoch:9,batch:6 loss:1.0952309370040894\n",
      "epoch:9,batch:7 loss:1.112418293952942\n",
      "epoch:9,batch:8 loss:1.0803871154785156\n",
      "epoch:9,batch:9 loss:1.1210122108459473\n",
      "epoch:9,batch:10 loss:1.1046059131622314\n",
      "epoch:9,batch:11 loss:1.086637020111084\n",
      "epoch:9,batch:12 loss:1.1038246154785156\n",
      "epoch:9,batch:13 loss:1.0960121154785156\n",
      "epoch:9,batch:14 loss:1.1264808177947998\n",
      "epoch:9,batch:15 loss:1.112418293952942\n",
      "epoch:9,batch:16 loss:1.1077308654785156\n",
      "epoch:9,batch:17 loss:1.0772621631622314\n",
      "epoch:9,batch:18 loss:1.096793293952942\n",
      "epoch:9,batch:19 loss:1.0874184370040894\n",
      "epoch:9,batch:20 loss:1.0936683416366577\n",
      "epoch:9,batch:21 loss:1.0952308177947998\n",
      "epoch:9,batch:22 loss:1.0999183654785156\n",
      "epoch:9,batch:23 loss:1.1147620677947998\n",
      "epoch:9,batch:24 loss:1.092887043952942\n",
      "epoch:9,batch:25 loss:1.0819495916366577\n",
      "epoch:9,batch:26 loss:1.0936683416366577\n",
      "epoch:9,batch:27 loss:1.0967934131622314\n",
      "epoch:9,batch:28 loss:1.0967934131622314\n",
      "epoch:9,batch:29 loss:1.1131994724273682\n",
      "epoch:9,batch:30 loss:1.1100746393203735\n",
      "epoch:9,batch:31 loss:1.104605793952942\n",
      "epoch:9,batch:32 loss:1.1014808416366577\n",
      "epoch:9,batch:33 loss:1.0819497108459473\n",
      "epoch:9,batch:34 loss:1.1077308654785156\n",
      "epoch:9,batch:35 loss:1.1092933416366577\n",
      "epoch:9,batch:36 loss:1.104605793952942\n",
      "epoch:9,batch:37 loss:1.1053870916366577\n",
      "epoch:9,batch:38 loss:1.1116371154785156\n",
      "epoch:9,batch:39 loss:1.0842933654785156\n",
      "epoch:9,batch:40 loss:1.0975745916366577\n",
      "epoch:9,batch:41 loss:1.1171058416366577\n",
      "epoch:9,batch:42 loss:1.1147620677947998\n",
      "epoch:9,batch:43 loss:1.096011996269226\n",
      "epoch:9,batch:44 loss:1.1092934608459473\n",
      "epoch:9,batch:45 loss:1.0881996154785156\n",
      "epoch:9,batch:46 loss:1.1467933654785156\n",
      "epoch:9,batch:47 loss:1.1077308654785156\n",
      "epoch:9,batch:48 loss:1.098355770111084\n",
      "epoch:9,batch:49 loss:1.0999183654785156\n",
      "epoch:9,batch:50 loss:1.0803871154785156\n",
      "epoch:9,batch:51 loss:1.0881996154785156\n",
      "epoch:9,batch:52 loss:1.085074543952942\n",
      "epoch:9,batch:53 loss:1.1350746154785156\n",
      "epoch:9,batch:54 loss:1.1053869724273682\n",
      "epoch:9,batch:55 loss:1.0764808654785156\n",
      "epoch:9,batch:56 loss:1.0913245677947998\n",
      "epoch:9,batch:57 loss:1.0905433893203735\n",
      "epoch:9,batch:58 loss:1.0921058654785156\n",
      "epoch:9,batch:59 loss:1.1124184131622314\n",
      "epoch:9,batch:60 loss:1.0999183654785156\n",
      "epoch:9,batch:61 loss:1.0960121154785156\n",
      "epoch:9,batch:62 loss:1.0694496631622314\n",
      "epoch:9,batch:63 loss:1.0874183177947998\n",
      "epoch:9,batch:64 loss:1.0983558893203735\n",
      "epoch:9,batch:65 loss:1.1069495677947998\n",
      "epoch:9,batch:66 loss:1.0975744724273682\n",
      "epoch:9,batch:67 loss:1.1116371154785156\n",
      "epoch:9,batch:68 loss:1.0835120677947998\n",
      "epoch:9,batch:69 loss:1.0889806747436523\n",
      "epoch:9,batch:70 loss:1.1186683177947998\n",
      "epoch:9,batch:71 loss:1.112418293952942\n",
      "epoch:9,batch:72 loss:1.1131995916366577\n",
      "epoch:9,batch:73 loss:1.0803871154785156\n",
      "epoch:9,batch:74 loss:1.1186683177947998\n",
      "epoch:9,batch:75 loss:1.080386996269226\n",
      "epoch:9,batch:76 loss:1.1030433177947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:14<08:51, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9,batch:77 loss:1.1006996631622314\n",
      "epoch:9, train_loss:1.1006996631622314\n",
      "epoch:10,batch:0 loss:1.1061683893203735\n",
      "epoch:10,batch:1 loss:1.074918270111084\n",
      "epoch:10,batch:2 loss:1.1155433654785156\n",
      "epoch:10,batch:3 loss:1.1194496154785156\n",
      "epoch:10,batch:4 loss:1.088980793952942\n",
      "epoch:10,batch:5 loss:1.090543270111084\n",
      "epoch:10,batch:6 loss:1.102262020111084\n",
      "epoch:10,batch:7 loss:1.096793293952942\n",
      "epoch:10,batch:8 loss:1.1077308654785156\n",
      "epoch:10,batch:9 loss:1.092887043952942\n",
      "epoch:10,batch:10 loss:1.1014808416366577\n",
      "epoch:10,batch:11 loss:1.0999183654785156\n",
      "epoch:10,batch:12 loss:1.0866371393203735\n",
      "epoch:10,batch:13 loss:1.1116371154785156\n",
      "epoch:10,batch:14 loss:1.0780433416366577\n",
      "epoch:10,batch:15 loss:1.102262020111084\n",
      "epoch:10,batch:16 loss:1.1014808416366577\n",
      "epoch:10,batch:17 loss:1.0897620916366577\n",
      "epoch:10,batch:18 loss:1.1014807224273682\n",
      "epoch:10,batch:19 loss:1.0553871393203735\n",
      "epoch:10,batch:20 loss:1.082730770111084\n",
      "epoch:10,batch:21 loss:1.1147620677947998\n",
      "epoch:10,batch:22 loss:1.081168293952942\n",
      "epoch:10,batch:23 loss:1.0819495916366577\n",
      "epoch:10,batch:24 loss:1.1092933416366577\n",
      "epoch:10,batch:25 loss:1.1131995916366577\n",
      "epoch:10,batch:26 loss:1.0991370677947998\n",
      "epoch:10,batch:27 loss:1.0999183654785156\n",
      "epoch:10,batch:28 loss:1.110074520111084\n",
      "epoch:10,batch:29 loss:1.1178871393203735\n",
      "epoch:10,batch:30 loss:1.1085121631622314\n",
      "epoch:10,batch:31 loss:1.0991370677947998\n",
      "epoch:10,batch:32 loss:1.1069496870040894\n",
      "epoch:10,batch:33 loss:1.1147620677947998\n",
      "epoch:10,batch:34 loss:1.0858558416366577\n",
      "epoch:10,batch:35 loss:1.1210122108459473\n",
      "epoch:10,batch:36 loss:1.121793270111084\n",
      "epoch:10,batch:37 loss:1.0756995677947998\n",
      "epoch:10,batch:38 loss:1.1116371154785156\n",
      "epoch:10,batch:39 loss:1.098355770111084\n",
      "epoch:10,batch:40 loss:1.0850746631622314\n",
      "epoch:10,batch:41 loss:1.123355746269226\n",
      "epoch:10,batch:42 loss:1.1155433654785156\n",
      "epoch:10,batch:43 loss:1.1046059131622314\n",
      "epoch:10,batch:44 loss:1.1288245916366577\n",
      "epoch:10,batch:45 loss:1.0881996154785156\n",
      "epoch:10,batch:46 loss:1.1116371154785156\n",
      "epoch:10,batch:47 loss:1.0913245677947998\n",
      "epoch:10,batch:48 loss:1.092887043952942\n",
      "epoch:10,batch:49 loss:1.0874184370040894\n",
      "epoch:10,batch:50 loss:1.1147620677947998\n",
      "epoch:10,batch:51 loss:1.0999183654785156\n",
      "epoch:10,batch:52 loss:1.0944496393203735\n",
      "epoch:10,batch:53 loss:1.0796058177947998\n",
      "epoch:10,batch:54 loss:1.116324543952942\n",
      "epoch:10,batch:55 loss:1.1225745677947998\n",
      "epoch:10,batch:56 loss:1.0975747108459473\n",
      "epoch:10,batch:57 loss:1.077262043952942\n",
      "epoch:10,batch:58 loss:1.086637020111084\n",
      "epoch:10,batch:59 loss:1.0842933654785156\n",
      "epoch:10,batch:60 loss:1.1100746393203735\n",
      "epoch:10,batch:61 loss:1.1014808416366577\n",
      "epoch:10,batch:62 loss:1.0913245677947998\n",
      "epoch:10,batch:63 loss:1.0788246393203735\n",
      "epoch:10,batch:64 loss:1.1100746393203735\n",
      "epoch:10,batch:65 loss:1.1210120916366577\n",
      "epoch:10,batch:66 loss:1.112418293952942\n",
      "epoch:10,batch:67 loss:1.1053870916366577\n",
      "epoch:10,batch:68 loss:1.115543246269226\n",
      "epoch:10,batch:69 loss:1.0819497108459473\n",
      "epoch:10,batch:70 loss:1.104605793952942\n",
      "epoch:10,batch:71 loss:1.100699543952942\n",
      "epoch:10,batch:72 loss:1.1171058416366577\n",
      "epoch:10,batch:73 loss:1.0913245677947998\n",
      "epoch:10,batch:74 loss:1.110074520111084\n",
      "epoch:10,batch:75 loss:1.1006996631622314\n",
      "epoch:10,batch:76 loss:1.1061683893203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:27<08:38, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10,batch:77 loss:1.0991370677947998\n",
      "epoch:10, train_loss:1.0991370677947998\n",
      "epoch:11,batch:0 loss:1.1171059608459473\n",
      "epoch:11,batch:1 loss:1.0889809131622314\n",
      "epoch:11,batch:2 loss:1.092105746269226\n",
      "epoch:11,batch:3 loss:1.0999183654785156\n",
      "epoch:11,batch:4 loss:1.0975745916366577\n",
      "epoch:11,batch:5 loss:1.1053870916366577\n",
      "epoch:11,batch:6 loss:1.096793293952942\n",
      "epoch:11,batch:7 loss:1.0999183654785156\n",
      "epoch:11,batch:8 loss:1.1194496154785156\n",
      "epoch:11,batch:9 loss:1.110074520111084\n",
      "epoch:11,batch:10 loss:1.0764808654785156\n",
      "epoch:11,batch:11 loss:1.1085121631622314\n",
      "epoch:11,batch:12 loss:1.0983558893203735\n",
      "epoch:11,batch:13 loss:1.099918246269226\n",
      "epoch:11,batch:14 loss:1.1092933416366577\n",
      "epoch:11,batch:15 loss:1.0780433416366577\n",
      "epoch:11,batch:16 loss:1.092887043952942\n",
      "epoch:11,batch:17 loss:1.0967934131622314\n",
      "epoch:11,batch:18 loss:1.0842933654785156\n",
      "epoch:11,batch:19 loss:1.0858558416366577\n",
      "epoch:11,batch:20 loss:1.0881996154785156\n",
      "epoch:11,batch:21 loss:1.1155433654785156\n",
      "epoch:11,batch:22 loss:1.1038246154785156\n",
      "epoch:11,batch:23 loss:1.1288244724273682\n",
      "epoch:11,batch:24 loss:1.1006996631622314\n",
      "epoch:11,batch:25 loss:1.0889809131622314\n",
      "epoch:11,batch:26 loss:1.0952308177947998\n",
      "epoch:11,batch:27 loss:1.100699543952942\n",
      "epoch:11,batch:28 loss:1.0936683416366577\n",
      "epoch:11,batch:29 loss:1.0999183654785156\n",
      "epoch:11,batch:30 loss:1.1186683177947998\n",
      "epoch:11,batch:31 loss:1.0952308177947998\n",
      "epoch:11,batch:32 loss:1.0936683416366577\n",
      "epoch:11,batch:33 loss:1.0975744724273682\n",
      "epoch:11,batch:34 loss:1.0983558893203735\n",
      "epoch:11,batch:35 loss:1.1210120916366577\n",
      "epoch:11,batch:36 loss:1.1163246631622314\n",
      "epoch:11,batch:37 loss:1.108512043952942\n",
      "epoch:11,batch:38 loss:1.1092933416366577\n",
      "epoch:11,batch:39 loss:1.0889809131622314\n",
      "epoch:11,batch:40 loss:1.0866371393203735\n",
      "epoch:11,batch:41 loss:1.0999183654785156\n",
      "epoch:11,batch:42 loss:1.1131995916366577\n",
      "epoch:11,batch:43 loss:1.1108558177947998\n",
      "epoch:11,batch:44 loss:1.110074520111084\n",
      "epoch:11,batch:45 loss:1.0967934131622314\n",
      "epoch:11,batch:46 loss:1.0991370677947998\n",
      "epoch:11,batch:47 loss:1.100699543952942\n",
      "epoch:11,batch:48 loss:1.1053869724273682\n",
      "epoch:11,batch:49 loss:1.081168293952942\n",
      "epoch:11,batch:50 loss:1.104605793952942\n",
      "epoch:11,batch:51 loss:1.0960121154785156\n",
      "epoch:11,batch:52 loss:1.0733559131622314\n",
      "epoch:11,batch:53 loss:1.0999183654785156\n",
      "epoch:11,batch:54 loss:1.1014808416366577\n",
      "epoch:11,batch:55 loss:1.096793293952942\n",
      "epoch:11,batch:56 loss:1.1116371154785156\n",
      "epoch:11,batch:57 loss:1.112418293952942\n",
      "epoch:11,batch:58 loss:1.0842933654785156\n",
      "epoch:11,batch:59 loss:1.1155433654785156\n",
      "epoch:11,batch:60 loss:1.078824520111084\n",
      "epoch:11,batch:61 loss:1.0936683416366577\n",
      "epoch:11,batch:62 loss:1.1178871393203735\n",
      "epoch:11,batch:63 loss:1.0874183177947998\n",
      "epoch:11,batch:64 loss:1.096793293952942\n",
      "epoch:11,batch:65 loss:1.088980793952942\n",
      "epoch:11,batch:66 loss:1.1077308654785156\n",
      "epoch:11,batch:67 loss:1.1100746393203735\n",
      "epoch:11,batch:68 loss:1.090543270111084\n",
      "epoch:11,batch:69 loss:1.1085121631622314\n",
      "epoch:11,batch:70 loss:1.0999183654785156\n",
      "epoch:11,batch:71 loss:1.0850746631622314\n",
      "epoch:11,batch:72 loss:1.0983558893203735\n",
      "epoch:11,batch:73 loss:1.1006996631622314\n",
      "epoch:11,batch:74 loss:1.102262020111084\n",
      "epoch:11,batch:75 loss:1.1053870916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [02:40<08:24, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11,batch:76 loss:1.137418270111084\n",
      "epoch:11,batch:77 loss:1.0936682224273682\n",
      "epoch:11, train_loss:1.0936682224273682\n",
      "epoch:12,batch:0 loss:1.1155433654785156\n",
      "epoch:12,batch:1 loss:1.0991370677947998\n",
      "epoch:12,batch:2 loss:1.113980770111084\n",
      "epoch:12,batch:3 loss:1.096793293952942\n",
      "epoch:12,batch:4 loss:1.1186683177947998\n",
      "epoch:12,batch:5 loss:1.1124184131622314\n",
      "epoch:12,batch:6 loss:1.1108558177947998\n",
      "epoch:12,batch:7 loss:1.1030433177947998\n",
      "epoch:12,batch:8 loss:1.0796058177947998\n",
      "epoch:12,batch:9 loss:1.094449520111084\n",
      "epoch:12,batch:10 loss:1.107730746269226\n",
      "epoch:12,batch:11 loss:1.1046059131622314\n",
      "epoch:12,batch:12 loss:1.0999183654785156\n",
      "epoch:12,batch:13 loss:1.108512043952942\n",
      "epoch:12,batch:14 loss:1.1217933893203735\n",
      "epoch:12,batch:15 loss:1.1163246631622314\n",
      "epoch:12,batch:16 loss:1.1171058416366577\n",
      "epoch:12,batch:17 loss:1.0881996154785156\n",
      "epoch:12,batch:18 loss:1.0842933654785156\n",
      "epoch:12,batch:19 loss:1.1225746870040894\n",
      "epoch:12,batch:20 loss:1.1053870916366577\n",
      "epoch:12,batch:21 loss:1.0827308893203735\n",
      "epoch:12,batch:22 loss:1.0921058654785156\n",
      "epoch:12,batch:23 loss:1.0842933654785156\n",
      "epoch:12,batch:24 loss:1.1014809608459473\n",
      "epoch:12,batch:25 loss:1.1092933416366577\n",
      "epoch:12,batch:26 loss:1.082730770111084\n",
      "epoch:12,batch:27 loss:1.0952306985855103\n",
      "epoch:12,batch:28 loss:1.1124184131622314\n",
      "epoch:12,batch:29 loss:1.104605793952942\n",
      "epoch:12,batch:30 loss:1.1014808416366577\n",
      "epoch:12,batch:31 loss:1.0967934131622314\n",
      "epoch:12,batch:32 loss:1.0999183654785156\n",
      "epoch:12,batch:33 loss:1.1030433177947998\n",
      "epoch:12,batch:34 loss:1.1171058416366577\n",
      "epoch:12,batch:35 loss:1.094449520111084\n",
      "epoch:12,batch:36 loss:1.1108558177947998\n",
      "epoch:12,batch:37 loss:1.096793293952942\n",
      "epoch:12,batch:38 loss:1.098355770111084\n",
      "epoch:12,batch:39 loss:1.0858558416366577\n",
      "epoch:12,batch:40 loss:1.0936684608459473\n",
      "epoch:12,batch:41 loss:1.1014808416366577\n",
      "epoch:12,batch:42 loss:1.0858558416366577\n",
      "epoch:12,batch:43 loss:1.100699543952942\n",
      "epoch:12,batch:44 loss:1.1069495677947998\n",
      "epoch:12,batch:45 loss:1.0928871631622314\n",
      "epoch:12,batch:46 loss:1.0991370677947998\n",
      "epoch:12,batch:47 loss:1.1092933416366577\n",
      "epoch:12,batch:48 loss:1.0842933654785156\n",
      "epoch:12,batch:49 loss:1.0960121154785156\n",
      "epoch:12,batch:50 loss:1.104605793952942\n",
      "epoch:12,batch:51 loss:1.1069495677947998\n",
      "epoch:12,batch:52 loss:1.0960121154785156\n",
      "epoch:12,batch:53 loss:1.0991371870040894\n",
      "epoch:12,batch:54 loss:1.098355770111084\n",
      "epoch:12,batch:55 loss:1.121793270111084\n",
      "epoch:12,batch:56 loss:1.0983558893203735\n",
      "epoch:12,batch:57 loss:1.1077308654785156\n",
      "epoch:12,batch:58 loss:1.1186683177947998\n",
      "epoch:12,batch:59 loss:1.1046059131622314\n",
      "epoch:12,batch:60 loss:1.0983558893203735\n",
      "epoch:12,batch:61 loss:1.1155433654785156\n",
      "epoch:12,batch:62 loss:1.082730770111084\n",
      "epoch:12,batch:63 loss:1.0819497108459473\n",
      "epoch:12,batch:64 loss:1.0835120677947998\n",
      "epoch:12,batch:65 loss:1.106168270111084\n",
      "epoch:12,batch:66 loss:1.0960121154785156\n",
      "epoch:12,batch:67 loss:1.1053870916366577\n",
      "epoch:12,batch:68 loss:1.1131997108459473\n",
      "epoch:12,batch:69 loss:1.0874183177947998\n",
      "epoch:12,batch:70 loss:1.0796058177947998\n",
      "epoch:12,batch:71 loss:1.092105746269226\n",
      "epoch:12,batch:72 loss:1.0975747108459473\n",
      "epoch:12,batch:73 loss:1.0936683416366577\n",
      "epoch:12,batch:74 loss:1.1155433654785156\n",
      "epoch:12,batch:75 loss:1.0913245677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [02:53<08:08, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12,batch:76 loss:1.0928871631622314\n",
      "epoch:12,batch:77 loss:1.0780433416366577\n",
      "epoch:12, train_loss:1.0780433416366577\n",
      "epoch:13,batch:0 loss:1.1100746393203735\n",
      "epoch:13,batch:1 loss:1.1116371154785156\n",
      "epoch:13,batch:2 loss:1.092887043952942\n",
      "epoch:13,batch:3 loss:1.121793270111084\n",
      "epoch:13,batch:4 loss:1.0960121154785156\n",
      "epoch:13,batch:5 loss:1.0936683416366577\n",
      "epoch:13,batch:6 loss:1.106168270111084\n",
      "epoch:13,batch:7 loss:1.0733559131622314\n",
      "epoch:13,batch:8 loss:1.092887043952942\n",
      "epoch:13,batch:9 loss:1.1069495677947998\n",
      "epoch:13,batch:10 loss:1.0858558416366577\n",
      "epoch:13,batch:11 loss:1.104605793952942\n",
      "epoch:13,batch:12 loss:1.1053870916366577\n",
      "epoch:13,batch:13 loss:1.1210120916366577\n",
      "epoch:13,batch:14 loss:1.0858559608459473\n",
      "epoch:13,batch:15 loss:1.0913245677947998\n",
      "epoch:13,batch:16 loss:1.0835121870040894\n",
      "epoch:13,batch:17 loss:1.098355770111084\n",
      "epoch:13,batch:18 loss:1.1092933416366577\n",
      "epoch:13,batch:19 loss:1.1022621393203735\n",
      "epoch:13,batch:20 loss:1.1147620677947998\n",
      "epoch:13,batch:21 loss:1.0842933654785156\n",
      "epoch:13,batch:22 loss:1.0780433416366577\n",
      "epoch:13,batch:23 loss:1.0991371870040894\n",
      "epoch:13,batch:24 loss:1.0780433416366577\n",
      "epoch:13,batch:25 loss:1.1217933893203735\n",
      "epoch:13,batch:26 loss:1.1264806985855103\n",
      "epoch:13,batch:27 loss:1.1077308654785156\n",
      "epoch:13,batch:28 loss:1.0756995677947998\n",
      "epoch:13,batch:29 loss:1.1030434370040894\n",
      "epoch:13,batch:30 loss:1.0913246870040894\n",
      "epoch:13,batch:31 loss:1.1124184131622314\n",
      "epoch:13,batch:32 loss:1.1085121631622314\n",
      "epoch:13,batch:33 loss:1.1147621870040894\n",
      "epoch:13,batch:34 loss:1.1030433177947998\n",
      "epoch:13,batch:35 loss:1.0811684131622314\n",
      "epoch:13,batch:36 loss:1.0717933177947998\n",
      "epoch:13,batch:37 loss:1.1202309131622314\n",
      "epoch:13,batch:38 loss:1.0811684131622314\n",
      "epoch:13,batch:39 loss:1.090543270111084\n",
      "epoch:13,batch:40 loss:1.100699543952942\n",
      "epoch:13,batch:41 loss:1.1053870916366577\n",
      "epoch:13,batch:42 loss:1.1241371631622314\n",
      "epoch:13,batch:43 loss:1.1077308654785156\n",
      "epoch:13,batch:44 loss:1.108512043952942\n",
      "epoch:13,batch:45 loss:1.0686683654785156\n",
      "epoch:13,batch:46 loss:1.0819494724273682\n",
      "epoch:13,batch:47 loss:1.092887043952942\n",
      "epoch:13,batch:48 loss:1.0944496393203735\n",
      "epoch:13,batch:49 loss:1.1155433654785156\n",
      "epoch:13,batch:50 loss:1.0764808654785156\n",
      "epoch:13,batch:51 loss:1.1116371154785156\n",
      "epoch:13,batch:52 loss:1.103824496269226\n",
      "epoch:13,batch:53 loss:1.1053870916366577\n",
      "epoch:13,batch:54 loss:1.1264808177947998\n",
      "epoch:13,batch:55 loss:1.1249184608459473\n",
      "epoch:13,batch:56 loss:1.116324543952942\n",
      "epoch:13,batch:57 loss:1.092887043952942\n",
      "epoch:13,batch:58 loss:1.0710121393203735\n",
      "epoch:13,batch:59 loss:1.1053870916366577\n",
      "epoch:13,batch:60 loss:1.080386996269226\n",
      "epoch:13,batch:61 loss:1.086637020111084\n",
      "epoch:13,batch:62 loss:1.0944496393203735\n",
      "epoch:13,batch:63 loss:1.1311683654785156\n",
      "epoch:13,batch:64 loss:1.0944496393203735\n",
      "epoch:13,batch:65 loss:1.0858558416366577\n",
      "epoch:13,batch:66 loss:1.1092933416366577\n",
      "epoch:13,batch:67 loss:1.092105746269226\n",
      "epoch:13,batch:68 loss:1.1210120916366577\n",
      "epoch:13,batch:69 loss:1.0921058654785156\n",
      "epoch:13,batch:70 loss:1.0999183654785156\n",
      "epoch:13,batch:71 loss:1.108512043952942\n",
      "epoch:13,batch:72 loss:1.1124184131622314\n",
      "epoch:13,batch:73 loss:1.1178871393203735\n",
      "epoch:13,batch:74 loss:1.104605793952942\n",
      "epoch:13,batch:75 loss:1.1053869724273682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:06<07:51, 13.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13,batch:76 loss:1.1155433654785156\n",
      "epoch:13,batch:77 loss:1.0881996154785156\n",
      "epoch:13, train_loss:1.0881996154785156\n",
      "epoch:14,batch:0 loss:1.1139808893203735\n",
      "epoch:14,batch:1 loss:1.1085121631622314\n",
      "epoch:14,batch:2 loss:1.0819494724273682\n",
      "epoch:14,batch:3 loss:1.0975745916366577\n",
      "epoch:14,batch:4 loss:1.1053870916366577\n",
      "epoch:14,batch:5 loss:1.1061683893203735\n",
      "epoch:14,batch:6 loss:1.1249183416366577\n",
      "epoch:14,batch:7 loss:1.1350746154785156\n",
      "epoch:14,batch:8 loss:1.1155433654785156\n",
      "epoch:14,batch:9 loss:1.1147620677947998\n",
      "epoch:14,batch:10 loss:1.094449520111084\n",
      "epoch:14,batch:11 loss:1.1022621393203735\n",
      "epoch:14,batch:12 loss:1.1405434608459473\n",
      "epoch:14,batch:13 loss:1.0913245677947998\n",
      "epoch:14,batch:14 loss:1.0569496154785156\n",
      "epoch:14,batch:15 loss:1.1038246154785156\n",
      "epoch:14,batch:16 loss:1.0897622108459473\n",
      "epoch:14,batch:17 loss:1.096793293952942\n",
      "epoch:14,batch:18 loss:1.1046059131622314\n",
      "epoch:14,batch:19 loss:1.0913245677947998\n",
      "epoch:14,batch:20 loss:1.1085121631622314\n",
      "epoch:14,batch:21 loss:1.1210122108459473\n",
      "epoch:14,batch:22 loss:1.104605793952942\n",
      "epoch:14,batch:23 loss:1.088980793952942\n",
      "epoch:14,batch:24 loss:1.131949543952942\n",
      "epoch:14,batch:25 loss:1.086637020111084\n",
      "epoch:14,batch:26 loss:1.1147620677947998\n",
      "epoch:14,batch:27 loss:1.098355770111084\n",
      "epoch:14,batch:28 loss:1.078824520111084\n",
      "epoch:14,batch:29 loss:1.1053872108459473\n",
      "epoch:14,batch:30 loss:1.069449543952942\n",
      "epoch:14,batch:31 loss:1.1069495677947998\n",
      "epoch:14,batch:32 loss:1.0991370677947998\n",
      "epoch:14,batch:33 loss:1.0975745916366577\n",
      "epoch:14,batch:34 loss:1.1046059131622314\n",
      "epoch:14,batch:35 loss:1.1077308654785156\n",
      "epoch:14,batch:36 loss:1.1053870916366577\n",
      "epoch:14,batch:37 loss:1.102262020111084\n",
      "epoch:14,batch:38 loss:1.1053870916366577\n",
      "epoch:14,batch:39 loss:1.1116371154785156\n",
      "epoch:14,batch:40 loss:1.0858558416366577\n",
      "epoch:14,batch:41 loss:1.1171059608459473\n",
      "epoch:14,batch:42 loss:1.0913245677947998\n",
      "epoch:14,batch:43 loss:1.0881997346878052\n",
      "epoch:14,batch:44 loss:1.0999183654785156\n",
      "epoch:14,batch:45 loss:1.0967934131622314\n",
      "epoch:14,batch:46 loss:1.096793293952942\n",
      "epoch:14,batch:47 loss:1.104605793952942\n",
      "epoch:14,batch:48 loss:1.0999183654785156\n",
      "epoch:14,batch:49 loss:1.0960121154785156\n",
      "epoch:14,batch:50 loss:1.096793293952942\n",
      "epoch:14,batch:51 loss:1.0569496154785156\n",
      "epoch:14,batch:52 loss:1.1030434370040894\n",
      "epoch:14,batch:53 loss:1.0936683416366577\n",
      "epoch:14,batch:54 loss:1.094449520111084\n",
      "epoch:14,batch:55 loss:1.1108558177947998\n",
      "epoch:14,batch:56 loss:1.0936682224273682\n",
      "epoch:14,batch:57 loss:1.0780433416366577\n",
      "epoch:14,batch:58 loss:1.0842933654785156\n",
      "epoch:14,batch:59 loss:1.1116371154785156\n",
      "epoch:14,batch:60 loss:1.059293270111084\n",
      "epoch:14,batch:61 loss:1.100699543952942\n",
      "epoch:14,batch:62 loss:1.117887020111084\n",
      "epoch:14,batch:63 loss:1.0936683416366577\n",
      "epoch:14,batch:64 loss:1.096793293952942\n",
      "epoch:14,batch:65 loss:1.0913245677947998\n",
      "epoch:14,batch:66 loss:1.120230793952942\n",
      "epoch:14,batch:67 loss:1.0991370677947998\n",
      "epoch:14,batch:68 loss:1.1116371154785156\n",
      "epoch:14,batch:69 loss:1.1202309131622314\n",
      "epoch:14,batch:70 loss:1.0952308177947998\n",
      "epoch:14,batch:71 loss:1.1069495677947998\n",
      "epoch:14,batch:72 loss:1.088980793952942\n",
      "epoch:14,batch:73 loss:1.1046059131622314\n",
      "epoch:14,batch:74 loss:1.1077308654785156\n",
      "epoch:14,batch:75 loss:1.113980770111084\n",
      "epoch:14,batch:76 loss:1.0866371393203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:20<07:47, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14,batch:77 loss:1.0952308177947998\n",
      "epoch:14, train_loss:1.0952308177947998\n",
      "epoch:15,batch:0 loss:1.1030433177947998\n",
      "epoch:15,batch:1 loss:1.112418293952942\n",
      "epoch:15,batch:2 loss:1.0944496393203735\n",
      "epoch:15,batch:3 loss:1.1077308654785156\n",
      "epoch:15,batch:4 loss:1.1053872108459473\n",
      "epoch:15,batch:5 loss:1.0842933654785156\n",
      "epoch:15,batch:6 loss:1.1131997108459473\n",
      "epoch:15,batch:7 loss:1.0874184370040894\n",
      "epoch:15,batch:8 loss:1.113980770111084\n",
      "epoch:15,batch:9 loss:1.1131995916366577\n",
      "epoch:15,batch:10 loss:1.0936684608459473\n",
      "epoch:15,batch:11 loss:1.0944496393203735\n",
      "epoch:15,batch:12 loss:1.0678870677947998\n",
      "epoch:15,batch:13 loss:1.0881996154785156\n",
      "epoch:15,batch:14 loss:1.1046059131622314\n",
      "epoch:15,batch:15 loss:1.0999183654785156\n",
      "epoch:15,batch:16 loss:1.1108558177947998\n",
      "epoch:15,batch:17 loss:1.1069495677947998\n",
      "epoch:15,batch:18 loss:1.092105746269226\n",
      "epoch:15,batch:19 loss:1.074918270111084\n",
      "epoch:15,batch:20 loss:1.1186684370040894\n",
      "epoch:15,batch:21 loss:1.096793293952942\n",
      "epoch:15,batch:22 loss:1.119449496269226\n",
      "epoch:15,batch:23 loss:1.0780433416366577\n",
      "epoch:15,batch:24 loss:1.1069495677947998\n",
      "epoch:15,batch:25 loss:1.0983558893203735\n",
      "epoch:15,batch:26 loss:1.096793293952942\n",
      "epoch:15,batch:27 loss:1.0913245677947998\n",
      "epoch:15,batch:28 loss:1.0913245677947998\n",
      "epoch:15,batch:29 loss:1.1069495677947998\n",
      "epoch:15,batch:30 loss:1.1061683893203735\n",
      "epoch:15,batch:31 loss:1.1186683177947998\n",
      "epoch:15,batch:32 loss:1.1194496154785156\n",
      "epoch:15,batch:33 loss:1.112418293952942\n",
      "epoch:15,batch:34 loss:1.1155433654785156\n",
      "epoch:15,batch:35 loss:1.0975747108459473\n",
      "epoch:15,batch:36 loss:1.086637020111084\n",
      "epoch:15,batch:37 loss:1.090543270111084\n",
      "epoch:15,batch:38 loss:1.0819497108459473\n",
      "epoch:15,batch:39 loss:1.0913246870040894\n",
      "epoch:15,batch:40 loss:1.1061683893203735\n",
      "epoch:15,batch:41 loss:1.1303870677947998\n",
      "epoch:15,batch:42 loss:1.0991370677947998\n",
      "epoch:15,batch:43 loss:1.0999183654785156\n",
      "epoch:15,batch:44 loss:1.1061683893203735\n",
      "epoch:15,batch:45 loss:1.116324543952942\n",
      "epoch:15,batch:46 loss:1.077262043952942\n",
      "epoch:15,batch:47 loss:1.1046059131622314\n",
      "epoch:15,batch:48 loss:1.0866371393203735\n",
      "epoch:15,batch:49 loss:1.1030434370040894\n",
      "epoch:15,batch:50 loss:1.1139808893203735\n",
      "epoch:15,batch:51 loss:1.0796058177947998\n",
      "epoch:15,batch:52 loss:1.124137043952942\n",
      "epoch:15,batch:53 loss:1.116324782371521\n",
      "epoch:15,batch:54 loss:1.112418293952942\n",
      "epoch:15,batch:55 loss:1.0991371870040894\n",
      "epoch:15,batch:56 loss:1.090543270111084\n",
      "epoch:15,batch:57 loss:1.1030434370040894\n",
      "epoch:15,batch:58 loss:1.0881996154785156\n",
      "epoch:15,batch:59 loss:1.085074543952942\n",
      "epoch:15,batch:60 loss:1.1155433654785156\n",
      "epoch:15,batch:61 loss:1.0881996154785156\n",
      "epoch:15,batch:62 loss:1.1163246631622314\n",
      "epoch:15,batch:63 loss:1.0991370677947998\n",
      "epoch:15,batch:64 loss:1.0936683416366577\n",
      "epoch:15,batch:65 loss:1.0819495916366577\n",
      "epoch:15,batch:66 loss:1.096011996269226\n",
      "epoch:15,batch:67 loss:1.113980770111084\n",
      "epoch:15,batch:68 loss:1.0936683416366577\n",
      "epoch:15,batch:69 loss:1.096793293952942\n",
      "epoch:15,batch:70 loss:1.120230793952942\n",
      "epoch:15,batch:71 loss:1.0780433416366577\n",
      "epoch:15,batch:72 loss:1.1030433177947998\n",
      "epoch:15,batch:73 loss:1.1053872108459473\n",
      "epoch:15,batch:74 loss:1.1077308654785156\n",
      "epoch:15,batch:75 loss:1.104605793952942\n",
      "epoch:15,batch:76 loss:1.088199496269226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:33<07:30, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15,batch:77 loss:1.0936683416366577\n",
      "epoch:15, train_loss:1.0936683416366577\n",
      "epoch:16,batch:0 loss:1.102262020111084\n",
      "epoch:16,batch:1 loss:1.1202309131622314\n",
      "epoch:16,batch:2 loss:1.1217933893203735\n",
      "epoch:16,batch:3 loss:1.0905433893203735\n",
      "epoch:16,batch:4 loss:1.1030433177947998\n",
      "epoch:16,batch:5 loss:1.0874183177947998\n",
      "epoch:16,batch:6 loss:1.1272621154785156\n",
      "epoch:16,batch:7 loss:1.1116371154785156\n",
      "epoch:16,batch:8 loss:1.0991370677947998\n",
      "epoch:16,batch:9 loss:1.0741370916366577\n",
      "epoch:16,batch:10 loss:1.1053870916366577\n",
      "epoch:16,batch:11 loss:1.0913245677947998\n",
      "epoch:16,batch:12 loss:1.110074520111084\n",
      "epoch:16,batch:13 loss:1.1038246154785156\n",
      "epoch:16,batch:14 loss:1.0756995677947998\n",
      "epoch:16,batch:15 loss:1.116324543952942\n",
      "epoch:16,batch:16 loss:1.1178871393203735\n",
      "epoch:16,batch:17 loss:1.0991371870040894\n",
      "epoch:16,batch:18 loss:1.1006996631622314\n",
      "epoch:16,batch:19 loss:1.1038246154785156\n",
      "epoch:16,batch:20 loss:1.1131995916366577\n",
      "epoch:16,batch:21 loss:1.1030433177947998\n",
      "epoch:16,batch:22 loss:1.1022621393203735\n",
      "epoch:16,batch:23 loss:1.1092932224273682\n",
      "epoch:16,batch:24 loss:1.1061683893203735\n",
      "epoch:16,batch:25 loss:1.086637020111084\n",
      "epoch:16,batch:26 loss:1.0655434131622314\n",
      "epoch:16,batch:27 loss:1.0897620916366577\n",
      "epoch:16,batch:28 loss:1.094449520111084\n",
      "epoch:16,batch:29 loss:1.1280434131622314\n",
      "epoch:16,batch:30 loss:1.0881996154785156\n",
      "epoch:16,batch:31 loss:1.0881996154785156\n",
      "epoch:16,batch:32 loss:1.1155433654785156\n",
      "epoch:16,batch:33 loss:1.1077308654785156\n",
      "epoch:16,batch:34 loss:1.0913246870040894\n",
      "epoch:16,batch:35 loss:1.1366369724273682\n",
      "epoch:16,batch:36 loss:1.110074520111084\n",
      "epoch:16,batch:37 loss:1.1210119724273682\n",
      "epoch:16,batch:38 loss:1.1022621393203735\n",
      "epoch:16,batch:39 loss:1.1108558177947998\n",
      "epoch:16,batch:40 loss:1.0921058654785156\n",
      "epoch:16,batch:41 loss:1.0835119485855103\n",
      "epoch:16,batch:42 loss:1.0967934131622314\n",
      "epoch:16,batch:43 loss:1.1116371154785156\n",
      "epoch:16,batch:44 loss:1.0999183654785156\n",
      "epoch:16,batch:45 loss:1.0858558416366577\n",
      "epoch:16,batch:46 loss:1.102262020111084\n",
      "epoch:16,batch:47 loss:1.0952309370040894\n",
      "epoch:16,batch:48 loss:1.0803871154785156\n",
      "epoch:16,batch:49 loss:1.099918246269226\n",
      "epoch:16,batch:50 loss:1.1038246154785156\n",
      "epoch:16,batch:51 loss:1.0960121154785156\n",
      "epoch:16,batch:52 loss:1.1108558177947998\n",
      "epoch:16,batch:53 loss:1.0913246870040894\n",
      "epoch:16,batch:54 loss:1.0975745916366577\n",
      "epoch:16,batch:55 loss:1.0913245677947998\n",
      "epoch:16,batch:56 loss:1.098355770111084\n",
      "epoch:16,batch:57 loss:1.1131995916366577\n",
      "epoch:16,batch:58 loss:1.1053870916366577\n",
      "epoch:16,batch:59 loss:1.096793293952942\n",
      "epoch:16,batch:60 loss:1.1014807224273682\n",
      "epoch:16,batch:61 loss:1.0811684131622314\n",
      "epoch:16,batch:62 loss:1.0913245677947998\n",
      "epoch:16,batch:63 loss:1.0803871154785156\n",
      "epoch:16,batch:64 loss:1.0975744724273682\n",
      "epoch:16,batch:65 loss:1.092887043952942\n",
      "epoch:16,batch:66 loss:1.0905433893203735\n",
      "epoch:16,batch:67 loss:1.1014808416366577\n",
      "epoch:16,batch:68 loss:1.1139808893203735\n",
      "epoch:16,batch:69 loss:1.1014809608459473\n",
      "epoch:16,batch:70 loss:1.0936683416366577\n",
      "epoch:16,batch:71 loss:1.1014808416366577\n",
      "epoch:16,batch:72 loss:1.1061683893203735\n",
      "epoch:16,batch:73 loss:1.0874183177947998\n",
      "epoch:16,batch:74 loss:1.1014809608459473\n",
      "epoch:16,batch:75 loss:1.1116371154785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [03:46<07:19, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16,batch:76 loss:1.1092933416366577\n",
      "epoch:16,batch:77 loss:1.0842933654785156\n",
      "epoch:16, train_loss:1.0842933654785156\n",
      "epoch:17,batch:0 loss:1.0944496393203735\n",
      "epoch:17,batch:1 loss:1.0944496393203735\n",
      "epoch:17,batch:2 loss:1.1069495677947998\n",
      "epoch:17,batch:3 loss:1.0842933654785156\n",
      "epoch:17,batch:4 loss:1.116324543952942\n",
      "epoch:17,batch:5 loss:1.106168270111084\n",
      "epoch:17,batch:6 loss:1.1061683893203735\n",
      "epoch:17,batch:7 loss:1.1085121631622314\n",
      "epoch:17,batch:8 loss:1.1178871393203735\n",
      "epoch:17,batch:9 loss:1.0991370677947998\n",
      "epoch:17,batch:10 loss:1.1288245916366577\n",
      "epoch:17,batch:11 loss:1.106168270111084\n",
      "epoch:17,batch:12 loss:1.082730770111084\n",
      "epoch:17,batch:13 loss:1.0897620916366577\n",
      "epoch:17,batch:14 loss:1.1092933416366577\n",
      "epoch:17,batch:15 loss:1.0897620916366577\n",
      "epoch:17,batch:16 loss:1.1053872108459473\n",
      "epoch:17,batch:17 loss:1.1225745677947998\n",
      "epoch:17,batch:18 loss:1.127261996269226\n",
      "epoch:17,batch:19 loss:1.1194496154785156\n",
      "epoch:17,batch:20 loss:1.1108558177947998\n",
      "epoch:17,batch:21 loss:1.1116371154785156\n",
      "epoch:17,batch:22 loss:1.0960121154785156\n",
      "epoch:17,batch:23 loss:1.113980770111084\n",
      "epoch:17,batch:24 loss:1.0983558893203735\n",
      "epoch:17,batch:25 loss:1.104605793952942\n",
      "epoch:17,batch:26 loss:1.115543246269226\n",
      "epoch:17,batch:27 loss:1.1225745677947998\n",
      "epoch:17,batch:28 loss:1.0858558416366577\n",
      "epoch:17,batch:29 loss:1.1085121631622314\n",
      "epoch:17,batch:30 loss:1.1077308654785156\n",
      "epoch:17,batch:31 loss:1.1006996631622314\n",
      "epoch:17,batch:32 loss:1.0897619724273682\n",
      "epoch:17,batch:33 loss:1.0874183177947998\n",
      "epoch:17,batch:34 loss:1.0952308177947998\n",
      "epoch:17,batch:35 loss:1.076480746269226\n",
      "epoch:17,batch:36 loss:1.1038246154785156\n",
      "epoch:17,batch:37 loss:1.0874183177947998\n",
      "epoch:17,batch:38 loss:1.0983558893203735\n",
      "epoch:17,batch:39 loss:1.0741369724273682\n",
      "epoch:17,batch:40 loss:1.0936683416366577\n",
      "epoch:17,batch:41 loss:1.1147620677947998\n",
      "epoch:17,batch:42 loss:1.0850746631622314\n",
      "epoch:17,batch:43 loss:1.0881996154785156\n",
      "epoch:17,batch:44 loss:1.0952308177947998\n",
      "epoch:17,batch:45 loss:1.1171058416366577\n",
      "epoch:17,batch:46 loss:1.100699543952942\n",
      "epoch:17,batch:47 loss:1.099918246269226\n",
      "epoch:17,batch:48 loss:1.1022621393203735\n",
      "epoch:17,batch:49 loss:1.0928871631622314\n",
      "epoch:17,batch:50 loss:1.112418293952942\n",
      "epoch:17,batch:51 loss:1.1092933416366577\n",
      "epoch:17,batch:52 loss:1.0960121154785156\n",
      "epoch:17,batch:53 loss:1.1022621393203735\n",
      "epoch:17,batch:54 loss:1.106168270111084\n",
      "epoch:17,batch:55 loss:1.1085121631622314\n",
      "epoch:17,batch:56 loss:1.1038246154785156\n",
      "epoch:17,batch:57 loss:1.117887020111084\n",
      "epoch:17,batch:58 loss:1.0975745916366577\n",
      "epoch:17,batch:59 loss:1.1030433177947998\n",
      "epoch:17,batch:60 loss:1.0780433416366577\n",
      "epoch:17,batch:61 loss:1.1038246154785156\n",
      "epoch:17,batch:62 loss:1.0866371393203735\n",
      "epoch:17,batch:63 loss:1.1100746393203735\n",
      "epoch:17,batch:64 loss:1.102262020111084\n",
      "epoch:17,batch:65 loss:1.0850746631622314\n",
      "epoch:17,batch:66 loss:1.090543270111084\n",
      "epoch:17,batch:67 loss:1.0991370677947998\n",
      "epoch:17,batch:68 loss:1.1241371631622314\n",
      "epoch:17,batch:69 loss:1.1288245916366577\n",
      "epoch:17,batch:70 loss:1.0780433416366577\n",
      "epoch:17,batch:71 loss:1.0897619724273682\n",
      "epoch:17,batch:72 loss:1.0827308893203735\n",
      "epoch:17,batch:73 loss:1.0796058177947998\n",
      "epoch:17,batch:74 loss:1.081168293952942\n",
      "epoch:17,batch:75 loss:1.0944496393203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [03:59<07:03, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17,batch:76 loss:1.077262043952942\n",
      "epoch:17,batch:77 loss:1.0952308177947998\n",
      "epoch:17, train_loss:1.0952308177947998\n",
      "epoch:18,batch:0 loss:1.085074543952942\n",
      "epoch:18,batch:1 loss:1.1046059131622314\n",
      "epoch:18,batch:2 loss:1.111636996269226\n",
      "epoch:18,batch:3 loss:1.0897620916366577\n",
      "epoch:18,batch:4 loss:1.1069495677947998\n",
      "epoch:18,batch:5 loss:1.0889809131622314\n",
      "epoch:18,batch:6 loss:1.1116371154785156\n",
      "epoch:18,batch:7 loss:1.1178871393203735\n",
      "epoch:18,batch:8 loss:1.0936683416366577\n",
      "epoch:18,batch:9 loss:1.0975745916366577\n",
      "epoch:18,batch:10 loss:1.086637020111084\n",
      "epoch:18,batch:11 loss:1.0850746631622314\n",
      "epoch:18,batch:12 loss:1.1038246154785156\n",
      "epoch:18,batch:13 loss:1.086637020111084\n",
      "epoch:18,batch:14 loss:1.102262020111084\n",
      "epoch:18,batch:15 loss:1.0858557224273682\n",
      "epoch:18,batch:16 loss:1.1288245916366577\n",
      "epoch:18,batch:17 loss:1.096793293952942\n",
      "epoch:18,batch:18 loss:1.0944496393203735\n",
      "epoch:18,batch:19 loss:1.0874183177947998\n",
      "epoch:18,batch:20 loss:1.1116371154785156\n",
      "epoch:18,batch:21 loss:1.1186684370040894\n",
      "epoch:18,batch:22 loss:1.0874183177947998\n",
      "epoch:18,batch:23 loss:1.0835120677947998\n",
      "epoch:18,batch:24 loss:1.100699543952942\n",
      "epoch:18,batch:25 loss:1.084293246269226\n",
      "epoch:18,batch:26 loss:1.1116371154785156\n",
      "epoch:18,batch:27 loss:1.1100746393203735\n",
      "epoch:18,batch:28 loss:1.1077308654785156\n",
      "epoch:18,batch:29 loss:1.110074520111084\n",
      "epoch:18,batch:30 loss:1.0741370916366577\n",
      "epoch:18,batch:31 loss:1.1014807224273682\n",
      "epoch:18,batch:32 loss:1.0913245677947998\n",
      "epoch:18,batch:33 loss:1.1280434131622314\n",
      "epoch:18,batch:34 loss:1.0975745916366577\n",
      "epoch:18,batch:35 loss:1.0952308177947998\n",
      "epoch:18,batch:36 loss:1.1092933416366577\n",
      "epoch:18,batch:37 loss:1.1124184131622314\n",
      "epoch:18,batch:38 loss:1.116324543952942\n",
      "epoch:18,batch:39 loss:1.0991370677947998\n",
      "epoch:18,batch:40 loss:1.1006996631622314\n",
      "epoch:18,batch:41 loss:1.0952308177947998\n",
      "epoch:18,batch:42 loss:1.0850746631622314\n",
      "epoch:18,batch:43 loss:1.1131995916366577\n",
      "epoch:18,batch:44 loss:1.0960121154785156\n",
      "epoch:18,batch:45 loss:1.1116371154785156\n",
      "epoch:18,batch:46 loss:1.1202309131622314\n",
      "epoch:18,batch:47 loss:1.0897620916366577\n",
      "epoch:18,batch:48 loss:1.1194496154785156\n",
      "epoch:18,batch:49 loss:1.0850746631622314\n",
      "epoch:18,batch:50 loss:1.0842933654785156\n",
      "epoch:18,batch:51 loss:1.0796058177947998\n",
      "epoch:18,batch:52 loss:1.082730770111084\n",
      "epoch:18,batch:53 loss:1.0960121154785156\n",
      "epoch:18,batch:54 loss:1.1092932224273682\n",
      "epoch:18,batch:55 loss:1.1147620677947998\n",
      "epoch:18,batch:56 loss:1.0936683416366577\n",
      "epoch:18,batch:57 loss:1.120230793952942\n",
      "epoch:18,batch:58 loss:1.0913246870040894\n",
      "epoch:18,batch:59 loss:1.0835120677947998\n",
      "epoch:18,batch:60 loss:1.1030433177947998\n",
      "epoch:18,batch:61 loss:1.0897622108459473\n",
      "epoch:18,batch:62 loss:1.0921058654785156\n",
      "epoch:18,batch:63 loss:1.0881996154785156\n",
      "epoch:18,batch:64 loss:1.1014808416366577\n",
      "epoch:18,batch:65 loss:1.0999183654785156\n",
      "epoch:18,batch:66 loss:1.1092933416366577\n",
      "epoch:18,batch:67 loss:1.117887020111084\n",
      "epoch:18,batch:68 loss:1.108512043952942\n",
      "epoch:18,batch:69 loss:1.0897620916366577\n",
      "epoch:18,batch:70 loss:1.0967934131622314\n",
      "epoch:18,batch:71 loss:1.1202309131622314\n",
      "epoch:18,batch:72 loss:1.102262020111084\n",
      "epoch:18,batch:73 loss:1.1108558177947998\n",
      "epoch:18,batch:74 loss:1.098355770111084\n",
      "epoch:18,batch:75 loss:1.1178871393203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:12<06:46, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18,batch:76 loss:1.106168270111084\n",
      "epoch:18,batch:77 loss:1.0952308177947998\n",
      "epoch:18, train_loss:1.0952308177947998\n",
      "epoch:19,batch:0 loss:1.0850746631622314\n",
      "epoch:19,batch:1 loss:1.0913245677947998\n",
      "epoch:19,batch:2 loss:1.0842933654785156\n",
      "epoch:19,batch:3 loss:1.100699543952942\n",
      "epoch:19,batch:4 loss:1.102262020111084\n",
      "epoch:19,batch:5 loss:1.096793293952942\n",
      "epoch:19,batch:6 loss:1.1053870916366577\n",
      "epoch:19,batch:7 loss:1.0913246870040894\n",
      "epoch:19,batch:8 loss:1.1124184131622314\n",
      "epoch:19,batch:9 loss:1.1077308654785156\n",
      "epoch:19,batch:10 loss:1.0897622108459473\n",
      "epoch:19,batch:11 loss:1.0881996154785156\n",
      "epoch:19,batch:12 loss:1.0897620916366577\n",
      "epoch:19,batch:13 loss:1.0811684131622314\n",
      "epoch:19,batch:14 loss:1.0780433416366577\n",
      "epoch:19,batch:15 loss:1.090543270111084\n",
      "epoch:19,batch:16 loss:1.1069495677947998\n",
      "epoch:19,batch:17 loss:1.0889809131622314\n",
      "epoch:19,batch:18 loss:1.1139808893203735\n",
      "epoch:19,batch:19 loss:1.0803871154785156\n",
      "epoch:19,batch:20 loss:1.0874184370040894\n",
      "epoch:19,batch:21 loss:1.0952308177947998\n",
      "epoch:19,batch:22 loss:1.0913244485855103\n",
      "epoch:19,batch:23 loss:1.1210120916366577\n",
      "epoch:19,batch:24 loss:1.1194496154785156\n",
      "epoch:19,batch:25 loss:1.104605793952942\n",
      "epoch:19,batch:26 loss:1.080386996269226\n",
      "epoch:19,batch:27 loss:1.116324543952942\n",
      "epoch:19,batch:28 loss:1.098355770111084\n",
      "epoch:19,batch:29 loss:1.090543270111084\n",
      "epoch:19,batch:30 loss:1.1014808416366577\n",
      "epoch:19,batch:31 loss:1.1163246631622314\n",
      "epoch:19,batch:32 loss:1.1108558177947998\n",
      "epoch:19,batch:33 loss:1.0975747108459473\n",
      "epoch:19,batch:34 loss:1.0952308177947998\n",
      "epoch:19,batch:35 loss:1.116324543952942\n",
      "epoch:19,batch:36 loss:1.108512043952942\n",
      "epoch:19,batch:37 loss:1.090543270111084\n",
      "epoch:19,batch:38 loss:1.0936684608459473\n",
      "epoch:19,batch:39 loss:1.110074520111084\n",
      "epoch:19,batch:40 loss:1.1108558177947998\n",
      "epoch:19,batch:41 loss:1.1046059131622314\n",
      "epoch:19,batch:42 loss:1.0913246870040894\n",
      "epoch:19,batch:43 loss:1.110074520111084\n",
      "epoch:19,batch:44 loss:1.1046059131622314\n",
      "epoch:19,batch:45 loss:1.099918246269226\n",
      "epoch:19,batch:46 loss:1.0897622108459473\n",
      "epoch:19,batch:47 loss:1.104605793952942\n",
      "epoch:19,batch:48 loss:1.106168270111084\n",
      "epoch:19,batch:49 loss:1.0858557224273682\n",
      "epoch:19,batch:50 loss:1.1077309846878052\n",
      "epoch:19,batch:51 loss:1.1233558654785156\n",
      "epoch:19,batch:52 loss:1.102262020111084\n",
      "epoch:19,batch:53 loss:1.1210119724273682\n",
      "epoch:19,batch:54 loss:1.0889809131622314\n",
      "epoch:19,batch:55 loss:1.0967934131622314\n",
      "epoch:19,batch:56 loss:1.1046059131622314\n",
      "epoch:19,batch:57 loss:1.1171059608459473\n",
      "epoch:19,batch:58 loss:1.0983558893203735\n",
      "epoch:19,batch:59 loss:1.108512043952942\n",
      "epoch:19,batch:60 loss:1.121793270111084\n",
      "epoch:19,batch:61 loss:1.106168270111084\n",
      "epoch:19,batch:62 loss:1.0936682224273682\n",
      "epoch:19,batch:63 loss:1.1053869724273682\n",
      "epoch:19,batch:64 loss:1.1272621154785156\n",
      "epoch:19,batch:65 loss:1.120230793952942\n",
      "epoch:19,batch:66 loss:1.080386996269226\n",
      "epoch:19,batch:67 loss:1.077262043952942\n",
      "epoch:19,batch:68 loss:1.116324543952942\n",
      "epoch:19,batch:69 loss:1.0842933654785156\n",
      "epoch:19,batch:70 loss:1.1077308654785156\n",
      "epoch:19,batch:71 loss:1.1069495677947998\n",
      "epoch:19,batch:72 loss:1.1108558177947998\n",
      "epoch:19,batch:73 loss:1.0639808177947998\n",
      "epoch:19,batch:74 loss:1.0889809131622314\n",
      "epoch:19,batch:75 loss:1.0897620916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [04:26<06:35, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19,batch:76 loss:1.1225744485855103\n",
      "epoch:19,batch:77 loss:1.1038246154785156\n",
      "epoch:19, train_loss:1.1038246154785156\n",
      "epoch:20,batch:0 loss:1.0819495916366577\n",
      "epoch:20,batch:1 loss:1.1241371631622314\n",
      "epoch:20,batch:2 loss:1.1069496870040894\n",
      "epoch:20,batch:3 loss:1.0850746631622314\n",
      "epoch:20,batch:4 loss:1.0858558416366577\n",
      "epoch:20,batch:5 loss:1.1147620677947998\n",
      "epoch:20,batch:6 loss:1.0960121154785156\n",
      "epoch:20,batch:7 loss:1.094449520111084\n",
      "epoch:20,batch:8 loss:1.1249183416366577\n",
      "epoch:20,batch:9 loss:1.0999183654785156\n",
      "epoch:20,batch:10 loss:1.0881997346878052\n",
      "epoch:20,batch:11 loss:1.074918270111084\n",
      "epoch:20,batch:12 loss:1.1092933416366577\n",
      "epoch:20,batch:13 loss:1.1350746154785156\n",
      "epoch:20,batch:14 loss:1.088980793952942\n",
      "epoch:20,batch:15 loss:1.0686683654785156\n",
      "epoch:20,batch:16 loss:1.1046059131622314\n",
      "epoch:20,batch:17 loss:1.1030434370040894\n",
      "epoch:20,batch:18 loss:1.1249183416366577\n",
      "epoch:20,batch:19 loss:1.0897620916366577\n",
      "epoch:20,batch:20 loss:1.1280434131622314\n",
      "epoch:20,batch:21 loss:1.100699543952942\n",
      "epoch:20,batch:22 loss:1.1038246154785156\n",
      "epoch:20,batch:23 loss:1.0913245677947998\n",
      "epoch:20,batch:24 loss:1.0819495916366577\n",
      "epoch:20,batch:25 loss:1.0819495916366577\n",
      "epoch:20,batch:26 loss:1.1202309131622314\n",
      "epoch:20,batch:27 loss:1.1171058416366577\n",
      "epoch:20,batch:28 loss:1.0796058177947998\n",
      "epoch:20,batch:29 loss:1.088980793952942\n",
      "epoch:20,batch:30 loss:1.1155433654785156\n",
      "epoch:20,batch:31 loss:1.0850746631622314\n",
      "epoch:20,batch:32 loss:1.1077308654785156\n",
      "epoch:20,batch:33 loss:1.1116371154785156\n",
      "epoch:20,batch:34 loss:1.1022621393203735\n",
      "epoch:20,batch:35 loss:1.0897620916366577\n",
      "epoch:20,batch:36 loss:1.1006996631622314\n",
      "epoch:20,batch:37 loss:1.1108559370040894\n",
      "epoch:20,batch:38 loss:1.1131995916366577\n",
      "epoch:20,batch:39 loss:1.1006996631622314\n",
      "epoch:20,batch:40 loss:1.1210119724273682\n",
      "epoch:20,batch:41 loss:1.0897620916366577\n",
      "epoch:20,batch:42 loss:1.0921058654785156\n",
      "epoch:20,batch:43 loss:1.106168270111084\n",
      "epoch:20,batch:44 loss:1.1038247346878052\n",
      "epoch:20,batch:45 loss:1.1038246154785156\n",
      "epoch:20,batch:46 loss:1.086637020111084\n",
      "epoch:20,batch:47 loss:1.100699543952942\n",
      "epoch:20,batch:48 loss:1.100699543952942\n",
      "epoch:20,batch:49 loss:1.0960121154785156\n",
      "epoch:20,batch:50 loss:1.1233558654785156\n",
      "epoch:20,batch:51 loss:1.104605793952942\n",
      "epoch:20,batch:52 loss:1.115543246269226\n",
      "epoch:20,batch:53 loss:1.0975745916366577\n",
      "epoch:20,batch:54 loss:1.1038246154785156\n",
      "epoch:20,batch:55 loss:1.0913244485855103\n",
      "epoch:20,batch:56 loss:1.0975745916366577\n",
      "epoch:20,batch:57 loss:1.0756995677947998\n",
      "epoch:20,batch:58 loss:1.0913245677947998\n",
      "epoch:20,batch:59 loss:1.1171058416366577\n",
      "epoch:20,batch:60 loss:1.1014807224273682\n",
      "epoch:20,batch:61 loss:1.1030433177947998\n",
      "epoch:20,batch:62 loss:1.086637020111084\n",
      "epoch:20,batch:63 loss:1.103824496269226\n",
      "epoch:20,batch:64 loss:1.106168270111084\n",
      "epoch:20,batch:65 loss:1.0858558416366577\n",
      "epoch:20,batch:66 loss:1.1069495677947998\n",
      "epoch:20,batch:67 loss:1.1108558177947998\n",
      "epoch:20,batch:68 loss:1.0913245677947998\n",
      "epoch:20,batch:69 loss:1.0983558893203735\n",
      "epoch:20,batch:70 loss:1.121793270111084\n",
      "epoch:20,batch:71 loss:1.1014808416366577\n",
      "epoch:20,batch:72 loss:1.0811684131622314\n",
      "epoch:20,batch:73 loss:1.074918508529663\n",
      "epoch:20,batch:74 loss:1.1147620677947998\n",
      "epoch:20,batch:75 loss:1.086637020111084\n",
      "epoch:20,batch:76 loss:1.113980770111084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [04:42<06:50, 14.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20,batch:77 loss:1.0881996154785156\n",
      "epoch:20, train_loss:1.0881996154785156\n",
      "epoch:21,batch:0 loss:1.1038246154785156\n",
      "epoch:21,batch:1 loss:1.0944496393203735\n",
      "epoch:21,batch:2 loss:1.104605793952942\n",
      "epoch:21,batch:3 loss:1.1038246154785156\n",
      "epoch:21,batch:4 loss:1.0897619724273682\n",
      "epoch:21,batch:5 loss:1.1030433177947998\n",
      "epoch:21,batch:6 loss:1.104605793952942\n",
      "epoch:21,batch:7 loss:1.1030433177947998\n",
      "epoch:21,batch:8 loss:1.0928871631622314\n",
      "epoch:21,batch:9 loss:1.1038246154785156\n",
      "epoch:21,batch:10 loss:1.0952308177947998\n",
      "epoch:21,batch:11 loss:1.1171059608459473\n",
      "epoch:21,batch:12 loss:1.0913245677947998\n",
      "epoch:21,batch:13 loss:1.0921058654785156\n",
      "epoch:21,batch:14 loss:1.1171058416366577\n",
      "epoch:21,batch:15 loss:1.1116371154785156\n",
      "epoch:21,batch:16 loss:1.0999183654785156\n",
      "epoch:21,batch:17 loss:1.1046059131622314\n",
      "epoch:21,batch:18 loss:1.108512043952942\n",
      "epoch:21,batch:19 loss:1.0842933654785156\n",
      "epoch:21,batch:20 loss:1.1108558177947998\n",
      "epoch:21,batch:21 loss:1.1171058416366577\n",
      "epoch:21,batch:22 loss:1.0991371870040894\n",
      "epoch:21,batch:23 loss:1.092887043952942\n",
      "epoch:21,batch:24 loss:1.086637020111084\n",
      "epoch:21,batch:25 loss:1.116324543952942\n",
      "epoch:21,batch:26 loss:1.1108559370040894\n",
      "epoch:21,batch:27 loss:1.086637020111084\n",
      "epoch:21,batch:28 loss:1.1092933416366577\n",
      "epoch:21,batch:29 loss:1.0999183654785156\n",
      "epoch:21,batch:30 loss:1.0913245677947998\n",
      "epoch:21,batch:31 loss:1.1014808416366577\n",
      "epoch:21,batch:32 loss:1.098355770111084\n",
      "epoch:21,batch:33 loss:1.077262043952942\n",
      "epoch:21,batch:34 loss:1.0983558893203735\n",
      "epoch:21,batch:35 loss:1.0881996154785156\n",
      "epoch:21,batch:36 loss:1.0897620916366577\n",
      "epoch:21,batch:37 loss:1.1100746393203735\n",
      "epoch:21,batch:38 loss:1.094449520111084\n",
      "epoch:21,batch:39 loss:1.104605793952942\n",
      "epoch:21,batch:40 loss:1.1046059131622314\n",
      "epoch:21,batch:41 loss:1.085074543952942\n",
      "epoch:21,batch:42 loss:1.0975745916366577\n",
      "epoch:21,batch:43 loss:1.0881996154785156\n",
      "epoch:21,batch:44 loss:1.1092934608459473\n",
      "epoch:21,batch:45 loss:1.1124184131622314\n",
      "epoch:21,batch:46 loss:1.106168270111084\n",
      "epoch:21,batch:47 loss:1.1053870916366577\n",
      "epoch:21,batch:48 loss:1.0835120677947998\n",
      "epoch:21,batch:49 loss:1.1233558654785156\n",
      "epoch:21,batch:50 loss:1.1124184131622314\n",
      "epoch:21,batch:51 loss:1.1061683893203735\n",
      "epoch:21,batch:52 loss:1.1030433177947998\n",
      "epoch:21,batch:53 loss:1.0936683416366577\n",
      "epoch:21,batch:54 loss:1.1014808416366577\n",
      "epoch:21,batch:55 loss:1.1108558177947998\n",
      "epoch:21,batch:56 loss:1.102262020111084\n",
      "epoch:21,batch:57 loss:1.1038246154785156\n",
      "epoch:21,batch:58 loss:1.1139808893203735\n",
      "epoch:21,batch:59 loss:1.0780433416366577\n",
      "epoch:21,batch:60 loss:1.0967934131622314\n",
      "epoch:21,batch:61 loss:1.0796058177947998\n",
      "epoch:21,batch:62 loss:1.1139808893203735\n",
      "epoch:21,batch:63 loss:1.0897620916366577\n",
      "epoch:21,batch:64 loss:1.1249183416366577\n",
      "epoch:21,batch:65 loss:1.1100746393203735\n",
      "epoch:21,batch:66 loss:1.0858558416366577\n",
      "epoch:21,batch:67 loss:1.100699543952942\n",
      "epoch:21,batch:68 loss:1.0874183177947998\n",
      "epoch:21,batch:69 loss:1.0913245677947998\n",
      "epoch:21,batch:70 loss:1.1014808416366577\n",
      "epoch:21,batch:71 loss:1.1038246154785156\n",
      "epoch:21,batch:72 loss:1.1046059131622314\n",
      "epoch:21,batch:73 loss:1.1014808416366577\n",
      "epoch:21,batch:74 loss:1.0686683654785156\n",
      "epoch:21,batch:75 loss:1.0983558893203735\n",
      "epoch:21,batch:76 loss:1.1077308654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [04:55<06:27, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21,batch:77 loss:1.1038246154785156\n",
      "epoch:21, train_loss:1.1038246154785156\n",
      "epoch:22,batch:0 loss:1.1428871154785156\n",
      "epoch:22,batch:1 loss:1.0858558416366577\n",
      "epoch:22,batch:2 loss:1.112418293952942\n",
      "epoch:22,batch:3 loss:1.1092933416366577\n",
      "epoch:22,batch:4 loss:1.1022621393203735\n",
      "epoch:22,batch:5 loss:1.1030431985855103\n",
      "epoch:22,batch:6 loss:1.0967934131622314\n",
      "epoch:22,batch:7 loss:1.1108558177947998\n",
      "epoch:22,batch:8 loss:1.088980793952942\n",
      "epoch:22,batch:9 loss:1.0928871631622314\n",
      "epoch:22,batch:10 loss:1.0991370677947998\n",
      "epoch:22,batch:11 loss:1.0960121154785156\n",
      "epoch:22,batch:12 loss:1.0999183654785156\n",
      "epoch:22,batch:13 loss:1.1108558177947998\n",
      "epoch:22,batch:14 loss:1.0999183654785156\n",
      "epoch:22,batch:15 loss:1.098355770111084\n",
      "epoch:22,batch:16 loss:1.1303871870040894\n",
      "epoch:22,batch:17 loss:1.1014808416366577\n",
      "epoch:22,batch:18 loss:1.1022621393203735\n",
      "epoch:22,batch:19 loss:1.123355746269226\n",
      "epoch:22,batch:20 loss:1.0983558893203735\n",
      "epoch:22,batch:21 loss:1.1108558177947998\n",
      "epoch:22,batch:22 loss:1.0874183177947998\n",
      "epoch:22,batch:23 loss:1.0913245677947998\n",
      "epoch:22,batch:24 loss:1.1108558177947998\n",
      "epoch:22,batch:25 loss:1.088980793952942\n",
      "epoch:22,batch:26 loss:1.099918246269226\n",
      "epoch:22,batch:27 loss:1.1225745677947998\n",
      "epoch:22,batch:28 loss:1.0991370677947998\n",
      "epoch:22,batch:29 loss:1.106168270111084\n",
      "epoch:22,batch:30 loss:1.0506995916366577\n",
      "epoch:22,batch:31 loss:1.0897620916366577\n",
      "epoch:22,batch:32 loss:1.1194496154785156\n",
      "epoch:22,batch:33 loss:1.098355770111084\n",
      "epoch:22,batch:34 loss:1.113980770111084\n",
      "epoch:22,batch:35 loss:1.100699543952942\n",
      "epoch:22,batch:36 loss:1.0733559131622314\n",
      "epoch:22,batch:37 loss:1.1108558177947998\n",
      "epoch:22,batch:38 loss:1.0967934131622314\n",
      "epoch:22,batch:39 loss:1.0991370677947998\n",
      "epoch:22,batch:40 loss:1.1030434370040894\n",
      "epoch:22,batch:41 loss:1.0928871631622314\n",
      "epoch:22,batch:42 loss:1.0897620916366577\n",
      "epoch:22,batch:43 loss:1.110074520111084\n",
      "epoch:22,batch:44 loss:1.0874183177947998\n",
      "epoch:22,batch:45 loss:1.1077308654785156\n",
      "epoch:22,batch:46 loss:1.0702308416366577\n",
      "epoch:22,batch:47 loss:1.1116371154785156\n",
      "epoch:22,batch:48 loss:1.090543270111084\n",
      "epoch:22,batch:49 loss:1.0983558893203735\n",
      "epoch:22,batch:50 loss:1.0999183654785156\n",
      "epoch:22,batch:51 loss:1.092887043952942\n",
      "epoch:22,batch:52 loss:1.0842933654785156\n",
      "epoch:22,batch:53 loss:1.0881996154785156\n",
      "epoch:22,batch:54 loss:1.119449496269226\n",
      "epoch:22,batch:55 loss:1.1264808177947998\n",
      "epoch:22,batch:56 loss:1.094449520111084\n",
      "epoch:22,batch:57 loss:1.1061683893203735\n",
      "epoch:22,batch:58 loss:1.1147620677947998\n",
      "epoch:22,batch:59 loss:1.0960121154785156\n",
      "epoch:22,batch:60 loss:1.0842933654785156\n",
      "epoch:22,batch:61 loss:1.1217933893203735\n",
      "epoch:22,batch:62 loss:1.1186683177947998\n",
      "epoch:22,batch:63 loss:1.0796058177947998\n",
      "epoch:22,batch:64 loss:1.1077308654785156\n",
      "epoch:22,batch:65 loss:1.0780433416366577\n",
      "epoch:22,batch:66 loss:1.0905433893203735\n",
      "epoch:22,batch:67 loss:1.0936683416366577\n",
      "epoch:22,batch:68 loss:1.0944496393203735\n",
      "epoch:22,batch:69 loss:1.096793293952942\n",
      "epoch:22,batch:70 loss:1.107730746269226\n",
      "epoch:22,batch:71 loss:1.0936682224273682\n",
      "epoch:22,batch:72 loss:1.090543270111084\n",
      "epoch:22,batch:73 loss:1.1014809608459473\n",
      "epoch:22,batch:74 loss:1.1014808416366577\n",
      "epoch:22,batch:75 loss:1.0999183654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:09<06:12, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22,batch:76 loss:1.1131995916366577\n",
      "epoch:22,batch:77 loss:1.099918246269226\n",
      "epoch:22, train_loss:1.099918246269226\n",
      "epoch:23,batch:0 loss:1.1022621393203735\n",
      "epoch:23,batch:1 loss:1.1155433654785156\n",
      "epoch:23,batch:2 loss:1.119449496269226\n",
      "epoch:23,batch:3 loss:1.120230793952942\n",
      "epoch:23,batch:4 loss:1.112418293952942\n",
      "epoch:23,batch:5 loss:1.0936684608459473\n",
      "epoch:23,batch:6 loss:1.1139808893203735\n",
      "epoch:23,batch:7 loss:1.1053872108459473\n",
      "epoch:23,batch:8 loss:1.096793293952942\n",
      "epoch:23,batch:9 loss:1.1131997108459473\n",
      "epoch:23,batch:10 loss:1.0858558416366577\n",
      "epoch:23,batch:11 loss:1.0975744724273682\n",
      "epoch:23,batch:12 loss:1.0889809131622314\n",
      "epoch:23,batch:13 loss:1.1038246154785156\n",
      "epoch:23,batch:14 loss:1.096793293952942\n",
      "epoch:23,batch:15 loss:1.1014808416366577\n",
      "epoch:23,batch:16 loss:1.1186684370040894\n",
      "epoch:23,batch:17 loss:1.0585120916366577\n",
      "epoch:23,batch:18 loss:1.0944496393203735\n",
      "epoch:23,batch:19 loss:1.077262043952942\n",
      "epoch:23,batch:20 loss:1.1053870916366577\n",
      "epoch:23,batch:21 loss:1.0702308416366577\n",
      "epoch:23,batch:22 loss:1.1022621393203735\n",
      "epoch:23,batch:23 loss:1.1124184131622314\n",
      "epoch:23,batch:24 loss:1.1155433654785156\n",
      "epoch:23,batch:25 loss:1.0811684131622314\n",
      "epoch:23,batch:26 loss:1.098355770111084\n",
      "epoch:23,batch:27 loss:1.1108558177947998\n",
      "epoch:23,batch:28 loss:1.1022621393203735\n",
      "epoch:23,batch:29 loss:1.0881996154785156\n",
      "epoch:23,batch:30 loss:1.0960121154785156\n",
      "epoch:23,batch:31 loss:1.103824496269226\n",
      "epoch:23,batch:32 loss:1.1147621870040894\n",
      "epoch:23,batch:33 loss:1.0991370677947998\n",
      "epoch:23,batch:34 loss:1.1116371154785156\n",
      "epoch:23,batch:35 loss:1.0819495916366577\n",
      "epoch:23,batch:36 loss:1.0858558416366577\n",
      "epoch:23,batch:37 loss:1.0983558893203735\n",
      "epoch:23,batch:38 loss:1.0991370677947998\n",
      "epoch:23,batch:39 loss:1.081168293952942\n",
      "epoch:23,batch:40 loss:1.084293246269226\n",
      "epoch:23,batch:41 loss:1.1069495677947998\n",
      "epoch:23,batch:42 loss:1.0975745916366577\n",
      "epoch:23,batch:43 loss:1.1217933893203735\n",
      "epoch:23,batch:44 loss:1.099918246269226\n",
      "epoch:23,batch:45 loss:1.1085119247436523\n",
      "epoch:23,batch:46 loss:1.0858557224273682\n",
      "epoch:23,batch:47 loss:1.1046059131622314\n",
      "epoch:23,batch:48 loss:1.1155433654785156\n",
      "epoch:23,batch:49 loss:1.1342933177947998\n",
      "epoch:23,batch:50 loss:1.0991370677947998\n",
      "epoch:23,batch:51 loss:1.0889809131622314\n",
      "epoch:23,batch:52 loss:1.1030433177947998\n",
      "epoch:23,batch:53 loss:1.1217933893203735\n",
      "epoch:23,batch:54 loss:1.1131994724273682\n",
      "epoch:23,batch:55 loss:1.0819494724273682\n",
      "epoch:23,batch:56 loss:1.1217933893203735\n",
      "epoch:23,batch:57 loss:1.096793293952942\n",
      "epoch:23,batch:58 loss:1.1006996631622314\n",
      "epoch:23,batch:59 loss:1.098355770111084\n",
      "epoch:23,batch:60 loss:1.0835120677947998\n",
      "epoch:23,batch:61 loss:1.092887043952942\n",
      "epoch:23,batch:62 loss:1.081168293952942\n",
      "epoch:23,batch:63 loss:1.1014807224273682\n",
      "epoch:23,batch:64 loss:1.1022621393203735\n",
      "epoch:23,batch:65 loss:1.1053870916366577\n",
      "epoch:23,batch:66 loss:1.108512043952942\n",
      "epoch:23,batch:67 loss:1.107730746269226\n",
      "epoch:23,batch:68 loss:1.0921058654785156\n",
      "epoch:23,batch:69 loss:1.0874183177947998\n",
      "epoch:23,batch:70 loss:1.0952308177947998\n",
      "epoch:23,batch:71 loss:1.1210120916366577\n",
      "epoch:23,batch:72 loss:1.0936683416366577\n",
      "epoch:23,batch:73 loss:1.088980793952942\n",
      "epoch:23,batch:74 loss:1.1038246154785156\n",
      "epoch:23,batch:75 loss:1.0999183654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [05:22<05:53, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23,batch:76 loss:1.096793293952942\n",
      "epoch:23,batch:77 loss:1.1030433177947998\n",
      "epoch:23, train_loss:1.1030433177947998\n",
      "epoch:24,batch:0 loss:1.092887043952942\n",
      "epoch:24,batch:1 loss:1.0999183654785156\n",
      "epoch:24,batch:2 loss:1.1069495677947998\n",
      "epoch:24,batch:3 loss:1.077262043952942\n",
      "epoch:24,batch:4 loss:1.074918270111084\n",
      "epoch:24,batch:5 loss:1.092887043952942\n",
      "epoch:24,batch:6 loss:1.1147620677947998\n",
      "epoch:24,batch:7 loss:1.090543270111084\n",
      "epoch:24,batch:8 loss:1.1030433177947998\n",
      "epoch:24,batch:9 loss:1.1171058416366577\n",
      "epoch:24,batch:10 loss:1.1014808416366577\n",
      "epoch:24,batch:11 loss:1.1014807224273682\n",
      "epoch:24,batch:12 loss:1.104605793952942\n",
      "epoch:24,batch:13 loss:1.1264808177947998\n",
      "epoch:24,batch:14 loss:1.1108558177947998\n",
      "epoch:24,batch:15 loss:1.1022621393203735\n",
      "epoch:24,batch:16 loss:1.078824520111084\n",
      "epoch:24,batch:17 loss:1.1069495677947998\n",
      "epoch:24,batch:18 loss:1.110074520111084\n",
      "epoch:24,batch:19 loss:1.1053870916366577\n",
      "epoch:24,batch:20 loss:1.0842933654785156\n",
      "epoch:24,batch:21 loss:1.108512043952942\n",
      "epoch:24,batch:22 loss:1.0967934131622314\n",
      "epoch:24,batch:23 loss:1.121793270111084\n",
      "epoch:24,batch:24 loss:1.0897620916366577\n",
      "epoch:24,batch:25 loss:1.1069495677947998\n",
      "epoch:24,batch:26 loss:1.0741370916366577\n",
      "epoch:24,batch:27 loss:1.1038246154785156\n",
      "epoch:24,batch:28 loss:1.104605793952942\n",
      "epoch:24,batch:29 loss:1.100699543952942\n",
      "epoch:24,batch:30 loss:1.0741372108459473\n",
      "epoch:24,batch:31 loss:1.0999183654785156\n",
      "epoch:24,batch:32 loss:1.085074543952942\n",
      "epoch:24,batch:33 loss:1.1155433654785156\n",
      "epoch:24,batch:34 loss:1.0952308177947998\n",
      "epoch:24,batch:35 loss:1.1178871393203735\n",
      "epoch:24,batch:36 loss:1.0913245677947998\n",
      "epoch:24,batch:37 loss:1.0842933654785156\n",
      "epoch:24,batch:38 loss:1.0960121154785156\n",
      "epoch:24,batch:39 loss:1.1053872108459473\n",
      "epoch:24,batch:40 loss:1.0796058177947998\n",
      "epoch:24,batch:41 loss:1.1163246631622314\n",
      "epoch:24,batch:42 loss:1.0897620916366577\n",
      "epoch:24,batch:43 loss:1.1092933416366577\n",
      "epoch:24,batch:44 loss:1.0952309370040894\n",
      "epoch:24,batch:45 loss:1.1249183416366577\n",
      "epoch:24,batch:46 loss:1.0936683416366577\n",
      "epoch:24,batch:47 loss:1.1061683893203735\n",
      "epoch:24,batch:48 loss:1.1249183416366577\n",
      "epoch:24,batch:49 loss:1.0967934131622314\n",
      "epoch:24,batch:50 loss:1.0803871154785156\n",
      "epoch:24,batch:51 loss:1.1030433177947998\n",
      "epoch:24,batch:52 loss:1.0850746631622314\n",
      "epoch:24,batch:53 loss:1.1053869724273682\n",
      "epoch:24,batch:54 loss:1.1202309131622314\n",
      "epoch:24,batch:55 loss:1.1077308654785156\n",
      "epoch:24,batch:56 loss:1.1108558177947998\n",
      "epoch:24,batch:57 loss:1.1069496870040894\n",
      "epoch:24,batch:58 loss:1.1124184131622314\n",
      "epoch:24,batch:59 loss:1.0921058654785156\n",
      "epoch:24,batch:60 loss:1.0788246393203735\n",
      "epoch:24,batch:61 loss:1.1014807224273682\n",
      "epoch:24,batch:62 loss:1.1249183416366577\n",
      "epoch:24,batch:63 loss:1.1092933416366577\n",
      "epoch:24,batch:64 loss:1.0717933177947998\n",
      "epoch:24,batch:65 loss:1.1092933416366577\n",
      "epoch:24,batch:66 loss:1.0975747108459473\n",
      "epoch:24,batch:67 loss:1.1077308654785156\n",
      "epoch:24,batch:68 loss:1.0874183177947998\n",
      "epoch:24,batch:69 loss:1.1061683893203735\n",
      "epoch:24,batch:70 loss:1.092887043952942\n",
      "epoch:24,batch:71 loss:1.1147620677947998\n",
      "epoch:24,batch:72 loss:1.102262020111084\n",
      "epoch:24,batch:73 loss:1.0944496393203735\n",
      "epoch:24,batch:74 loss:1.0928869247436523\n",
      "epoch:24,batch:75 loss:1.0967934131622314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [05:35<05:37, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24,batch:76 loss:1.120230793952942\n",
      "epoch:24,batch:77 loss:1.090543270111084\n",
      "epoch:24, train_loss:1.090543270111084\n",
      "epoch:25,batch:0 loss:1.1030433177947998\n",
      "epoch:25,batch:1 loss:1.0835120677947998\n",
      "epoch:25,batch:2 loss:1.0960121154785156\n",
      "epoch:25,batch:3 loss:1.1210120916366577\n",
      "epoch:25,batch:4 loss:1.0827308893203735\n",
      "epoch:25,batch:5 loss:1.0897620916366577\n",
      "epoch:25,batch:6 loss:1.0999183654785156\n",
      "epoch:25,batch:7 loss:1.0967934131622314\n",
      "epoch:25,batch:8 loss:1.102262020111084\n",
      "epoch:25,batch:9 loss:1.102262020111084\n",
      "epoch:25,batch:10 loss:1.0991371870040894\n",
      "epoch:25,batch:11 loss:1.0936683416366577\n",
      "epoch:25,batch:12 loss:1.0999183654785156\n",
      "epoch:25,batch:13 loss:1.0647621154785156\n",
      "epoch:25,batch:14 loss:1.116324543952942\n",
      "epoch:25,batch:15 loss:1.0921058654785156\n",
      "epoch:25,batch:16 loss:1.0975745916366577\n",
      "epoch:25,batch:17 loss:1.096793293952942\n",
      "epoch:25,batch:18 loss:1.1225745677947998\n",
      "epoch:25,batch:19 loss:1.121793270111084\n",
      "epoch:25,batch:20 loss:1.107730746269226\n",
      "epoch:25,batch:21 loss:1.1100746393203735\n",
      "epoch:25,batch:22 loss:1.1038246154785156\n",
      "epoch:25,batch:23 loss:1.1186683177947998\n",
      "epoch:25,batch:24 loss:1.1061683893203735\n",
      "epoch:25,batch:25 loss:1.086637020111084\n",
      "epoch:25,batch:26 loss:1.1030433177947998\n",
      "epoch:25,batch:27 loss:1.0952308177947998\n",
      "epoch:25,batch:28 loss:1.1131995916366577\n",
      "epoch:25,batch:29 loss:1.0960121154785156\n",
      "epoch:25,batch:30 loss:1.1061683893203735\n",
      "epoch:25,batch:31 loss:1.1100746393203735\n",
      "epoch:25,batch:32 loss:1.1366370916366577\n",
      "epoch:25,batch:33 loss:1.1108558177947998\n",
      "epoch:25,batch:34 loss:1.1155433654785156\n",
      "epoch:25,batch:35 loss:1.100699543952942\n",
      "epoch:25,batch:36 loss:1.125699520111084\n",
      "epoch:25,batch:37 loss:1.077262043952942\n",
      "epoch:25,batch:38 loss:1.1155433654785156\n",
      "epoch:25,batch:39 loss:1.1022621393203735\n",
      "epoch:25,batch:40 loss:1.073355793952942\n",
      "epoch:25,batch:41 loss:1.092887043952942\n",
      "epoch:25,batch:42 loss:1.1006996631622314\n",
      "epoch:25,batch:43 loss:1.0796058177947998\n",
      "epoch:25,batch:44 loss:1.1053872108459473\n",
      "epoch:25,batch:45 loss:1.092105746269226\n",
      "epoch:25,batch:46 loss:1.0960121154785156\n",
      "epoch:25,batch:47 loss:1.0928871631622314\n",
      "epoch:25,batch:48 loss:1.129605770111084\n",
      "epoch:25,batch:49 loss:1.1022621393203735\n",
      "epoch:25,batch:50 loss:1.108512043952942\n",
      "epoch:25,batch:51 loss:1.113980770111084\n",
      "epoch:25,batch:52 loss:1.0803871154785156\n",
      "epoch:25,batch:53 loss:1.0921058654785156\n",
      "epoch:25,batch:54 loss:1.1046059131622314\n",
      "epoch:25,batch:55 loss:1.0874183177947998\n",
      "epoch:25,batch:56 loss:1.0811684131622314\n",
      "epoch:25,batch:57 loss:1.1124184131622314\n",
      "epoch:25,batch:58 loss:1.085074543952942\n",
      "epoch:25,batch:59 loss:1.108512043952942\n",
      "epoch:25,batch:60 loss:1.1030433177947998\n",
      "epoch:25,batch:61 loss:1.1233558654785156\n",
      "epoch:25,batch:62 loss:1.099918246269226\n",
      "epoch:25,batch:63 loss:1.098355770111084\n",
      "epoch:25,batch:64 loss:1.1014808416366577\n",
      "epoch:25,batch:65 loss:1.1131995916366577\n",
      "epoch:25,batch:66 loss:1.1077308654785156\n",
      "epoch:25,batch:67 loss:1.0999183654785156\n",
      "epoch:25,batch:68 loss:1.1053870916366577\n",
      "epoch:25,batch:69 loss:1.0756996870040894\n",
      "epoch:25,batch:70 loss:1.0772621631622314\n",
      "epoch:25,batch:71 loss:1.096793293952942\n",
      "epoch:25,batch:72 loss:1.0842933654785156\n",
      "epoch:25,batch:73 loss:1.0936683416366577\n",
      "epoch:25,batch:74 loss:1.086637020111084\n",
      "epoch:25,batch:75 loss:1.1171058416366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [05:48<05:21, 13.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25,batch:76 loss:1.102262020111084\n",
      "epoch:25,batch:77 loss:1.0803871154785156\n",
      "epoch:25, train_loss:1.0803871154785156\n",
      "epoch:26,batch:0 loss:1.1038246154785156\n",
      "epoch:26,batch:1 loss:1.1233558654785156\n",
      "epoch:26,batch:2 loss:1.0788246393203735\n",
      "epoch:26,batch:3 loss:1.0913245677947998\n",
      "epoch:26,batch:4 loss:1.1139808893203735\n",
      "epoch:26,batch:5 loss:1.1108558177947998\n",
      "epoch:26,batch:6 loss:1.092887043952942\n",
      "epoch:26,batch:7 loss:1.094449520111084\n",
      "epoch:26,batch:8 loss:1.108512043952942\n",
      "epoch:26,batch:9 loss:1.0991371870040894\n",
      "epoch:26,batch:10 loss:1.1006996631622314\n",
      "epoch:26,batch:11 loss:1.129605770111084\n",
      "epoch:26,batch:12 loss:1.0991370677947998\n",
      "epoch:26,batch:13 loss:1.106168270111084\n",
      "epoch:26,batch:14 loss:1.1092934608459473\n",
      "epoch:26,batch:15 loss:1.1038246154785156\n",
      "epoch:26,batch:16 loss:1.116324543952942\n",
      "epoch:26,batch:17 loss:1.0897620916366577\n",
      "epoch:26,batch:18 loss:1.0897620916366577\n",
      "epoch:26,batch:19 loss:1.0928871631622314\n",
      "epoch:26,batch:20 loss:1.098355770111084\n",
      "epoch:26,batch:21 loss:1.0881996154785156\n",
      "epoch:26,batch:22 loss:1.0921058654785156\n",
      "epoch:26,batch:23 loss:1.1217933893203735\n",
      "epoch:26,batch:24 loss:1.084293246269226\n",
      "epoch:26,batch:25 loss:1.0866371393203735\n",
      "epoch:26,batch:26 loss:1.0944496393203735\n",
      "epoch:26,batch:27 loss:1.0944496393203735\n",
      "epoch:26,batch:28 loss:1.1030433177947998\n",
      "epoch:26,batch:29 loss:1.1053869724273682\n",
      "epoch:26,batch:30 loss:1.088980793952942\n",
      "epoch:26,batch:31 loss:1.103824496269226\n",
      "epoch:26,batch:32 loss:1.1171058416366577\n",
      "epoch:26,batch:33 loss:1.1092933416366577\n",
      "epoch:26,batch:34 loss:1.0983558893203735\n",
      "epoch:26,batch:35 loss:1.106168270111084\n",
      "epoch:26,batch:36 loss:1.100699543952942\n",
      "epoch:26,batch:37 loss:1.1171059608459473\n",
      "epoch:26,batch:38 loss:1.0819495916366577\n",
      "epoch:26,batch:39 loss:1.104605793952942\n",
      "epoch:26,batch:40 loss:1.1100746393203735\n",
      "epoch:26,batch:41 loss:1.074918270111084\n",
      "epoch:26,batch:42 loss:1.1210119724273682\n",
      "epoch:26,batch:43 loss:1.0897620916366577\n",
      "epoch:26,batch:44 loss:1.1053870916366577\n",
      "epoch:26,batch:45 loss:1.0905433893203735\n",
      "epoch:26,batch:46 loss:1.1030433177947998\n",
      "epoch:26,batch:47 loss:1.0772621631622314\n",
      "epoch:26,batch:48 loss:1.0960121154785156\n",
      "epoch:26,batch:49 loss:1.128043293952942\n",
      "epoch:26,batch:50 loss:1.1163246631622314\n",
      "epoch:26,batch:51 loss:1.094449520111084\n",
      "epoch:26,batch:52 loss:1.100699543952942\n",
      "epoch:26,batch:53 loss:1.0952309370040894\n",
      "epoch:26,batch:54 loss:1.0858558416366577\n",
      "epoch:26,batch:55 loss:1.1116371154785156\n",
      "epoch:26,batch:56 loss:1.0913246870040894\n",
      "epoch:26,batch:57 loss:1.0975745916366577\n",
      "epoch:26,batch:58 loss:1.1147621870040894\n",
      "epoch:26,batch:59 loss:1.0999183654785156\n",
      "epoch:26,batch:60 loss:1.098355770111084\n",
      "epoch:26,batch:61 loss:1.0921058654785156\n",
      "epoch:26,batch:62 loss:1.0796058177947998\n",
      "epoch:26,batch:63 loss:1.102262020111084\n",
      "epoch:26,batch:64 loss:1.1194496154785156\n",
      "epoch:26,batch:65 loss:1.0858558416366577\n",
      "epoch:26,batch:66 loss:1.1092933416366577\n",
      "epoch:26,batch:67 loss:1.098355770111084\n",
      "epoch:26,batch:68 loss:1.1366370916366577\n",
      "epoch:26,batch:69 loss:1.0725746154785156\n",
      "epoch:26,batch:70 loss:1.117887020111084\n",
      "epoch:26,batch:71 loss:1.092887043952942\n",
      "epoch:26,batch:72 loss:1.081168293952942\n",
      "epoch:26,batch:73 loss:1.092105746269226\n",
      "epoch:26,batch:74 loss:1.0921058654785156\n",
      "epoch:26,batch:75 loss:1.1038246154785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [06:01<05:06, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26,batch:76 loss:1.0999183654785156\n",
      "epoch:26,batch:77 loss:1.1046059131622314\n",
      "epoch:26, train_loss:1.1046059131622314\n",
      "epoch:27,batch:0 loss:1.1069495677947998\n",
      "epoch:27,batch:1 loss:1.1077308654785156\n",
      "epoch:27,batch:2 loss:1.0928871631622314\n",
      "epoch:27,batch:3 loss:1.0960121154785156\n",
      "epoch:27,batch:4 loss:1.0999183654785156\n",
      "epoch:27,batch:5 loss:1.1053870916366577\n",
      "epoch:27,batch:6 loss:1.1046059131622314\n",
      "epoch:27,batch:7 loss:1.1092932224273682\n",
      "epoch:27,batch:8 loss:1.0866371393203735\n",
      "epoch:27,batch:9 loss:1.1092933416366577\n",
      "epoch:27,batch:10 loss:1.1264808177947998\n",
      "epoch:27,batch:11 loss:1.0874183177947998\n",
      "epoch:27,batch:12 loss:1.1092933416366577\n",
      "epoch:27,batch:13 loss:1.1053872108459473\n",
      "epoch:27,batch:14 loss:1.0616371631622314\n",
      "epoch:27,batch:15 loss:1.102262020111084\n",
      "epoch:27,batch:16 loss:1.1014808416366577\n",
      "epoch:27,batch:17 loss:1.0991370677947998\n",
      "epoch:27,batch:18 loss:1.0913245677947998\n",
      "epoch:27,batch:19 loss:1.1022621393203735\n",
      "epoch:27,batch:20 loss:1.088980793952942\n",
      "epoch:27,batch:21 loss:1.1178871393203735\n",
      "epoch:27,batch:22 loss:1.1194496154785156\n",
      "epoch:27,batch:23 loss:1.1186683177947998\n",
      "epoch:27,batch:24 loss:1.0975745916366577\n",
      "epoch:27,batch:25 loss:1.0835120677947998\n",
      "epoch:27,batch:26 loss:1.0835120677947998\n",
      "epoch:27,batch:27 loss:1.0952308177947998\n",
      "epoch:27,batch:28 loss:1.1092933416366577\n",
      "epoch:27,batch:29 loss:1.088199496269226\n",
      "epoch:27,batch:30 loss:1.106168270111084\n",
      "epoch:27,batch:31 loss:1.0827308893203735\n",
      "epoch:27,batch:32 loss:1.1155433654785156\n",
      "epoch:27,batch:33 loss:1.1116371154785156\n",
      "epoch:27,batch:34 loss:1.1077308654785156\n",
      "epoch:27,batch:35 loss:1.1147620677947998\n",
      "epoch:27,batch:36 loss:1.0842933654785156\n",
      "epoch:27,batch:37 loss:1.0835120677947998\n",
      "epoch:27,batch:38 loss:1.1131995916366577\n",
      "epoch:27,batch:39 loss:1.1108558177947998\n",
      "epoch:27,batch:40 loss:1.1155433654785156\n",
      "epoch:27,batch:41 loss:1.0678870677947998\n",
      "epoch:27,batch:42 loss:1.0780433416366577\n",
      "epoch:27,batch:43 loss:1.0944496393203735\n",
      "epoch:27,batch:44 loss:1.113980770111084\n",
      "epoch:27,batch:45 loss:1.1022621393203735\n",
      "epoch:27,batch:46 loss:1.0897619724273682\n",
      "epoch:27,batch:47 loss:1.0858558416366577\n",
      "epoch:27,batch:48 loss:1.1194496154785156\n",
      "epoch:27,batch:49 loss:1.104605793952942\n",
      "epoch:27,batch:50 loss:1.0952308177947998\n",
      "epoch:27,batch:51 loss:1.081168293952942\n",
      "epoch:27,batch:52 loss:1.098355770111084\n",
      "epoch:27,batch:53 loss:1.0913245677947998\n",
      "epoch:27,batch:54 loss:1.1053870916366577\n",
      "epoch:27,batch:55 loss:1.0835120677947998\n",
      "epoch:27,batch:56 loss:1.111636996269226\n",
      "epoch:27,batch:57 loss:1.1014808416366577\n",
      "epoch:27,batch:58 loss:1.102262020111084\n",
      "epoch:27,batch:59 loss:1.127261996269226\n",
      "epoch:27,batch:60 loss:1.123355746269226\n",
      "epoch:27,batch:61 loss:1.1280434131622314\n",
      "epoch:27,batch:62 loss:1.096793293952942\n",
      "epoch:27,batch:63 loss:1.0772621631622314\n",
      "epoch:27,batch:64 loss:1.1038246154785156\n",
      "epoch:27,batch:65 loss:1.098355770111084\n",
      "epoch:27,batch:66 loss:1.103824496269226\n",
      "epoch:27,batch:67 loss:1.0975745916366577\n",
      "epoch:27,batch:68 loss:1.0858558416366577\n",
      "epoch:27,batch:69 loss:1.1006996631622314\n",
      "epoch:27,batch:70 loss:1.111636996269226\n",
      "epoch:27,batch:71 loss:1.0858558416366577\n",
      "epoch:27,batch:72 loss:1.0835120677947998\n",
      "epoch:27,batch:73 loss:1.0944496393203735\n",
      "epoch:27,batch:74 loss:1.0663245916366577\n",
      "epoch:27,batch:75 loss:1.1030433177947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [06:15<04:52, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27,batch:76 loss:1.1319496631622314\n",
      "epoch:27,batch:77 loss:1.1233558654785156\n",
      "epoch:27, train_loss:1.1233558654785156\n",
      "epoch:28,batch:0 loss:1.0944496393203735\n",
      "epoch:28,batch:1 loss:1.1046059131622314\n",
      "epoch:28,batch:2 loss:1.1139808893203735\n",
      "epoch:28,batch:3 loss:1.1116371154785156\n",
      "epoch:28,batch:4 loss:1.1046059131622314\n",
      "epoch:28,batch:5 loss:1.0921058654785156\n",
      "epoch:28,batch:6 loss:1.086637020111084\n",
      "epoch:28,batch:7 loss:1.108512043952942\n",
      "epoch:28,batch:8 loss:1.1171058416366577\n",
      "epoch:28,batch:9 loss:1.1092933416366577\n",
      "epoch:28,batch:10 loss:1.1061683893203735\n",
      "epoch:28,batch:11 loss:1.0835120677947998\n",
      "epoch:28,batch:12 loss:1.0850746631622314\n",
      "epoch:28,batch:13 loss:1.1030433177947998\n",
      "epoch:28,batch:14 loss:1.092887043952942\n",
      "epoch:28,batch:15 loss:1.0991370677947998\n",
      "epoch:28,batch:16 loss:1.0921058654785156\n",
      "epoch:28,batch:17 loss:1.1171059608459473\n",
      "epoch:28,batch:18 loss:1.0764808654785156\n",
      "epoch:28,batch:19 loss:1.088199496269226\n",
      "epoch:28,batch:20 loss:1.0999183654785156\n",
      "epoch:28,batch:21 loss:1.1092933416366577\n",
      "epoch:28,batch:22 loss:1.119449496269226\n",
      "epoch:28,batch:23 loss:1.0944496393203735\n",
      "epoch:28,batch:24 loss:1.1124184131622314\n",
      "epoch:28,batch:25 loss:1.128043293952942\n",
      "epoch:28,batch:26 loss:1.124137043952942\n",
      "epoch:28,batch:27 loss:1.1108558177947998\n",
      "epoch:28,batch:28 loss:1.108512043952942\n",
      "epoch:28,batch:29 loss:1.1030433177947998\n",
      "epoch:28,batch:30 loss:1.1061683893203735\n",
      "epoch:28,batch:31 loss:1.0913245677947998\n",
      "epoch:28,batch:32 loss:1.0936683416366577\n",
      "epoch:28,batch:33 loss:1.0858558416366577\n",
      "epoch:28,batch:34 loss:1.0936682224273682\n",
      "epoch:28,batch:35 loss:1.1194496154785156\n",
      "epoch:28,batch:36 loss:1.0921058654785156\n",
      "epoch:28,batch:37 loss:1.1038246154785156\n",
      "epoch:28,batch:38 loss:1.0975747108459473\n",
      "epoch:28,batch:39 loss:1.0991370677947998\n",
      "epoch:28,batch:40 loss:1.0967934131622314\n",
      "epoch:28,batch:41 loss:1.0983558893203735\n",
      "epoch:28,batch:42 loss:1.1241371631622314\n",
      "epoch:28,batch:43 loss:1.0975745916366577\n",
      "epoch:28,batch:44 loss:1.0842933654785156\n",
      "epoch:28,batch:45 loss:1.0889809131622314\n",
      "epoch:28,batch:46 loss:1.0897620916366577\n",
      "epoch:28,batch:47 loss:1.0999183654785156\n",
      "epoch:28,batch:48 loss:1.1069495677947998\n",
      "epoch:28,batch:49 loss:1.0921058654785156\n",
      "epoch:28,batch:50 loss:1.082730770111084\n",
      "epoch:28,batch:51 loss:1.1131995916366577\n",
      "epoch:28,batch:52 loss:1.0936683416366577\n",
      "epoch:28,batch:53 loss:1.1053870916366577\n",
      "epoch:28,batch:54 loss:1.0827308893203735\n",
      "epoch:28,batch:55 loss:1.0960121154785156\n",
      "epoch:28,batch:56 loss:1.0936684608459473\n",
      "epoch:28,batch:57 loss:1.1030433177947998\n",
      "epoch:28,batch:58 loss:1.0983558893203735\n",
      "epoch:28,batch:59 loss:1.0897620916366577\n",
      "epoch:28,batch:60 loss:1.1186683177947998\n",
      "epoch:28,batch:61 loss:1.1053869724273682\n",
      "epoch:28,batch:62 loss:1.0850746631622314\n",
      "epoch:28,batch:63 loss:1.1053870916366577\n",
      "epoch:28,batch:64 loss:1.116324543952942\n",
      "epoch:28,batch:65 loss:1.0905433893203735\n",
      "epoch:28,batch:66 loss:1.0928871631622314\n",
      "epoch:28,batch:67 loss:1.0999183654785156\n",
      "epoch:28,batch:68 loss:1.106168270111084\n",
      "epoch:28,batch:69 loss:1.1053872108459473\n",
      "epoch:28,batch:70 loss:1.0842933654785156\n",
      "epoch:28,batch:71 loss:1.094449520111084\n",
      "epoch:28,batch:72 loss:1.1092934608459473\n",
      "epoch:28,batch:73 loss:1.1155433654785156\n",
      "epoch:28,batch:74 loss:1.1077308654785156\n",
      "epoch:28,batch:75 loss:1.096793293952942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [06:28<04:38, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28,batch:76 loss:1.0858558416366577\n",
      "epoch:28,batch:77 loss:1.1038246154785156\n",
      "epoch:28, train_loss:1.1038246154785156\n",
      "epoch:29,batch:0 loss:1.110074520111084\n",
      "epoch:29,batch:1 loss:1.0983558893203735\n",
      "epoch:29,batch:2 loss:1.096793293952942\n",
      "epoch:29,batch:3 loss:1.1116371154785156\n",
      "epoch:29,batch:4 loss:1.125699520111084\n",
      "epoch:29,batch:5 loss:1.1139808893203735\n",
      "epoch:29,batch:6 loss:1.1092933416366577\n",
      "epoch:29,batch:7 loss:1.061637043952942\n",
      "epoch:29,batch:8 loss:1.0936684608459473\n",
      "epoch:29,batch:9 loss:1.090543270111084\n",
      "epoch:29,batch:10 loss:1.1069495677947998\n",
      "epoch:29,batch:11 loss:1.0874183177947998\n",
      "epoch:29,batch:12 loss:1.086637020111084\n",
      "epoch:29,batch:13 loss:1.120230793952942\n",
      "epoch:29,batch:14 loss:1.1155433654785156\n",
      "epoch:29,batch:15 loss:1.0975745916366577\n",
      "epoch:29,batch:16 loss:1.0936682224273682\n",
      "epoch:29,batch:17 loss:1.0889809131622314\n",
      "epoch:29,batch:18 loss:1.0702308416366577\n",
      "epoch:29,batch:19 loss:1.0803871154785156\n",
      "epoch:29,batch:20 loss:1.1014809608459473\n",
      "epoch:29,batch:21 loss:1.1092933416366577\n",
      "epoch:29,batch:22 loss:1.1014809608459473\n",
      "epoch:29,batch:23 loss:1.0921058654785156\n",
      "epoch:29,batch:24 loss:1.0975745916366577\n",
      "epoch:29,batch:25 loss:1.1210120916366577\n",
      "epoch:29,batch:26 loss:1.0850746631622314\n",
      "epoch:29,batch:27 loss:1.0756995677947998\n",
      "epoch:29,batch:28 loss:1.116324543952942\n",
      "epoch:29,batch:29 loss:1.1006996631622314\n",
      "epoch:29,batch:30 loss:1.1139808893203735\n",
      "epoch:29,batch:31 loss:1.088980793952942\n",
      "epoch:29,batch:32 loss:1.0858559608459473\n",
      "epoch:29,batch:33 loss:1.0999183654785156\n",
      "epoch:29,batch:34 loss:1.096793293952942\n",
      "epoch:29,batch:35 loss:1.0741370916366577\n",
      "epoch:29,batch:36 loss:1.1038246154785156\n",
      "epoch:29,batch:37 loss:1.094449520111084\n",
      "epoch:29,batch:38 loss:1.0921058654785156\n",
      "epoch:29,batch:39 loss:1.0788246393203735\n",
      "epoch:29,batch:40 loss:1.0780434608459473\n",
      "epoch:29,batch:41 loss:1.0967934131622314\n",
      "epoch:29,batch:42 loss:1.1006996631622314\n",
      "epoch:29,batch:43 loss:1.108512043952942\n",
      "epoch:29,batch:44 loss:1.106168270111084\n",
      "epoch:29,batch:45 loss:1.1053870916366577\n",
      "epoch:29,batch:46 loss:1.1124184131622314\n",
      "epoch:29,batch:47 loss:1.0717933177947998\n",
      "epoch:29,batch:48 loss:1.0842933654785156\n",
      "epoch:29,batch:49 loss:1.113980770111084\n",
      "epoch:29,batch:50 loss:1.1100746393203735\n",
      "epoch:29,batch:51 loss:1.1171058416366577\n",
      "epoch:29,batch:52 loss:1.1303870677947998\n",
      "epoch:29,batch:53 loss:1.1241371631622314\n",
      "epoch:29,batch:54 loss:1.096793293952942\n",
      "epoch:29,batch:55 loss:1.1178871393203735\n",
      "epoch:29,batch:56 loss:1.0913245677947998\n",
      "epoch:29,batch:57 loss:1.102262020111084\n",
      "epoch:29,batch:58 loss:1.1069495677947998\n",
      "epoch:29,batch:59 loss:1.0999183654785156\n",
      "epoch:29,batch:60 loss:1.1155433654785156\n",
      "epoch:29,batch:61 loss:1.0881996154785156\n",
      "epoch:29,batch:62 loss:1.104605793952942\n",
      "epoch:29,batch:63 loss:1.1046059131622314\n",
      "epoch:29,batch:64 loss:1.092887043952942\n",
      "epoch:29,batch:65 loss:1.1147620677947998\n",
      "epoch:29,batch:66 loss:1.1030433177947998\n",
      "epoch:29,batch:67 loss:1.1030434370040894\n",
      "epoch:29,batch:68 loss:1.120230793952942\n",
      "epoch:29,batch:69 loss:1.098355770111084\n",
      "epoch:29,batch:70 loss:1.094449520111084\n",
      "epoch:29,batch:71 loss:1.090543270111084\n",
      "epoch:29,batch:72 loss:1.0858558416366577\n",
      "epoch:29,batch:73 loss:1.1241371631622314\n",
      "epoch:29,batch:74 loss:1.1303871870040894\n",
      "epoch:29,batch:75 loss:1.108512043952942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [06:41<04:23, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29,batch:76 loss:1.119449496269226\n",
      "epoch:29,batch:77 loss:1.071012020111084\n",
      "epoch:29, train_loss:1.071012020111084\n",
      "epoch:30,batch:0 loss:1.125699520111084\n",
      "epoch:30,batch:1 loss:1.1085121631622314\n",
      "epoch:30,batch:2 loss:1.108512043952942\n",
      "epoch:30,batch:3 loss:1.0827308893203735\n",
      "epoch:30,batch:4 loss:1.096793293952942\n",
      "epoch:30,batch:5 loss:1.1171058416366577\n",
      "epoch:30,batch:6 loss:1.0835120677947998\n",
      "epoch:30,batch:7 loss:1.0788246393203735\n",
      "epoch:30,batch:8 loss:1.1038246154785156\n",
      "epoch:30,batch:9 loss:1.1124184131622314\n",
      "epoch:30,batch:10 loss:1.0991370677947998\n",
      "epoch:30,batch:11 loss:1.116324543952942\n",
      "epoch:30,batch:12 loss:1.0928871631622314\n",
      "epoch:30,batch:13 loss:1.100699543952942\n",
      "epoch:30,batch:14 loss:1.1014809608459473\n",
      "epoch:30,batch:15 loss:1.0835121870040894\n",
      "epoch:30,batch:16 loss:1.086637020111084\n",
      "epoch:30,batch:17 loss:1.1022621393203735\n",
      "epoch:30,batch:18 loss:1.1069495677947998\n",
      "epoch:30,batch:19 loss:1.110074520111084\n",
      "epoch:30,batch:20 loss:1.0842934846878052\n",
      "epoch:30,batch:21 loss:1.1077308654785156\n",
      "epoch:30,batch:22 loss:1.0952308177947998\n",
      "epoch:30,batch:23 loss:1.1092933416366577\n",
      "epoch:30,batch:24 loss:1.104605793952942\n",
      "epoch:30,batch:25 loss:1.098355770111084\n",
      "epoch:30,batch:26 loss:1.0889809131622314\n",
      "epoch:30,batch:27 loss:1.1006996631622314\n",
      "epoch:30,batch:28 loss:1.0975745916366577\n",
      "epoch:30,batch:29 loss:1.0881996154785156\n",
      "epoch:30,batch:30 loss:1.107730746269226\n",
      "epoch:30,batch:31 loss:1.0811684131622314\n",
      "epoch:30,batch:32 loss:1.096793293952942\n",
      "epoch:30,batch:33 loss:1.0913245677947998\n",
      "epoch:30,batch:34 loss:1.0663245916366577\n",
      "epoch:30,batch:35 loss:1.0905433893203735\n",
      "epoch:30,batch:36 loss:1.0991370677947998\n",
      "epoch:30,batch:37 loss:1.1046059131622314\n",
      "epoch:30,batch:38 loss:1.1171058416366577\n",
      "epoch:30,batch:39 loss:1.092105746269226\n",
      "epoch:30,batch:40 loss:1.1077308654785156\n",
      "epoch:30,batch:41 loss:1.0952308177947998\n",
      "epoch:30,batch:42 loss:1.1272621154785156\n",
      "epoch:30,batch:43 loss:1.086637020111084\n",
      "epoch:30,batch:44 loss:1.1069495677947998\n",
      "epoch:30,batch:45 loss:1.1108558177947998\n",
      "epoch:30,batch:46 loss:1.098355770111084\n",
      "epoch:30,batch:47 loss:1.0944496393203735\n",
      "epoch:30,batch:48 loss:1.092887043952942\n",
      "epoch:30,batch:49 loss:1.1131995916366577\n",
      "epoch:30,batch:50 loss:1.1046059131622314\n",
      "epoch:30,batch:51 loss:1.1163246631622314\n",
      "epoch:30,batch:52 loss:1.0983558893203735\n",
      "epoch:30,batch:53 loss:1.117887020111084\n",
      "epoch:30,batch:54 loss:1.1038246154785156\n",
      "epoch:30,batch:55 loss:1.0944496393203735\n",
      "epoch:30,batch:56 loss:1.0967934131622314\n",
      "epoch:30,batch:57 loss:1.0944496393203735\n",
      "epoch:30,batch:58 loss:1.0999183654785156\n",
      "epoch:30,batch:59 loss:1.0967934131622314\n",
      "epoch:30,batch:60 loss:1.0796058177947998\n",
      "epoch:30,batch:61 loss:1.0624182224273682\n",
      "epoch:30,batch:62 loss:1.1014808416366577\n",
      "epoch:30,batch:63 loss:1.092887043952942\n",
      "epoch:30,batch:64 loss:1.0991371870040894\n",
      "epoch:30,batch:65 loss:1.0874183177947998\n",
      "epoch:30,batch:66 loss:1.1147620677947998\n",
      "epoch:30,batch:67 loss:1.096793293952942\n",
      "epoch:30,batch:68 loss:1.1077308654785156\n",
      "epoch:30,batch:69 loss:1.1085119247436523\n",
      "epoch:30,batch:70 loss:1.1178871393203735\n",
      "epoch:30,batch:71 loss:1.0866371393203735\n",
      "epoch:30,batch:72 loss:1.128043293952942\n",
      "epoch:30,batch:73 loss:1.0921058654785156\n",
      "epoch:30,batch:74 loss:1.098355770111084\n",
      "epoch:30,batch:75 loss:1.1335121393203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [06:54<04:10, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30,batch:76 loss:1.1225745677947998\n",
      "epoch:30,batch:77 loss:1.1014807224273682\n",
      "epoch:30, train_loss:1.1014807224273682\n",
      "epoch:31,batch:0 loss:1.0835121870040894\n",
      "epoch:31,batch:1 loss:1.0960121154785156\n",
      "epoch:31,batch:2 loss:1.0631996393203735\n",
      "epoch:31,batch:3 loss:1.0764808654785156\n",
      "epoch:31,batch:4 loss:1.1038246154785156\n",
      "epoch:31,batch:5 loss:1.1022621393203735\n",
      "epoch:31,batch:6 loss:1.0819495916366577\n",
      "epoch:31,batch:7 loss:1.1405434608459473\n",
      "epoch:31,batch:8 loss:1.0874183177947998\n",
      "epoch:31,batch:9 loss:1.0991371870040894\n",
      "epoch:31,batch:10 loss:1.1077308654785156\n",
      "epoch:31,batch:11 loss:1.0850746631622314\n",
      "epoch:31,batch:12 loss:1.0874184370040894\n",
      "epoch:31,batch:13 loss:1.1116371154785156\n",
      "epoch:31,batch:14 loss:1.098355770111084\n",
      "epoch:31,batch:15 loss:1.0936683416366577\n",
      "epoch:31,batch:16 loss:1.0819495916366577\n",
      "epoch:31,batch:17 loss:1.0905433893203735\n",
      "epoch:31,batch:18 loss:1.0975747108459473\n",
      "epoch:31,batch:19 loss:1.1092934608459473\n",
      "epoch:31,batch:20 loss:1.1171058416366577\n",
      "epoch:31,batch:21 loss:1.0788246393203735\n",
      "epoch:31,batch:22 loss:1.0999183654785156\n",
      "epoch:31,batch:23 loss:1.1006996631622314\n",
      "epoch:31,batch:24 loss:1.1163246631622314\n",
      "epoch:31,batch:25 loss:1.1014808416366577\n",
      "epoch:31,batch:26 loss:1.1030434370040894\n",
      "epoch:31,batch:27 loss:1.0960121154785156\n",
      "epoch:31,batch:28 loss:1.0913245677947998\n",
      "epoch:31,batch:29 loss:1.0866371393203735\n",
      "epoch:31,batch:30 loss:1.1303870677947998\n",
      "epoch:31,batch:31 loss:1.112418293952942\n",
      "epoch:31,batch:32 loss:1.1233558654785156\n",
      "epoch:31,batch:33 loss:1.1319496631622314\n",
      "epoch:31,batch:34 loss:1.0881996154785156\n",
      "epoch:31,batch:35 loss:1.094449520111084\n",
      "epoch:31,batch:36 loss:1.0960121154785156\n",
      "epoch:31,batch:37 loss:1.1006996631622314\n",
      "epoch:31,batch:38 loss:1.0881996154785156\n",
      "epoch:31,batch:39 loss:1.0928871631622314\n",
      "epoch:31,batch:40 loss:1.1038246154785156\n",
      "epoch:31,batch:41 loss:1.107730746269226\n",
      "epoch:31,batch:42 loss:1.1108558177947998\n",
      "epoch:31,batch:43 loss:1.1053870916366577\n",
      "epoch:31,batch:44 loss:1.108512043952942\n",
      "epoch:31,batch:45 loss:1.0749183893203735\n",
      "epoch:31,batch:46 loss:1.0796059370040894\n",
      "epoch:31,batch:47 loss:1.0999183654785156\n",
      "epoch:31,batch:48 loss:1.1374183893203735\n",
      "epoch:31,batch:49 loss:1.1022621393203735\n",
      "epoch:31,batch:50 loss:1.113980770111084\n",
      "epoch:31,batch:51 loss:1.1100746393203735\n",
      "epoch:31,batch:52 loss:1.119449496269226\n",
      "epoch:31,batch:53 loss:1.100699543952942\n",
      "epoch:31,batch:54 loss:1.1006996631622314\n",
      "epoch:31,batch:55 loss:1.128043293952942\n",
      "epoch:31,batch:56 loss:1.0991370677947998\n",
      "epoch:31,batch:57 loss:1.1131995916366577\n",
      "epoch:31,batch:58 loss:1.0827308893203735\n",
      "epoch:31,batch:59 loss:1.096793293952942\n",
      "epoch:31,batch:60 loss:1.0827308893203735\n",
      "epoch:31,batch:61 loss:1.1092933416366577\n",
      "epoch:31,batch:62 loss:1.088980793952942\n",
      "epoch:31,batch:63 loss:1.1077308654785156\n",
      "epoch:31,batch:64 loss:1.124137043952942\n",
      "epoch:31,batch:65 loss:1.0967934131622314\n",
      "epoch:31,batch:66 loss:1.1186683177947998\n",
      "epoch:31,batch:67 loss:1.0850746631622314\n",
      "epoch:31,batch:68 loss:1.0741370916366577\n",
      "epoch:31,batch:69 loss:1.0952308177947998\n",
      "epoch:31,batch:70 loss:1.0913245677947998\n",
      "epoch:31,batch:71 loss:1.0741370916366577\n",
      "epoch:31,batch:72 loss:1.094449520111084\n",
      "epoch:31,batch:73 loss:1.1280431747436523\n",
      "epoch:31,batch:74 loss:1.1053870916366577\n",
      "epoch:31,batch:75 loss:1.0967934131622314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [07:07<03:57, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31,batch:76 loss:1.0999183654785156\n",
      "epoch:31,batch:77 loss:1.1053870916366577\n",
      "epoch:31, train_loss:1.1053870916366577\n",
      "epoch:32,batch:0 loss:1.1374183893203735\n",
      "epoch:32,batch:1 loss:1.088980793952942\n",
      "epoch:32,batch:2 loss:1.0975747108459473\n",
      "epoch:32,batch:3 loss:1.1053872108459473\n",
      "epoch:32,batch:4 loss:1.1241371631622314\n",
      "epoch:32,batch:5 loss:1.1077308654785156\n",
      "epoch:32,batch:6 loss:1.108512043952942\n",
      "epoch:32,batch:7 loss:1.0874183177947998\n",
      "epoch:32,batch:8 loss:1.1069496870040894\n",
      "epoch:32,batch:9 loss:1.0991370677947998\n",
      "epoch:32,batch:10 loss:1.1327308416366577\n",
      "epoch:32,batch:11 loss:1.0827308893203735\n",
      "epoch:32,batch:12 loss:1.1053870916366577\n",
      "epoch:32,batch:13 loss:1.0819494724273682\n",
      "epoch:32,batch:14 loss:1.096011996269226\n",
      "epoch:32,batch:15 loss:1.0913245677947998\n",
      "epoch:32,batch:16 loss:1.1264808177947998\n",
      "epoch:32,batch:17 loss:1.0952308177947998\n",
      "epoch:32,batch:18 loss:1.0952308177947998\n",
      "epoch:32,batch:19 loss:1.094449520111084\n",
      "epoch:32,batch:20 loss:1.106168270111084\n",
      "epoch:32,batch:21 loss:1.1030433177947998\n",
      "epoch:32,batch:22 loss:1.0999183654785156\n",
      "epoch:32,batch:23 loss:1.0827308893203735\n",
      "epoch:32,batch:24 loss:1.1053870916366577\n",
      "epoch:32,batch:25 loss:1.1069495677947998\n",
      "epoch:32,batch:26 loss:1.0897620916366577\n",
      "epoch:32,batch:27 loss:1.0874183177947998\n",
      "epoch:32,batch:28 loss:1.1116371154785156\n",
      "epoch:32,batch:29 loss:1.128043293952942\n",
      "epoch:32,batch:30 loss:1.078824520111084\n",
      "epoch:32,batch:31 loss:1.0952308177947998\n",
      "epoch:32,batch:32 loss:1.0850746631622314\n",
      "epoch:32,batch:33 loss:1.1131995916366577\n",
      "epoch:32,batch:34 loss:1.1061683893203735\n",
      "epoch:32,batch:35 loss:1.1077308654785156\n",
      "epoch:32,batch:36 loss:1.1022621393203735\n",
      "epoch:32,batch:37 loss:1.0881996154785156\n",
      "epoch:32,batch:38 loss:1.1163246631622314\n",
      "epoch:32,batch:39 loss:1.1116371154785156\n",
      "epoch:32,batch:40 loss:1.108512043952942\n",
      "epoch:32,batch:41 loss:1.123355746269226\n",
      "epoch:32,batch:42 loss:1.110074520111084\n",
      "epoch:32,batch:43 loss:1.0967934131622314\n",
      "epoch:32,batch:44 loss:1.085074543952942\n",
      "epoch:32,batch:45 loss:1.0991371870040894\n",
      "epoch:32,batch:46 loss:1.092887043952942\n",
      "epoch:32,batch:47 loss:1.0819494724273682\n",
      "epoch:32,batch:48 loss:1.0889809131622314\n",
      "epoch:32,batch:49 loss:1.1030433177947998\n",
      "epoch:32,batch:50 loss:1.1030433177947998\n",
      "epoch:32,batch:51 loss:1.111636996269226\n",
      "epoch:32,batch:52 loss:1.102262020111084\n",
      "epoch:32,batch:53 loss:1.1100746393203735\n",
      "epoch:32,batch:54 loss:1.113980770111084\n",
      "epoch:32,batch:55 loss:1.094449520111084\n",
      "epoch:32,batch:56 loss:1.1077308654785156\n",
      "epoch:32,batch:57 loss:1.0960121154785156\n",
      "epoch:32,batch:58 loss:1.0835121870040894\n",
      "epoch:32,batch:59 loss:1.059293270111084\n",
      "epoch:32,batch:60 loss:1.1147621870040894\n",
      "epoch:32,batch:61 loss:1.1030433177947998\n",
      "epoch:32,batch:62 loss:1.086637020111084\n",
      "epoch:32,batch:63 loss:1.0975745916366577\n",
      "epoch:32,batch:64 loss:1.1131995916366577\n",
      "epoch:32,batch:65 loss:1.0960121154785156\n",
      "epoch:32,batch:66 loss:1.1155433654785156\n",
      "epoch:32,batch:67 loss:1.121793270111084\n",
      "epoch:32,batch:68 loss:1.0967934131622314\n",
      "epoch:32,batch:69 loss:1.0858557224273682\n",
      "epoch:32,batch:70 loss:1.0803871154785156\n",
      "epoch:32,batch:71 loss:1.0842933654785156\n",
      "epoch:32,batch:72 loss:1.0936683416366577\n",
      "epoch:32,batch:73 loss:1.1014807224273682\n",
      "epoch:32,batch:74 loss:1.1022621393203735\n",
      "epoch:32,batch:75 loss:1.0780433416366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [07:20<03:43, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32,batch:76 loss:1.096793293952942\n",
      "epoch:32,batch:77 loss:1.102262020111084\n",
      "epoch:32, train_loss:1.102262020111084\n",
      "epoch:33,batch:0 loss:1.106168270111084\n",
      "epoch:33,batch:1 loss:1.0881997346878052\n",
      "epoch:33,batch:2 loss:1.0936683416366577\n",
      "epoch:33,batch:3 loss:1.1014808416366577\n",
      "epoch:33,batch:4 loss:1.0921058654785156\n",
      "epoch:33,batch:5 loss:1.1163246631622314\n",
      "epoch:33,batch:6 loss:1.1030433177947998\n",
      "epoch:33,batch:7 loss:1.0991371870040894\n",
      "epoch:33,batch:8 loss:1.0850746631622314\n",
      "epoch:33,batch:9 loss:1.0772621631622314\n",
      "epoch:33,batch:10 loss:1.1155433654785156\n",
      "epoch:33,batch:11 loss:1.080386996269226\n",
      "epoch:33,batch:12 loss:1.1116371154785156\n",
      "epoch:33,batch:13 loss:1.0999183654785156\n",
      "epoch:33,batch:14 loss:1.119449496269226\n",
      "epoch:33,batch:15 loss:1.106168508529663\n",
      "epoch:33,batch:16 loss:1.0991370677947998\n",
      "epoch:33,batch:17 loss:1.1069495677947998\n",
      "epoch:33,batch:18 loss:1.094449520111084\n",
      "epoch:33,batch:19 loss:1.1053870916366577\n",
      "epoch:33,batch:20 loss:1.0960121154785156\n",
      "epoch:33,batch:21 loss:1.0960121154785156\n",
      "epoch:33,batch:22 loss:1.1194496154785156\n",
      "epoch:33,batch:23 loss:1.1092933416366577\n",
      "epoch:33,batch:24 loss:1.1006996631622314\n",
      "epoch:33,batch:25 loss:1.1116371154785156\n",
      "epoch:33,batch:26 loss:1.0788246393203735\n",
      "epoch:33,batch:27 loss:1.0835120677947998\n",
      "epoch:33,batch:28 loss:1.092887043952942\n",
      "epoch:33,batch:29 loss:1.1108558177947998\n",
      "epoch:33,batch:30 loss:1.1124184131622314\n",
      "epoch:33,batch:31 loss:1.113980770111084\n",
      "epoch:33,batch:32 loss:1.1077308654785156\n",
      "epoch:33,batch:33 loss:1.0936683416366577\n",
      "epoch:33,batch:34 loss:1.086637020111084\n",
      "epoch:33,batch:35 loss:1.0991370677947998\n",
      "epoch:33,batch:36 loss:1.090543270111084\n",
      "epoch:33,batch:37 loss:1.1006996631622314\n",
      "epoch:33,batch:38 loss:1.086637020111084\n",
      "epoch:33,batch:39 loss:1.0999183654785156\n",
      "epoch:33,batch:40 loss:1.0717933177947998\n",
      "epoch:33,batch:41 loss:1.104605793952942\n",
      "epoch:33,batch:42 loss:1.1077308654785156\n",
      "epoch:33,batch:43 loss:1.1100746393203735\n",
      "epoch:33,batch:44 loss:1.082730770111084\n",
      "epoch:33,batch:45 loss:1.1085121631622314\n",
      "epoch:33,batch:46 loss:1.1210119724273682\n",
      "epoch:33,batch:47 loss:1.1077308654785156\n",
      "epoch:33,batch:48 loss:1.092105746269226\n",
      "epoch:33,batch:49 loss:1.0897622108459473\n",
      "epoch:33,batch:50 loss:1.1131995916366577\n",
      "epoch:33,batch:51 loss:1.1100746393203735\n",
      "epoch:33,batch:52 loss:1.092887043952942\n",
      "epoch:33,batch:53 loss:1.100699543952942\n",
      "epoch:33,batch:54 loss:1.1038246154785156\n",
      "epoch:33,batch:55 loss:1.1030433177947998\n",
      "epoch:33,batch:56 loss:1.0874183177947998\n",
      "epoch:33,batch:57 loss:1.1155433654785156\n",
      "epoch:33,batch:58 loss:1.0756995677947998\n",
      "epoch:33,batch:59 loss:1.0936683416366577\n",
      "epoch:33,batch:60 loss:1.1014808416366577\n",
      "epoch:33,batch:61 loss:1.1194496154785156\n",
      "epoch:33,batch:62 loss:1.1178871393203735\n",
      "epoch:33,batch:63 loss:1.0835120677947998\n",
      "epoch:33,batch:64 loss:1.1155433654785156\n",
      "epoch:33,batch:65 loss:1.0803871154785156\n",
      "epoch:33,batch:66 loss:1.098355770111084\n",
      "epoch:33,batch:67 loss:1.1014808416366577\n",
      "epoch:33,batch:68 loss:1.1014808416366577\n",
      "epoch:33,batch:69 loss:1.1171057224273682\n",
      "epoch:33,batch:70 loss:1.1100746393203735\n",
      "epoch:33,batch:71 loss:1.0796058177947998\n",
      "epoch:33,batch:72 loss:1.100699543952942\n",
      "epoch:33,batch:73 loss:1.1061683893203735\n",
      "epoch:33,batch:74 loss:1.1155433654785156\n",
      "epoch:33,batch:75 loss:1.1131995916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [07:34<03:30, 13.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33,batch:76 loss:1.0928871631622314\n",
      "epoch:33,batch:77 loss:1.0913245677947998\n",
      "epoch:33, train_loss:1.0913245677947998\n",
      "epoch:34,batch:0 loss:1.0960121154785156\n",
      "epoch:34,batch:1 loss:1.1053872108459473\n",
      "epoch:34,batch:2 loss:1.1108558177947998\n",
      "epoch:34,batch:3 loss:1.1038246154785156\n",
      "epoch:34,batch:4 loss:1.0897620916366577\n",
      "epoch:34,batch:5 loss:1.073355793952942\n",
      "epoch:34,batch:6 loss:1.0897622108459473\n",
      "epoch:34,batch:7 loss:1.094449520111084\n",
      "epoch:34,batch:8 loss:1.092887043952942\n",
      "epoch:34,batch:9 loss:1.088199496269226\n",
      "epoch:34,batch:10 loss:1.1171059608459473\n",
      "epoch:34,batch:11 loss:1.1311683654785156\n",
      "epoch:34,batch:12 loss:1.1100746393203735\n",
      "epoch:34,batch:13 loss:1.106168508529663\n",
      "epoch:34,batch:14 loss:1.0952309370040894\n",
      "epoch:34,batch:15 loss:1.081168293952942\n",
      "epoch:34,batch:16 loss:1.1155433654785156\n",
      "epoch:34,batch:17 loss:1.1030433177947998\n",
      "epoch:34,batch:18 loss:1.1053870916366577\n",
      "epoch:34,batch:19 loss:1.0999183654785156\n",
      "epoch:34,batch:20 loss:1.0905433893203735\n",
      "epoch:34,batch:21 loss:1.0874183177947998\n",
      "epoch:34,batch:22 loss:1.0952308177947998\n",
      "epoch:34,batch:23 loss:1.1171057224273682\n",
      "epoch:34,batch:24 loss:1.0686683654785156\n",
      "epoch:34,batch:25 loss:1.1030434370040894\n",
      "epoch:34,batch:26 loss:1.1108559370040894\n",
      "epoch:34,batch:27 loss:1.0936683416366577\n",
      "epoch:34,batch:28 loss:1.085074543952942\n",
      "epoch:34,batch:29 loss:1.1030433177947998\n",
      "epoch:34,batch:30 loss:1.0881996154785156\n",
      "epoch:34,batch:31 loss:1.100699543952942\n",
      "epoch:34,batch:32 loss:1.0960121154785156\n",
      "epoch:34,batch:33 loss:1.1296058893203735\n",
      "epoch:34,batch:34 loss:1.0842933654785156\n",
      "epoch:34,batch:35 loss:1.1085121631622314\n",
      "epoch:34,batch:36 loss:1.0671058893203735\n",
      "epoch:34,batch:37 loss:1.1038246154785156\n",
      "epoch:34,batch:38 loss:1.0991370677947998\n",
      "epoch:34,batch:39 loss:1.1085121631622314\n",
      "epoch:34,batch:40 loss:1.0827308893203735\n",
      "epoch:34,batch:41 loss:1.1030433177947998\n",
      "epoch:34,batch:42 loss:1.100699543952942\n",
      "epoch:34,batch:43 loss:1.1163246631622314\n",
      "epoch:34,batch:44 loss:1.0975745916366577\n",
      "epoch:34,batch:45 loss:1.102262020111084\n",
      "epoch:34,batch:46 loss:1.0944496393203735\n",
      "epoch:34,batch:47 loss:1.1108559370040894\n",
      "epoch:34,batch:48 loss:1.112418293952942\n",
      "epoch:34,batch:49 loss:1.1233558654785156\n",
      "epoch:34,batch:50 loss:1.0897620916366577\n",
      "epoch:34,batch:51 loss:1.1116371154785156\n",
      "epoch:34,batch:52 loss:1.0897620916366577\n",
      "epoch:34,batch:53 loss:1.0975745916366577\n",
      "epoch:34,batch:54 loss:1.1108556985855103\n",
      "epoch:34,batch:55 loss:1.0913245677947998\n",
      "epoch:34,batch:56 loss:1.0936683416366577\n",
      "epoch:34,batch:57 loss:1.1108556985855103\n",
      "epoch:34,batch:58 loss:1.072574496269226\n",
      "epoch:34,batch:59 loss:1.1053870916366577\n",
      "epoch:34,batch:60 loss:1.1038246154785156\n",
      "epoch:34,batch:61 loss:1.0952309370040894\n",
      "epoch:34,batch:62 loss:1.0944496393203735\n",
      "epoch:34,batch:63 loss:1.0983558893203735\n",
      "epoch:34,batch:64 loss:1.0897620916366577\n",
      "epoch:34,batch:65 loss:1.102262020111084\n",
      "epoch:34,batch:66 loss:1.113980770111084\n",
      "epoch:34,batch:67 loss:1.1108558177947998\n",
      "epoch:34,batch:68 loss:1.110074520111084\n",
      "epoch:34,batch:69 loss:1.0905433893203735\n",
      "epoch:34,batch:70 loss:1.1210122108459473\n",
      "epoch:34,batch:71 loss:1.0960121154785156\n",
      "epoch:34,batch:72 loss:1.1077308654785156\n",
      "epoch:34,batch:73 loss:1.108512043952942\n",
      "epoch:34,batch:74 loss:1.1069495677947998\n",
      "epoch:34,batch:75 loss:1.0944496393203735\n",
      "epoch:34,batch:76 loss:1.0999183654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [07:47<03:17, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34,batch:77 loss:1.125699520111084\n",
      "epoch:34, train_loss:1.125699520111084\n",
      "epoch:35,batch:0 loss:1.1147620677947998\n",
      "epoch:35,batch:1 loss:1.106168270111084\n",
      "epoch:35,batch:2 loss:1.0788246393203735\n",
      "epoch:35,batch:3 loss:1.0881996154785156\n",
      "epoch:35,batch:4 loss:1.100699543952942\n",
      "epoch:35,batch:5 loss:1.1069495677947998\n",
      "epoch:35,batch:6 loss:1.1131995916366577\n",
      "epoch:35,batch:7 loss:1.110074520111084\n",
      "epoch:35,batch:8 loss:1.0764808654785156\n",
      "epoch:35,batch:9 loss:1.082730770111084\n",
      "epoch:35,batch:10 loss:1.0819495916366577\n",
      "epoch:35,batch:11 loss:1.094449520111084\n",
      "epoch:35,batch:12 loss:1.1077308654785156\n",
      "epoch:35,batch:13 loss:1.1085121631622314\n",
      "epoch:35,batch:14 loss:1.0952308177947998\n",
      "epoch:35,batch:15 loss:1.0874183177947998\n",
      "epoch:35,batch:16 loss:1.1092932224273682\n",
      "epoch:35,batch:17 loss:1.117887020111084\n",
      "epoch:35,batch:18 loss:1.115543246269226\n",
      "epoch:35,batch:19 loss:1.0835120677947998\n",
      "epoch:35,batch:20 loss:1.1249183416366577\n",
      "epoch:35,batch:21 loss:1.078824520111084\n",
      "epoch:35,batch:22 loss:1.090543270111084\n",
      "epoch:35,batch:23 loss:1.1053870916366577\n",
      "epoch:35,batch:24 loss:1.1006996631622314\n",
      "epoch:35,batch:25 loss:1.0897620916366577\n",
      "epoch:35,batch:26 loss:1.1124184131622314\n",
      "epoch:35,batch:27 loss:1.1163246631622314\n",
      "epoch:35,batch:28 loss:1.0796059370040894\n",
      "epoch:35,batch:29 loss:1.1100746393203735\n",
      "epoch:35,batch:30 loss:1.082730770111084\n",
      "epoch:35,batch:31 loss:1.1366370916366577\n",
      "epoch:35,batch:32 loss:1.1030433177947998\n",
      "epoch:35,batch:33 loss:1.1147620677947998\n",
      "epoch:35,batch:34 loss:1.1014809608459473\n",
      "epoch:35,batch:35 loss:1.099918246269226\n",
      "epoch:35,batch:36 loss:1.1327309608459473\n",
      "epoch:35,batch:37 loss:1.085074543952942\n",
      "epoch:35,batch:38 loss:1.0967934131622314\n",
      "epoch:35,batch:39 loss:1.110074520111084\n",
      "epoch:35,batch:40 loss:1.0913245677947998\n",
      "epoch:35,batch:41 loss:1.1100746393203735\n",
      "epoch:35,batch:42 loss:1.0741370916366577\n",
      "epoch:35,batch:43 loss:1.1171058416366577\n",
      "epoch:35,batch:44 loss:1.0960121154785156\n",
      "epoch:35,batch:45 loss:1.1069495677947998\n",
      "epoch:35,batch:46 loss:1.112418293952942\n",
      "epoch:35,batch:47 loss:1.084293246269226\n",
      "epoch:35,batch:48 loss:1.1194496154785156\n",
      "epoch:35,batch:49 loss:1.1038246154785156\n",
      "epoch:35,batch:50 loss:1.1116371154785156\n",
      "epoch:35,batch:51 loss:1.0881996154785156\n",
      "epoch:35,batch:52 loss:1.102262020111084\n",
      "epoch:35,batch:53 loss:1.086637020111084\n",
      "epoch:35,batch:54 loss:1.0975745916366577\n",
      "epoch:35,batch:55 loss:1.1053870916366577\n",
      "epoch:35,batch:56 loss:1.112418293952942\n",
      "epoch:35,batch:57 loss:1.0866371393203735\n",
      "epoch:35,batch:58 loss:1.1139808893203735\n",
      "epoch:35,batch:59 loss:1.0967931747436523\n",
      "epoch:35,batch:60 loss:1.106168270111084\n",
      "epoch:35,batch:61 loss:1.1053869724273682\n",
      "epoch:35,batch:62 loss:1.0858558416366577\n",
      "epoch:35,batch:63 loss:1.0975745916366577\n",
      "epoch:35,batch:64 loss:1.106168270111084\n",
      "epoch:35,batch:65 loss:1.0967934131622314\n",
      "epoch:35,batch:66 loss:1.106168270111084\n",
      "epoch:35,batch:67 loss:1.090543270111084\n",
      "epoch:35,batch:68 loss:1.092887043952942\n",
      "epoch:35,batch:69 loss:1.102262020111084\n",
      "epoch:35,batch:70 loss:1.0967934131622314\n",
      "epoch:35,batch:71 loss:1.0913245677947998\n",
      "epoch:35,batch:72 loss:1.0928871631622314\n",
      "epoch:35,batch:73 loss:1.099918246269226\n",
      "epoch:35,batch:74 loss:1.088980793952942\n",
      "epoch:35,batch:75 loss:1.080386996269226\n",
      "epoch:35,batch:76 loss:1.1186683177947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [08:00<03:04, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35,batch:77 loss:1.0999183654785156\n",
      "epoch:35, train_loss:1.0999183654785156\n",
      "epoch:36,batch:0 loss:1.1108558177947998\n",
      "epoch:36,batch:1 loss:1.1131994724273682\n",
      "epoch:36,batch:2 loss:1.1069495677947998\n",
      "epoch:36,batch:3 loss:1.1053870916366577\n",
      "epoch:36,batch:4 loss:1.1131995916366577\n",
      "epoch:36,batch:5 loss:1.102262020111084\n",
      "epoch:36,batch:6 loss:1.0928871631622314\n",
      "epoch:36,batch:7 loss:1.1092933416366577\n",
      "epoch:36,batch:8 loss:1.0928871631622314\n",
      "epoch:36,batch:9 loss:1.0858558416366577\n",
      "epoch:36,batch:10 loss:1.094449520111084\n",
      "epoch:36,batch:11 loss:1.1319496631622314\n",
      "epoch:36,batch:12 loss:1.0913246870040894\n",
      "epoch:36,batch:13 loss:1.1069495677947998\n",
      "epoch:36,batch:14 loss:1.127261996269226\n",
      "epoch:36,batch:15 loss:1.0881996154785156\n",
      "epoch:36,batch:16 loss:1.0881996154785156\n",
      "epoch:36,batch:17 loss:1.0936683416366577\n",
      "epoch:36,batch:18 loss:1.1069495677947998\n",
      "epoch:36,batch:19 loss:1.1163246631622314\n",
      "epoch:36,batch:20 loss:1.0944496393203735\n",
      "epoch:36,batch:21 loss:1.0913245677947998\n",
      "epoch:36,batch:22 loss:1.0702308416366577\n",
      "epoch:36,batch:23 loss:1.0749183893203735\n",
      "epoch:36,batch:24 loss:1.078824520111084\n",
      "epoch:36,batch:25 loss:1.100699543952942\n",
      "epoch:36,batch:26 loss:1.0921058654785156\n",
      "epoch:36,batch:27 loss:1.0850746631622314\n",
      "epoch:36,batch:28 loss:1.0913246870040894\n",
      "epoch:36,batch:29 loss:1.1288245916366577\n",
      "epoch:36,batch:30 loss:1.110074520111084\n",
      "epoch:36,batch:31 loss:1.1092933416366577\n",
      "epoch:36,batch:32 loss:1.112418293952942\n",
      "epoch:36,batch:33 loss:1.1069495677947998\n",
      "epoch:36,batch:34 loss:1.094449520111084\n",
      "epoch:36,batch:35 loss:1.0999183654785156\n",
      "epoch:36,batch:36 loss:1.0889809131622314\n",
      "epoch:36,batch:37 loss:1.1264808177947998\n",
      "epoch:36,batch:38 loss:1.0936683416366577\n",
      "epoch:36,batch:39 loss:1.1264808177947998\n",
      "epoch:36,batch:40 loss:1.1069495677947998\n",
      "epoch:36,batch:41 loss:1.1202309131622314\n",
      "epoch:36,batch:42 loss:1.077262043952942\n",
      "epoch:36,batch:43 loss:1.0991370677947998\n",
      "epoch:36,batch:44 loss:1.0858559608459473\n",
      "epoch:36,batch:45 loss:1.0975744724273682\n",
      "epoch:36,batch:46 loss:1.086637020111084\n",
      "epoch:36,batch:47 loss:1.0803871154785156\n",
      "epoch:36,batch:48 loss:1.0874184370040894\n",
      "epoch:36,batch:49 loss:1.124137043952942\n",
      "epoch:36,batch:50 loss:1.088980793952942\n",
      "epoch:36,batch:51 loss:1.0983558893203735\n",
      "epoch:36,batch:52 loss:1.086637020111084\n",
      "epoch:36,batch:53 loss:1.1225745677947998\n",
      "epoch:36,batch:54 loss:1.0835120677947998\n",
      "epoch:36,batch:55 loss:1.1014808416366577\n",
      "epoch:36,batch:56 loss:1.1241371631622314\n",
      "epoch:36,batch:57 loss:1.1210119724273682\n",
      "epoch:36,batch:58 loss:1.1006996631622314\n",
      "epoch:36,batch:59 loss:1.0952308177947998\n",
      "epoch:36,batch:60 loss:1.1124184131622314\n",
      "epoch:36,batch:61 loss:1.124137043952942\n",
      "epoch:36,batch:62 loss:1.0803871154785156\n",
      "epoch:36,batch:63 loss:1.1085121631622314\n",
      "epoch:36,batch:64 loss:1.0944496393203735\n",
      "epoch:36,batch:65 loss:1.098355770111084\n",
      "epoch:36,batch:66 loss:1.0991369485855103\n",
      "epoch:36,batch:67 loss:1.0835120677947998\n",
      "epoch:36,batch:68 loss:1.0960121154785156\n",
      "epoch:36,batch:69 loss:1.1092933416366577\n",
      "epoch:36,batch:70 loss:1.1030433177947998\n",
      "epoch:36,batch:71 loss:1.0694494247436523\n",
      "epoch:36,batch:72 loss:1.1014808416366577\n",
      "epoch:36,batch:73 loss:1.074918270111084\n",
      "epoch:36,batch:74 loss:1.1030434370040894\n",
      "epoch:36,batch:75 loss:1.0991370677947998\n",
      "epoch:36,batch:76 loss:1.107730746269226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [08:13<02:50, 13.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36,batch:77 loss:1.1272621154785156\n",
      "epoch:36, train_loss:1.1272621154785156\n",
      "epoch:37,batch:0 loss:1.072574496269226\n",
      "epoch:37,batch:1 loss:1.092887043952942\n",
      "epoch:37,batch:2 loss:1.0999183654785156\n",
      "epoch:37,batch:3 loss:1.1006996631622314\n",
      "epoch:37,batch:4 loss:1.076480746269226\n",
      "epoch:37,batch:5 loss:1.0967934131622314\n",
      "epoch:37,batch:6 loss:1.086637020111084\n",
      "epoch:37,batch:7 loss:1.1006996631622314\n",
      "epoch:37,batch:8 loss:1.1186683177947998\n",
      "epoch:37,batch:9 loss:1.100699543952942\n",
      "epoch:37,batch:10 loss:1.0952308177947998\n",
      "epoch:37,batch:11 loss:1.1108558177947998\n",
      "epoch:37,batch:12 loss:1.110074520111084\n",
      "epoch:37,batch:13 loss:1.1194496154785156\n",
      "epoch:37,batch:14 loss:1.1342933177947998\n",
      "epoch:37,batch:15 loss:1.098355770111084\n",
      "epoch:37,batch:16 loss:1.0928871631622314\n",
      "epoch:37,batch:17 loss:1.1053870916366577\n",
      "epoch:37,batch:18 loss:1.1014808416366577\n",
      "epoch:37,batch:19 loss:1.124137043952942\n",
      "epoch:37,batch:20 loss:1.1069496870040894\n",
      "epoch:37,batch:21 loss:1.094449520111084\n",
      "epoch:37,batch:22 loss:1.0819495916366577\n",
      "epoch:37,batch:23 loss:1.108512043952942\n",
      "epoch:37,batch:24 loss:1.0725746154785156\n",
      "epoch:37,batch:25 loss:1.1014808416366577\n",
      "epoch:37,batch:26 loss:1.0999183654785156\n",
      "epoch:37,batch:27 loss:1.1163246631622314\n",
      "epoch:37,batch:28 loss:1.0889809131622314\n",
      "epoch:37,batch:29 loss:1.0850746631622314\n",
      "epoch:37,batch:30 loss:1.113980770111084\n",
      "epoch:37,batch:31 loss:1.110074520111084\n",
      "epoch:37,batch:32 loss:1.0960121154785156\n",
      "epoch:37,batch:33 loss:1.1006996631622314\n",
      "epoch:37,batch:34 loss:1.1194496154785156\n",
      "epoch:37,batch:35 loss:1.0928871631622314\n",
      "epoch:37,batch:36 loss:1.0952306985855103\n",
      "epoch:37,batch:37 loss:1.0866371393203735\n",
      "epoch:37,batch:38 loss:1.0960121154785156\n",
      "epoch:37,batch:39 loss:1.102262020111084\n",
      "epoch:37,batch:40 loss:1.1022621393203735\n",
      "epoch:37,batch:41 loss:1.1038246154785156\n",
      "epoch:37,batch:42 loss:1.0921058654785156\n",
      "epoch:37,batch:43 loss:1.1147620677947998\n",
      "epoch:37,batch:44 loss:1.1014809608459473\n",
      "epoch:37,batch:45 loss:1.090543270111084\n",
      "epoch:37,batch:46 loss:1.1085121631622314\n",
      "epoch:37,batch:47 loss:1.0741370916366577\n",
      "epoch:37,batch:48 loss:1.1038246154785156\n",
      "epoch:37,batch:49 loss:1.0913245677947998\n",
      "epoch:37,batch:50 loss:1.086637020111084\n",
      "epoch:37,batch:51 loss:1.1014809608459473\n",
      "epoch:37,batch:52 loss:1.1014808416366577\n",
      "epoch:37,batch:53 loss:1.0827308893203735\n",
      "epoch:37,batch:54 loss:1.111636996269226\n",
      "epoch:37,batch:55 loss:1.0928871631622314\n",
      "epoch:37,batch:56 loss:1.0983558893203735\n",
      "epoch:37,batch:57 loss:1.1014808416366577\n",
      "epoch:37,batch:58 loss:1.1069496870040894\n",
      "epoch:37,batch:59 loss:1.1030433177947998\n",
      "epoch:37,batch:60 loss:1.110074520111084\n",
      "epoch:37,batch:61 loss:1.1077308654785156\n",
      "epoch:37,batch:62 loss:1.0960121154785156\n",
      "epoch:37,batch:63 loss:1.1116371154785156\n",
      "epoch:37,batch:64 loss:1.113980770111084\n",
      "epoch:37,batch:65 loss:1.120230793952942\n",
      "epoch:37,batch:66 loss:1.1100746393203735\n",
      "epoch:37,batch:67 loss:1.094449520111084\n",
      "epoch:37,batch:68 loss:1.0858558416366577\n",
      "epoch:37,batch:69 loss:1.0991370677947998\n",
      "epoch:37,batch:70 loss:1.1116371154785156\n",
      "epoch:37,batch:71 loss:1.1046059131622314\n",
      "epoch:37,batch:72 loss:1.104605793952942\n",
      "epoch:37,batch:73 loss:1.096011996269226\n",
      "epoch:37,batch:74 loss:1.1030433177947998\n",
      "epoch:37,batch:75 loss:1.0960121154785156\n",
      "epoch:37,batch:76 loss:1.0921058654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [08:26<02:38, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37,batch:77 loss:1.0952308177947998\n",
      "epoch:37, train_loss:1.0952308177947998\n",
      "epoch:38,batch:0 loss:1.0842933654785156\n",
      "epoch:38,batch:1 loss:1.092887043952942\n",
      "epoch:38,batch:2 loss:1.1225745677947998\n",
      "epoch:38,batch:3 loss:1.1038246154785156\n",
      "epoch:38,batch:4 loss:1.098355770111084\n",
      "epoch:38,batch:5 loss:1.1006996631622314\n",
      "epoch:38,batch:6 loss:1.0803871154785156\n",
      "epoch:38,batch:7 loss:1.1069495677947998\n",
      "epoch:38,batch:8 loss:1.1030433177947998\n",
      "epoch:38,batch:9 loss:1.1038246154785156\n",
      "epoch:38,batch:10 loss:1.1155433654785156\n",
      "epoch:38,batch:11 loss:1.1061683893203735\n",
      "epoch:38,batch:12 loss:1.0991370677947998\n",
      "epoch:38,batch:13 loss:1.1327308416366577\n",
      "epoch:38,batch:14 loss:1.1147620677947998\n",
      "epoch:38,batch:15 loss:1.1069495677947998\n",
      "epoch:38,batch:16 loss:1.110074520111084\n",
      "epoch:38,batch:17 loss:1.098355770111084\n",
      "epoch:38,batch:18 loss:1.1046059131622314\n",
      "epoch:38,batch:19 loss:1.1014809608459473\n",
      "epoch:38,batch:20 loss:1.0905433893203735\n",
      "epoch:38,batch:21 loss:1.110074520111084\n",
      "epoch:38,batch:22 loss:1.0850744247436523\n",
      "epoch:38,batch:23 loss:1.106168270111084\n",
      "epoch:38,batch:24 loss:1.0881996154785156\n",
      "epoch:38,batch:25 loss:1.0967934131622314\n",
      "epoch:38,batch:26 loss:1.0913244485855103\n",
      "epoch:38,batch:27 loss:1.0780433416366577\n",
      "epoch:38,batch:28 loss:1.1155433654785156\n",
      "epoch:38,batch:29 loss:1.0881996154785156\n",
      "epoch:38,batch:30 loss:1.1147621870040894\n",
      "epoch:38,batch:31 loss:1.0835120677947998\n",
      "epoch:38,batch:32 loss:1.0874184370040894\n",
      "epoch:38,batch:33 loss:1.098355770111084\n",
      "epoch:38,batch:34 loss:1.0983558893203735\n",
      "epoch:38,batch:35 loss:1.1171058416366577\n",
      "epoch:38,batch:36 loss:1.0991370677947998\n",
      "epoch:38,batch:37 loss:1.0850746631622314\n",
      "epoch:38,batch:38 loss:1.088980793952942\n",
      "epoch:38,batch:39 loss:1.1077308654785156\n",
      "epoch:38,batch:40 loss:1.100699543952942\n",
      "epoch:38,batch:41 loss:1.1006996631622314\n",
      "epoch:38,batch:42 loss:1.1053872108459473\n",
      "epoch:38,batch:43 loss:1.1131997108459473\n",
      "epoch:38,batch:44 loss:1.0866371393203735\n",
      "epoch:38,batch:45 loss:1.0983558893203735\n",
      "epoch:38,batch:46 loss:1.1100746393203735\n",
      "epoch:38,batch:47 loss:1.0983558893203735\n",
      "epoch:38,batch:48 loss:1.0835121870040894\n",
      "epoch:38,batch:49 loss:1.113980770111084\n",
      "epoch:38,batch:50 loss:1.096793293952942\n",
      "epoch:38,batch:51 loss:1.1046059131622314\n",
      "epoch:38,batch:52 loss:1.1053870916366577\n",
      "epoch:38,batch:53 loss:1.102262020111084\n",
      "epoch:38,batch:54 loss:1.1014807224273682\n",
      "epoch:38,batch:55 loss:1.1085121631622314\n",
      "epoch:38,batch:56 loss:1.108512043952942\n",
      "epoch:38,batch:57 loss:1.0975747108459473\n",
      "epoch:38,batch:58 loss:1.0960121154785156\n",
      "epoch:38,batch:59 loss:1.0725746154785156\n",
      "epoch:38,batch:60 loss:1.0936683416366577\n",
      "epoch:38,batch:61 loss:1.1006996631622314\n",
      "epoch:38,batch:62 loss:1.1038246154785156\n",
      "epoch:38,batch:63 loss:1.098355770111084\n",
      "epoch:38,batch:64 loss:1.0874183177947998\n",
      "epoch:38,batch:65 loss:1.1014808416366577\n",
      "epoch:38,batch:66 loss:1.1014808416366577\n",
      "epoch:38,batch:67 loss:1.1108558177947998\n",
      "epoch:38,batch:68 loss:1.107730746269226\n",
      "epoch:38,batch:69 loss:1.1194496154785156\n",
      "epoch:38,batch:70 loss:1.124137043952942\n",
      "epoch:38,batch:71 loss:1.0967934131622314\n",
      "epoch:38,batch:72 loss:1.0858557224273682\n",
      "epoch:38,batch:73 loss:1.1092933416366577\n",
      "epoch:38,batch:74 loss:1.0897622108459473\n",
      "epoch:38,batch:75 loss:1.0983558893203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [08:41<02:29, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38,batch:76 loss:1.082730770111084\n",
      "epoch:38,batch:77 loss:1.098355770111084\n",
      "epoch:38, train_loss:1.098355770111084\n",
      "epoch:39,batch:0 loss:1.1030433177947998\n",
      "epoch:39,batch:1 loss:1.1194496154785156\n",
      "epoch:39,batch:2 loss:1.0960121154785156\n",
      "epoch:39,batch:3 loss:1.0835120677947998\n",
      "epoch:39,batch:4 loss:1.106168270111084\n",
      "epoch:39,batch:5 loss:1.0975747108459473\n",
      "epoch:39,batch:6 loss:1.1131997108459473\n",
      "epoch:39,batch:7 loss:1.0874183177947998\n",
      "epoch:39,batch:8 loss:1.100699543952942\n",
      "epoch:39,batch:9 loss:1.0975745916366577\n",
      "epoch:39,batch:10 loss:1.1085121631622314\n",
      "epoch:39,batch:11 loss:1.0967934131622314\n",
      "epoch:39,batch:12 loss:1.1092933416366577\n",
      "epoch:39,batch:13 loss:1.1155433654785156\n",
      "epoch:39,batch:14 loss:1.1272621154785156\n",
      "epoch:39,batch:15 loss:1.1006996631622314\n",
      "epoch:39,batch:16 loss:1.1147620677947998\n",
      "epoch:39,batch:17 loss:1.106168270111084\n",
      "epoch:39,batch:18 loss:1.1171058416366577\n",
      "epoch:39,batch:19 loss:1.0936683416366577\n",
      "epoch:39,batch:20 loss:1.069449543952942\n",
      "epoch:39,batch:21 loss:1.094449520111084\n",
      "epoch:39,batch:22 loss:1.0967934131622314\n",
      "epoch:39,batch:23 loss:1.1139808893203735\n",
      "epoch:39,batch:24 loss:1.0796058177947998\n",
      "epoch:39,batch:25 loss:1.1022621393203735\n",
      "epoch:39,batch:26 loss:1.1077308654785156\n",
      "epoch:39,batch:27 loss:1.0803871154785156\n",
      "epoch:39,batch:28 loss:1.1069495677947998\n",
      "epoch:39,batch:29 loss:1.0936683416366577\n",
      "epoch:39,batch:30 loss:1.1014808416366577\n",
      "epoch:39,batch:31 loss:1.0874183177947998\n",
      "epoch:39,batch:32 loss:1.115543246269226\n",
      "epoch:39,batch:33 loss:1.0936683416366577\n",
      "epoch:39,batch:34 loss:1.0936682224273682\n",
      "epoch:39,batch:35 loss:1.098355770111084\n",
      "epoch:39,batch:36 loss:1.0897620916366577\n",
      "epoch:39,batch:37 loss:1.0819495916366577\n",
      "epoch:39,batch:38 loss:1.096793293952942\n",
      "epoch:39,batch:39 loss:1.1288245916366577\n",
      "epoch:39,batch:40 loss:1.0999183654785156\n",
      "epoch:39,batch:41 loss:1.1069495677947998\n",
      "epoch:39,batch:42 loss:1.1061683893203735\n",
      "epoch:39,batch:43 loss:1.1100746393203735\n",
      "epoch:39,batch:44 loss:1.0897620916366577\n",
      "epoch:39,batch:45 loss:1.0936683416366577\n",
      "epoch:39,batch:46 loss:1.0936684608459473\n",
      "epoch:39,batch:47 loss:1.1022621393203735\n",
      "epoch:39,batch:48 loss:1.125699520111084\n",
      "epoch:39,batch:49 loss:1.113980770111084\n",
      "epoch:39,batch:50 loss:1.0999183654785156\n",
      "epoch:39,batch:51 loss:1.0764808654785156\n",
      "epoch:39,batch:52 loss:1.0819495916366577\n",
      "epoch:39,batch:53 loss:1.0819497108459473\n",
      "epoch:39,batch:54 loss:1.086637020111084\n",
      "epoch:39,batch:55 loss:1.0991370677947998\n",
      "epoch:39,batch:56 loss:1.0936682224273682\n",
      "epoch:39,batch:57 loss:1.1178871393203735\n",
      "epoch:39,batch:58 loss:1.1131995916366577\n",
      "epoch:39,batch:59 loss:1.1038246154785156\n",
      "epoch:39,batch:60 loss:1.0999183654785156\n",
      "epoch:39,batch:61 loss:1.108512043952942\n",
      "epoch:39,batch:62 loss:1.0850746631622314\n",
      "epoch:39,batch:63 loss:1.0725746154785156\n",
      "epoch:39,batch:64 loss:1.121793270111084\n",
      "epoch:39,batch:65 loss:1.0983558893203735\n",
      "epoch:39,batch:66 loss:1.0921058654785156\n",
      "epoch:39,batch:67 loss:1.100699543952942\n",
      "epoch:39,batch:68 loss:1.0967934131622314\n",
      "epoch:39,batch:69 loss:1.1272621154785156\n",
      "epoch:39,batch:70 loss:1.1014808416366577\n",
      "epoch:39,batch:71 loss:1.0967934131622314\n",
      "epoch:39,batch:72 loss:1.090543270111084\n",
      "epoch:39,batch:73 loss:1.1171058416366577\n",
      "epoch:39,batch:74 loss:1.090543270111084\n",
      "epoch:39,batch:75 loss:1.110074520111084\n",
      "epoch:39,batch:76 loss:1.0835120677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [08:57<02:25, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39,batch:77 loss:1.113980770111084\n",
      "epoch:39, train_loss:1.113980770111084\n",
      "epoch:40,batch:0 loss:1.104605793952942\n",
      "epoch:40,batch:1 loss:1.1163246631622314\n",
      "epoch:40,batch:2 loss:1.081168293952942\n",
      "epoch:40,batch:3 loss:1.116324543952942\n",
      "epoch:40,batch:4 loss:1.096793293952942\n",
      "epoch:40,batch:5 loss:1.1069495677947998\n",
      "epoch:40,batch:6 loss:1.0803871154785156\n",
      "epoch:40,batch:7 loss:1.1030434370040894\n",
      "epoch:40,batch:8 loss:1.077262043952942\n",
      "epoch:40,batch:9 loss:1.0889806747436523\n",
      "epoch:40,batch:10 loss:1.0858558416366577\n",
      "epoch:40,batch:11 loss:1.110074520111084\n",
      "epoch:40,batch:12 loss:1.0983558893203735\n",
      "epoch:40,batch:13 loss:1.112418293952942\n",
      "epoch:40,batch:14 loss:1.0913245677947998\n",
      "epoch:40,batch:15 loss:1.0780433416366577\n",
      "epoch:40,batch:16 loss:1.0983558893203735\n",
      "epoch:40,batch:17 loss:1.081168293952942\n",
      "epoch:40,batch:18 loss:1.1092932224273682\n",
      "epoch:40,batch:19 loss:1.121793270111084\n",
      "epoch:40,batch:20 loss:1.0772621631622314\n",
      "epoch:40,batch:21 loss:1.0913246870040894\n",
      "epoch:40,batch:22 loss:1.0835121870040894\n",
      "epoch:40,batch:23 loss:1.1022621393203735\n",
      "epoch:40,batch:24 loss:1.0991370677947998\n",
      "epoch:40,batch:25 loss:1.1069495677947998\n",
      "epoch:40,batch:26 loss:1.0858558416366577\n",
      "epoch:40,batch:27 loss:1.104605793952942\n",
      "epoch:40,batch:28 loss:1.1163246631622314\n",
      "epoch:40,batch:29 loss:1.116324543952942\n",
      "epoch:40,batch:30 loss:1.1131995916366577\n",
      "epoch:40,batch:31 loss:1.0936684608459473\n",
      "epoch:40,batch:32 loss:1.1296058893203735\n",
      "epoch:40,batch:33 loss:1.1092933416366577\n",
      "epoch:40,batch:34 loss:1.1069495677947998\n",
      "epoch:40,batch:35 loss:1.0999183654785156\n",
      "epoch:40,batch:36 loss:1.0756996870040894\n",
      "epoch:40,batch:37 loss:1.0983558893203735\n",
      "epoch:40,batch:38 loss:1.100699543952942\n",
      "epoch:40,batch:39 loss:1.1014807224273682\n",
      "epoch:40,batch:40 loss:1.0991370677947998\n",
      "epoch:40,batch:41 loss:1.0967934131622314\n",
      "epoch:40,batch:42 loss:1.1092933416366577\n",
      "epoch:40,batch:43 loss:1.1077308654785156\n",
      "epoch:40,batch:44 loss:1.1171058416366577\n",
      "epoch:40,batch:45 loss:1.0983558893203735\n",
      "epoch:40,batch:46 loss:1.0991370677947998\n",
      "epoch:40,batch:47 loss:1.1077308654785156\n",
      "epoch:40,batch:48 loss:1.1038246154785156\n",
      "epoch:40,batch:49 loss:1.1014808416366577\n",
      "epoch:40,batch:50 loss:1.0881996154785156\n",
      "epoch:40,batch:51 loss:1.0796059370040894\n",
      "epoch:40,batch:52 loss:1.0811684131622314\n",
      "epoch:40,batch:53 loss:1.1342933177947998\n",
      "epoch:40,batch:54 loss:1.0881996154785156\n",
      "epoch:40,batch:55 loss:1.090543270111084\n",
      "epoch:40,batch:56 loss:1.1077308654785156\n",
      "epoch:40,batch:57 loss:1.0913246870040894\n",
      "epoch:40,batch:58 loss:1.0960121154785156\n",
      "epoch:40,batch:59 loss:1.106168270111084\n",
      "epoch:40,batch:60 loss:1.0952308177947998\n",
      "epoch:40,batch:61 loss:1.1108558177947998\n",
      "epoch:40,batch:62 loss:1.1100746393203735\n",
      "epoch:40,batch:63 loss:1.0936683416366577\n",
      "epoch:40,batch:64 loss:1.1085121631622314\n",
      "epoch:40,batch:65 loss:1.1006996631622314\n",
      "epoch:40,batch:66 loss:1.0944496393203735\n",
      "epoch:40,batch:67 loss:1.1139808893203735\n",
      "epoch:40,batch:68 loss:1.104605793952942\n",
      "epoch:40,batch:69 loss:1.0850746631622314\n",
      "epoch:40,batch:70 loss:1.1069495677947998\n",
      "epoch:40,batch:71 loss:1.0921058654785156\n",
      "epoch:40,batch:72 loss:1.0983558893203735\n",
      "epoch:40,batch:73 loss:1.1053870916366577\n",
      "epoch:40,batch:74 loss:1.111636996269226\n",
      "epoch:40,batch:75 loss:1.1139808893203735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [09:11<02:07, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40,batch:76 loss:1.0991370677947998\n",
      "epoch:40,batch:77 loss:1.1077308654785156\n",
      "epoch:40, train_loss:1.1077308654785156\n",
      "epoch:41,batch:0 loss:1.100699543952942\n",
      "epoch:41,batch:1 loss:1.0952309370040894\n",
      "epoch:41,batch:2 loss:1.0881996154785156\n",
      "epoch:41,batch:3 loss:1.106168270111084\n",
      "epoch:41,batch:4 loss:1.1217933893203735\n",
      "epoch:41,batch:5 loss:1.086637020111084\n",
      "epoch:41,batch:6 loss:1.0874183177947998\n",
      "epoch:41,batch:7 loss:1.113980770111084\n",
      "epoch:41,batch:8 loss:1.1131995916366577\n",
      "epoch:41,batch:9 loss:1.0999183654785156\n",
      "epoch:41,batch:10 loss:1.1085121631622314\n",
      "epoch:41,batch:11 loss:1.0874184370040894\n",
      "epoch:41,batch:12 loss:1.0756995677947998\n",
      "epoch:41,batch:13 loss:1.1085121631622314\n",
      "epoch:41,batch:14 loss:1.092887043952942\n",
      "epoch:41,batch:15 loss:1.1014808416366577\n",
      "epoch:41,batch:16 loss:1.1046059131622314\n",
      "epoch:41,batch:17 loss:1.0881996154785156\n",
      "epoch:41,batch:18 loss:1.0999183654785156\n",
      "epoch:41,batch:19 loss:1.1014808416366577\n",
      "epoch:41,batch:20 loss:1.0796058177947998\n",
      "epoch:41,batch:21 loss:1.0866371393203735\n",
      "epoch:41,batch:22 loss:1.129605770111084\n",
      "epoch:41,batch:23 loss:1.096793293952942\n",
      "epoch:41,batch:24 loss:1.0967934131622314\n",
      "epoch:41,batch:25 loss:1.0866371393203735\n",
      "epoch:41,batch:26 loss:1.1006996631622314\n",
      "epoch:41,batch:27 loss:1.1046059131622314\n",
      "epoch:41,batch:28 loss:1.085074543952942\n",
      "epoch:41,batch:29 loss:1.1014809608459473\n",
      "epoch:41,batch:30 loss:1.0842933654785156\n",
      "epoch:41,batch:31 loss:1.1085119247436523\n",
      "epoch:41,batch:32 loss:1.0796058177947998\n",
      "epoch:41,batch:33 loss:1.090543270111084\n",
      "epoch:41,batch:34 loss:1.120230793952942\n",
      "epoch:41,batch:35 loss:1.094449520111084\n",
      "epoch:41,batch:36 loss:1.1092934608459473\n",
      "epoch:41,batch:37 loss:1.0842933654785156\n",
      "epoch:41,batch:38 loss:1.0835120677947998\n",
      "epoch:41,batch:39 loss:1.092887043952942\n",
      "epoch:41,batch:40 loss:1.0952308177947998\n",
      "epoch:41,batch:41 loss:1.1092933416366577\n",
      "epoch:41,batch:42 loss:1.0842933654785156\n",
      "epoch:41,batch:43 loss:1.0967934131622314\n",
      "epoch:41,batch:44 loss:1.1147620677947998\n",
      "epoch:41,batch:45 loss:1.1038246154785156\n",
      "epoch:41,batch:46 loss:1.1061683893203735\n",
      "epoch:41,batch:47 loss:1.1014808416366577\n",
      "epoch:41,batch:48 loss:1.0991370677947998\n",
      "epoch:41,batch:49 loss:1.0921058654785156\n",
      "epoch:41,batch:50 loss:1.1022621393203735\n",
      "epoch:41,batch:51 loss:1.1053870916366577\n",
      "epoch:41,batch:52 loss:1.1022621393203735\n",
      "epoch:41,batch:53 loss:1.096793293952942\n",
      "epoch:41,batch:54 loss:1.117887020111084\n",
      "epoch:41,batch:55 loss:1.0944496393203735\n",
      "epoch:41,batch:56 loss:1.1092933416366577\n",
      "epoch:41,batch:57 loss:1.0858558416366577\n",
      "epoch:41,batch:58 loss:1.0991370677947998\n",
      "epoch:41,batch:59 loss:1.108512043952942\n",
      "epoch:41,batch:60 loss:1.0936683416366577\n",
      "epoch:41,batch:61 loss:1.0975745916366577\n",
      "epoch:41,batch:62 loss:1.096793293952942\n",
      "epoch:41,batch:63 loss:1.1116371154785156\n",
      "epoch:41,batch:64 loss:1.1303870677947998\n",
      "epoch:41,batch:65 loss:1.102262020111084\n",
      "epoch:41,batch:66 loss:1.0999183654785156\n",
      "epoch:41,batch:67 loss:1.1147620677947998\n",
      "epoch:41,batch:68 loss:1.0944496393203735\n",
      "epoch:41,batch:69 loss:1.1092933416366577\n",
      "epoch:41,batch:70 loss:1.1147620677947998\n",
      "epoch:41,batch:71 loss:1.1194496154785156\n",
      "epoch:41,batch:72 loss:1.0897620916366577\n",
      "epoch:41,batch:73 loss:1.113980770111084\n",
      "epoch:41,batch:74 loss:1.1069495677947998\n",
      "epoch:41,batch:75 loss:1.0889809131622314\n",
      "epoch:41,batch:76 loss:1.1131995916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [09:24<01:52, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41,batch:77 loss:1.100699543952942\n",
      "epoch:41, train_loss:1.100699543952942\n",
      "epoch:42,batch:0 loss:1.0897619724273682\n",
      "epoch:42,batch:1 loss:1.100699543952942\n",
      "epoch:42,batch:2 loss:1.102262020111084\n",
      "epoch:42,batch:3 loss:1.0913245677947998\n",
      "epoch:42,batch:4 loss:1.106168270111084\n",
      "epoch:42,batch:5 loss:1.0764808654785156\n",
      "epoch:42,batch:6 loss:1.100699543952942\n",
      "epoch:42,batch:7 loss:1.0928871631622314\n",
      "epoch:42,batch:8 loss:1.107730746269226\n",
      "epoch:42,batch:9 loss:1.1069495677947998\n",
      "epoch:42,batch:10 loss:1.1014807224273682\n",
      "epoch:42,batch:11 loss:1.0975747108459473\n",
      "epoch:42,batch:12 loss:1.080386996269226\n",
      "epoch:42,batch:13 loss:1.1303870677947998\n",
      "epoch:42,batch:14 loss:1.106168270111084\n",
      "epoch:42,batch:15 loss:1.096793293952942\n",
      "epoch:42,batch:16 loss:1.0772621631622314\n",
      "epoch:42,batch:17 loss:1.113980770111084\n",
      "epoch:42,batch:18 loss:1.102262020111084\n",
      "epoch:42,batch:19 loss:1.096793293952942\n",
      "epoch:42,batch:20 loss:1.0905433893203735\n",
      "epoch:42,batch:21 loss:1.1202309131622314\n",
      "epoch:42,batch:22 loss:1.0983558893203735\n",
      "epoch:42,batch:23 loss:1.1233558654785156\n",
      "epoch:42,batch:24 loss:1.0921058654785156\n",
      "epoch:42,batch:25 loss:1.098355770111084\n",
      "epoch:42,batch:26 loss:1.100699543952942\n",
      "epoch:42,batch:27 loss:1.0952308177947998\n",
      "epoch:42,batch:28 loss:1.0913246870040894\n",
      "epoch:42,batch:29 loss:1.1046059131622314\n",
      "epoch:42,batch:30 loss:1.1053872108459473\n",
      "epoch:42,batch:31 loss:1.090543270111084\n",
      "epoch:42,batch:32 loss:1.1256996393203735\n",
      "epoch:42,batch:33 loss:1.0733559131622314\n",
      "epoch:42,batch:34 loss:1.0952308177947998\n",
      "epoch:42,batch:35 loss:1.110074520111084\n",
      "epoch:42,batch:36 loss:1.0999183654785156\n",
      "epoch:42,batch:37 loss:1.103824496269226\n",
      "epoch:42,batch:38 loss:1.1116371154785156\n",
      "epoch:42,batch:39 loss:1.0991370677947998\n",
      "epoch:42,batch:40 loss:1.0913245677947998\n",
      "epoch:42,batch:41 loss:1.1116371154785156\n",
      "epoch:42,batch:42 loss:1.1077309846878052\n",
      "epoch:42,batch:43 loss:1.1155433654785156\n",
      "epoch:42,batch:44 loss:1.1116371154785156\n",
      "epoch:42,batch:45 loss:1.1171058416366577\n",
      "epoch:42,batch:46 loss:1.1171059608459473\n",
      "epoch:42,batch:47 loss:1.0788246393203735\n",
      "epoch:42,batch:48 loss:1.1139808893203735\n",
      "epoch:42,batch:49 loss:1.0936683416366577\n",
      "epoch:42,batch:50 loss:1.106168270111084\n",
      "epoch:42,batch:51 loss:1.1077308654785156\n",
      "epoch:42,batch:52 loss:1.096793293952942\n",
      "epoch:42,batch:53 loss:1.1241371631622314\n",
      "epoch:42,batch:54 loss:1.0991371870040894\n",
      "epoch:42,batch:55 loss:1.1006996631622314\n",
      "epoch:42,batch:56 loss:1.1108558177947998\n",
      "epoch:42,batch:57 loss:1.1006996631622314\n",
      "epoch:42,batch:58 loss:1.100699543952942\n",
      "epoch:42,batch:59 loss:1.0936683416366577\n",
      "epoch:42,batch:60 loss:1.0913245677947998\n",
      "epoch:42,batch:61 loss:1.0991370677947998\n",
      "epoch:42,batch:62 loss:1.0967934131622314\n",
      "epoch:42,batch:63 loss:1.1233558654785156\n",
      "epoch:42,batch:64 loss:1.1163246631622314\n",
      "epoch:42,batch:65 loss:1.0960121154785156\n",
      "epoch:42,batch:66 loss:1.0967934131622314\n",
      "epoch:42,batch:67 loss:1.0514808893203735\n",
      "epoch:42,batch:68 loss:1.0975745916366577\n",
      "epoch:42,batch:69 loss:1.0811684131622314\n",
      "epoch:42,batch:70 loss:1.103824496269226\n",
      "epoch:42,batch:71 loss:1.0881996154785156\n",
      "epoch:42,batch:72 loss:1.0944496393203735\n",
      "epoch:42,batch:73 loss:1.1061683893203735\n",
      "epoch:42,batch:74 loss:1.102262020111084\n",
      "epoch:42,batch:75 loss:1.0975747108459473\n",
      "epoch:42,batch:76 loss:1.081168293952942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [09:38<01:36, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42,batch:77 loss:1.1014808416366577\n",
      "epoch:42, train_loss:1.1014808416366577\n",
      "epoch:43,batch:0 loss:1.094449520111084\n",
      "epoch:43,batch:1 loss:1.1342933177947998\n",
      "epoch:43,batch:2 loss:1.1210120916366577\n",
      "epoch:43,batch:3 loss:1.0881996154785156\n",
      "epoch:43,batch:4 loss:1.0874183177947998\n",
      "epoch:43,batch:5 loss:1.1171058416366577\n",
      "epoch:43,batch:6 loss:1.0975745916366577\n",
      "epoch:43,batch:7 loss:1.082730770111084\n",
      "epoch:43,batch:8 loss:1.094449520111084\n",
      "epoch:43,batch:9 loss:1.0905433893203735\n",
      "epoch:43,batch:10 loss:1.1069495677947998\n",
      "epoch:43,batch:11 loss:1.1014807224273682\n",
      "epoch:43,batch:12 loss:1.1092933416366577\n",
      "epoch:43,batch:13 loss:1.098355770111084\n",
      "epoch:43,batch:14 loss:1.103824496269226\n",
      "epoch:43,batch:15 loss:1.0983558893203735\n",
      "epoch:43,batch:16 loss:1.100699543952942\n",
      "epoch:43,batch:17 loss:1.0999183654785156\n",
      "epoch:43,batch:18 loss:1.1108558177947998\n",
      "epoch:43,batch:19 loss:1.1030433177947998\n",
      "epoch:43,batch:20 loss:1.121793270111084\n",
      "epoch:43,batch:21 loss:1.1147620677947998\n",
      "epoch:43,batch:22 loss:1.0991371870040894\n",
      "epoch:43,batch:23 loss:1.069449543952942\n",
      "epoch:43,batch:24 loss:1.069449543952942\n",
      "epoch:43,batch:25 loss:1.1100746393203735\n",
      "epoch:43,batch:26 loss:1.1178871393203735\n",
      "epoch:43,batch:27 loss:1.1202309131622314\n",
      "epoch:43,batch:28 loss:1.1038246154785156\n",
      "epoch:43,batch:29 loss:1.1116371154785156\n",
      "epoch:43,batch:30 loss:1.1053870916366577\n",
      "epoch:43,batch:31 loss:1.086637020111084\n",
      "epoch:43,batch:32 loss:1.0975745916366577\n",
      "epoch:43,batch:33 loss:1.1139808893203735\n",
      "epoch:43,batch:34 loss:1.124137043952942\n",
      "epoch:43,batch:35 loss:1.0881996154785156\n",
      "epoch:43,batch:36 loss:1.1249183416366577\n",
      "epoch:43,batch:37 loss:1.0999183654785156\n",
      "epoch:43,batch:38 loss:1.1186683177947998\n",
      "epoch:43,batch:39 loss:1.0835120677947998\n",
      "epoch:43,batch:40 loss:1.1092933416366577\n",
      "epoch:43,batch:41 loss:1.107730746269226\n",
      "epoch:43,batch:42 loss:1.069449543952942\n",
      "epoch:43,batch:43 loss:1.1217933893203735\n",
      "epoch:43,batch:44 loss:1.1092933416366577\n",
      "epoch:43,batch:45 loss:1.0819495916366577\n",
      "epoch:43,batch:46 loss:1.096011996269226\n",
      "epoch:43,batch:47 loss:1.1006996631622314\n",
      "epoch:43,batch:48 loss:1.0811684131622314\n",
      "epoch:43,batch:49 loss:1.1014808416366577\n",
      "epoch:43,batch:50 loss:1.0655434131622314\n",
      "epoch:43,batch:51 loss:1.0897620916366577\n",
      "epoch:43,batch:52 loss:1.1171058416366577\n",
      "epoch:43,batch:53 loss:1.1100746393203735\n",
      "epoch:43,batch:54 loss:1.0889809131622314\n",
      "epoch:43,batch:55 loss:1.0897620916366577\n",
      "epoch:43,batch:56 loss:1.121793270111084\n",
      "epoch:43,batch:57 loss:1.1155433654785156\n",
      "epoch:43,batch:58 loss:1.0921058654785156\n",
      "epoch:43,batch:59 loss:1.0913245677947998\n",
      "epoch:43,batch:60 loss:1.1225745677947998\n",
      "epoch:43,batch:61 loss:1.0960121154785156\n",
      "epoch:43,batch:62 loss:1.0960121154785156\n",
      "epoch:43,batch:63 loss:1.1288245916366577\n",
      "epoch:43,batch:64 loss:1.1233558654785156\n",
      "epoch:43,batch:65 loss:1.0874184370040894\n",
      "epoch:43,batch:66 loss:1.102262020111084\n",
      "epoch:43,batch:67 loss:1.1249183416366577\n",
      "epoch:43,batch:68 loss:1.0874183177947998\n",
      "epoch:43,batch:69 loss:1.088980793952942\n",
      "epoch:43,batch:70 loss:1.0796058177947998\n",
      "epoch:43,batch:71 loss:1.1046059131622314\n",
      "epoch:43,batch:72 loss:1.0866371393203735\n",
      "epoch:43,batch:73 loss:1.0897620916366577\n",
      "epoch:43,batch:74 loss:1.0858558416366577\n",
      "epoch:43,batch:75 loss:1.0764808654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [09:54<01:26, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43,batch:76 loss:1.0741370916366577\n",
      "epoch:43,batch:77 loss:1.104605793952942\n",
      "epoch:43, train_loss:1.104605793952942\n",
      "epoch:44,batch:0 loss:1.0952308177947998\n",
      "epoch:44,batch:1 loss:1.0983558893203735\n",
      "epoch:44,batch:2 loss:1.1046059131622314\n",
      "epoch:44,batch:3 loss:1.0999183654785156\n",
      "epoch:44,batch:4 loss:1.0975745916366577\n",
      "epoch:44,batch:5 loss:1.0936684608459473\n",
      "epoch:44,batch:6 loss:1.0897620916366577\n",
      "epoch:44,batch:7 loss:1.1139808893203735\n",
      "epoch:44,batch:8 loss:1.0858558416366577\n",
      "epoch:44,batch:9 loss:1.0991370677947998\n",
      "epoch:44,batch:10 loss:1.1014809608459473\n",
      "epoch:44,batch:11 loss:1.0889809131622314\n",
      "epoch:44,batch:12 loss:1.0952308177947998\n",
      "epoch:44,batch:13 loss:1.0936683416366577\n",
      "epoch:44,batch:14 loss:1.1217933893203735\n",
      "epoch:44,batch:15 loss:1.1085121631622314\n",
      "epoch:44,batch:16 loss:1.0991370677947998\n",
      "epoch:44,batch:17 loss:1.0780434608459473\n",
      "epoch:44,batch:18 loss:1.0991371870040894\n",
      "epoch:44,batch:19 loss:1.0741369724273682\n",
      "epoch:44,batch:20 loss:1.0772621631622314\n",
      "epoch:44,batch:21 loss:1.0874183177947998\n",
      "epoch:44,batch:22 loss:1.0952308177947998\n",
      "epoch:44,batch:23 loss:1.0842933654785156\n",
      "epoch:44,batch:24 loss:1.1030433177947998\n",
      "epoch:44,batch:25 loss:1.1147620677947998\n",
      "epoch:44,batch:26 loss:1.094449520111084\n",
      "epoch:44,batch:27 loss:1.0928871631622314\n",
      "epoch:44,batch:28 loss:1.0842933654785156\n",
      "epoch:44,batch:29 loss:1.102262020111084\n",
      "epoch:44,batch:30 loss:1.1171058416366577\n",
      "epoch:44,batch:31 loss:1.1327308416366577\n",
      "epoch:44,batch:32 loss:1.1116371154785156\n",
      "epoch:44,batch:33 loss:1.0921058654785156\n",
      "epoch:44,batch:34 loss:1.102262258529663\n",
      "epoch:44,batch:35 loss:1.106168270111084\n",
      "epoch:44,batch:36 loss:1.0991370677947998\n",
      "epoch:44,batch:37 loss:1.1131995916366577\n",
      "epoch:44,batch:38 loss:1.0866371393203735\n",
      "epoch:44,batch:39 loss:1.0991370677947998\n",
      "epoch:44,batch:40 loss:1.1046059131622314\n",
      "epoch:44,batch:41 loss:1.096793293952942\n",
      "epoch:44,batch:42 loss:1.1014808416366577\n",
      "epoch:44,batch:43 loss:1.092887043952942\n",
      "epoch:44,batch:44 loss:1.1147620677947998\n",
      "epoch:44,batch:45 loss:1.106168270111084\n",
      "epoch:44,batch:46 loss:1.0921058654785156\n",
      "epoch:44,batch:47 loss:1.0952309370040894\n",
      "epoch:44,batch:48 loss:1.1108558177947998\n",
      "epoch:44,batch:49 loss:1.1061683893203735\n",
      "epoch:44,batch:50 loss:1.104605793952942\n",
      "epoch:44,batch:51 loss:1.102262020111084\n",
      "epoch:44,batch:52 loss:1.1014808416366577\n",
      "epoch:44,batch:53 loss:1.0928871631622314\n",
      "epoch:44,batch:54 loss:1.1171059608459473\n",
      "epoch:44,batch:55 loss:1.0928871631622314\n",
      "epoch:44,batch:56 loss:1.104605793952942\n",
      "epoch:44,batch:57 loss:1.1131995916366577\n",
      "epoch:44,batch:58 loss:1.1092933416366577\n",
      "epoch:44,batch:59 loss:1.0803871154785156\n",
      "epoch:44,batch:60 loss:1.1335121393203735\n",
      "epoch:44,batch:61 loss:1.0874183177947998\n",
      "epoch:44,batch:62 loss:1.090543270111084\n",
      "epoch:44,batch:63 loss:1.103824496269226\n",
      "epoch:44,batch:64 loss:1.1202306747436523\n",
      "epoch:44,batch:65 loss:1.1092933416366577\n",
      "epoch:44,batch:66 loss:1.071012020111084\n",
      "epoch:44,batch:67 loss:1.104605793952942\n",
      "epoch:44,batch:68 loss:1.1186683177947998\n",
      "epoch:44,batch:69 loss:1.1147620677947998\n",
      "epoch:44,batch:70 loss:1.092887043952942\n",
      "epoch:44,batch:71 loss:1.0936683416366577\n",
      "epoch:44,batch:72 loss:1.0975745916366577\n",
      "epoch:44,batch:73 loss:1.1131995916366577\n",
      "epoch:44,batch:74 loss:1.0999183654785156\n",
      "epoch:44,batch:75 loss:1.100699543952942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [10:07<01:11, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44,batch:76 loss:1.0866371393203735\n",
      "epoch:44,batch:77 loss:1.1217933893203735\n",
      "epoch:44, train_loss:1.1217933893203735\n",
      "epoch:45,batch:0 loss:1.1092934608459473\n",
      "epoch:45,batch:1 loss:1.100699543952942\n",
      "epoch:45,batch:2 loss:1.1053870916366577\n",
      "epoch:45,batch:3 loss:1.0897620916366577\n",
      "epoch:45,batch:4 loss:1.0944496393203735\n",
      "epoch:45,batch:5 loss:1.0936683416366577\n",
      "epoch:45,batch:6 loss:1.117887020111084\n",
      "epoch:45,batch:7 loss:1.0975745916366577\n",
      "epoch:45,batch:8 loss:1.1100746393203735\n",
      "epoch:45,batch:9 loss:1.1077308654785156\n",
      "epoch:45,batch:10 loss:1.110074520111084\n",
      "epoch:45,batch:11 loss:1.0913245677947998\n",
      "epoch:45,batch:12 loss:1.1163246631622314\n",
      "epoch:45,batch:13 loss:1.0717933177947998\n",
      "epoch:45,batch:14 loss:1.0889809131622314\n",
      "epoch:45,batch:15 loss:1.088980793952942\n",
      "epoch:45,batch:16 loss:1.0999183654785156\n",
      "epoch:45,batch:17 loss:1.0921058654785156\n",
      "epoch:45,batch:18 loss:1.1249184608459473\n",
      "epoch:45,batch:19 loss:1.098355770111084\n",
      "epoch:45,batch:20 loss:1.0960121154785156\n",
      "epoch:45,batch:21 loss:1.0952308177947998\n",
      "epoch:45,batch:22 loss:1.113980770111084\n",
      "epoch:45,batch:23 loss:1.065543293952942\n",
      "epoch:45,batch:24 loss:1.1030433177947998\n",
      "epoch:45,batch:25 loss:1.1006996631622314\n",
      "epoch:45,batch:26 loss:1.0936683416366577\n",
      "epoch:45,batch:27 loss:1.100699543952942\n",
      "epoch:45,batch:28 loss:1.1249183416366577\n",
      "epoch:45,batch:29 loss:1.1014807224273682\n",
      "epoch:45,batch:30 loss:1.099918246269226\n",
      "epoch:45,batch:31 loss:1.0952308177947998\n",
      "epoch:45,batch:32 loss:1.1186683177947998\n",
      "epoch:45,batch:33 loss:1.135855793952942\n",
      "epoch:45,batch:34 loss:1.1085121631622314\n",
      "epoch:45,batch:35 loss:1.094449520111084\n",
      "epoch:45,batch:36 loss:1.1014808416366577\n",
      "epoch:45,batch:37 loss:1.103824496269226\n",
      "epoch:45,batch:38 loss:1.1038246154785156\n",
      "epoch:45,batch:39 loss:1.096011996269226\n",
      "epoch:45,batch:40 loss:1.111636996269226\n",
      "epoch:45,batch:41 loss:1.090543270111084\n",
      "epoch:45,batch:42 loss:1.1069495677947998\n",
      "epoch:45,batch:43 loss:1.1077308654785156\n",
      "epoch:45,batch:44 loss:1.1264808177947998\n",
      "epoch:45,batch:45 loss:1.0835120677947998\n",
      "epoch:45,batch:46 loss:1.0889809131622314\n",
      "epoch:45,batch:47 loss:1.0936683416366577\n",
      "epoch:45,batch:48 loss:1.0874183177947998\n",
      "epoch:45,batch:49 loss:1.0960121154785156\n",
      "epoch:45,batch:50 loss:1.0921058654785156\n",
      "epoch:45,batch:51 loss:1.1030433177947998\n",
      "epoch:45,batch:52 loss:1.112418293952942\n",
      "epoch:45,batch:53 loss:1.1108558177947998\n",
      "epoch:45,batch:54 loss:1.1131995916366577\n",
      "epoch:45,batch:55 loss:1.0842933654785156\n",
      "epoch:45,batch:56 loss:1.0913245677947998\n",
      "epoch:45,batch:57 loss:1.0866371393203735\n",
      "epoch:45,batch:58 loss:1.1139808893203735\n",
      "epoch:45,batch:59 loss:1.1069495677947998\n",
      "epoch:45,batch:60 loss:1.088980793952942\n",
      "epoch:45,batch:61 loss:1.112418293952942\n",
      "epoch:45,batch:62 loss:1.1092933416366577\n",
      "epoch:45,batch:63 loss:1.078824520111084\n",
      "epoch:45,batch:64 loss:1.0756995677947998\n",
      "epoch:45,batch:65 loss:1.096011996269226\n",
      "epoch:45,batch:66 loss:1.1092933416366577\n",
      "epoch:45,batch:67 loss:1.092105746269226\n",
      "epoch:45,batch:68 loss:1.1077309846878052\n",
      "epoch:45,batch:69 loss:1.1053870916366577\n",
      "epoch:45,batch:70 loss:1.0936683416366577\n",
      "epoch:45,batch:71 loss:1.1014808416366577\n",
      "epoch:45,batch:72 loss:1.1061683893203735\n",
      "epoch:45,batch:73 loss:1.1053872108459473\n",
      "epoch:45,batch:74 loss:1.0921058654785156\n",
      "epoch:45,batch:75 loss:1.1030434370040894\n",
      "epoch:45,batch:76 loss:1.0897619724273682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [10:21<00:56, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45,batch:77 loss:1.096793293952942\n",
      "epoch:45, train_loss:1.096793293952942\n",
      "epoch:46,batch:0 loss:1.120230793952942\n",
      "epoch:46,batch:1 loss:1.1053869724273682\n",
      "epoch:46,batch:2 loss:1.0827308893203735\n",
      "epoch:46,batch:3 loss:1.081168293952942\n",
      "epoch:46,batch:4 loss:1.0975745916366577\n",
      "epoch:46,batch:5 loss:1.1147620677947998\n",
      "epoch:46,batch:6 loss:1.094449520111084\n",
      "epoch:46,batch:7 loss:1.0897620916366577\n",
      "epoch:46,batch:8 loss:1.0983558893203735\n",
      "epoch:46,batch:9 loss:1.1069495677947998\n",
      "epoch:46,batch:10 loss:1.1022621393203735\n",
      "epoch:46,batch:11 loss:1.0764808654785156\n",
      "epoch:46,batch:12 loss:1.0975744724273682\n",
      "epoch:46,batch:13 loss:1.0686683654785156\n",
      "epoch:46,batch:14 loss:1.0975745916366577\n",
      "epoch:46,batch:15 loss:1.120230793952942\n",
      "epoch:46,batch:16 loss:1.0858559608459473\n",
      "epoch:46,batch:17 loss:1.1006996631622314\n",
      "epoch:46,batch:18 loss:1.092887043952942\n",
      "epoch:46,batch:19 loss:1.1022621393203735\n",
      "epoch:46,batch:20 loss:1.1077308654785156\n",
      "epoch:46,batch:21 loss:1.1014808416366577\n",
      "epoch:46,batch:22 loss:1.096793293952942\n",
      "epoch:46,batch:23 loss:1.1163246631622314\n",
      "epoch:46,batch:24 loss:1.1014808416366577\n",
      "epoch:46,batch:25 loss:1.1131994724273682\n",
      "epoch:46,batch:26 loss:1.1030433177947998\n",
      "epoch:46,batch:27 loss:1.069449543952942\n",
      "epoch:46,batch:28 loss:1.1296058893203735\n",
      "epoch:46,batch:29 loss:1.1131995916366577\n",
      "epoch:46,batch:30 loss:1.1163246631622314\n",
      "epoch:46,batch:31 loss:1.120230793952942\n",
      "epoch:46,batch:32 loss:1.1053872108459473\n",
      "epoch:46,batch:33 loss:1.078824520111084\n",
      "epoch:46,batch:34 loss:1.098355770111084\n",
      "epoch:46,batch:35 loss:1.108512043952942\n",
      "epoch:46,batch:36 loss:1.1014807224273682\n",
      "epoch:46,batch:37 loss:1.090543270111084\n",
      "epoch:46,batch:38 loss:1.1046059131622314\n",
      "epoch:46,batch:39 loss:1.112418293952942\n",
      "epoch:46,batch:40 loss:1.1171059608459473\n",
      "epoch:46,batch:41 loss:1.1194496154785156\n",
      "epoch:46,batch:42 loss:1.117887020111084\n",
      "epoch:46,batch:43 loss:1.106168270111084\n",
      "epoch:46,batch:44 loss:1.0897620916366577\n",
      "epoch:46,batch:45 loss:1.096011996269226\n",
      "epoch:46,batch:46 loss:1.1155433654785156\n",
      "epoch:46,batch:47 loss:1.0796058177947998\n",
      "epoch:46,batch:48 loss:1.1241371631622314\n",
      "epoch:46,batch:49 loss:1.0913245677947998\n",
      "epoch:46,batch:50 loss:1.0983558893203735\n",
      "epoch:46,batch:51 loss:1.1030434370040894\n",
      "epoch:46,batch:52 loss:1.0975747108459473\n",
      "epoch:46,batch:53 loss:1.0999183654785156\n",
      "epoch:46,batch:54 loss:1.0991370677947998\n",
      "epoch:46,batch:55 loss:1.0764808654785156\n",
      "epoch:46,batch:56 loss:1.0772621631622314\n",
      "epoch:46,batch:57 loss:1.0913245677947998\n",
      "epoch:46,batch:58 loss:1.0928871631622314\n",
      "epoch:46,batch:59 loss:1.077262043952942\n",
      "epoch:46,batch:60 loss:1.0960121154785156\n",
      "epoch:46,batch:61 loss:1.1116371154785156\n",
      "epoch:46,batch:62 loss:1.0999183654785156\n",
      "epoch:46,batch:63 loss:1.0874184370040894\n",
      "epoch:46,batch:64 loss:1.098355770111084\n",
      "epoch:46,batch:65 loss:1.0897620916366577\n",
      "epoch:46,batch:66 loss:1.113980770111084\n",
      "epoch:46,batch:67 loss:1.1311683654785156\n",
      "epoch:46,batch:68 loss:1.0960121154785156\n",
      "epoch:46,batch:69 loss:1.0952309370040894\n",
      "epoch:46,batch:70 loss:1.0928871631622314\n",
      "epoch:46,batch:71 loss:1.1053870916366577\n",
      "epoch:46,batch:72 loss:1.1014808416366577\n",
      "epoch:46,batch:73 loss:1.094449520111084\n",
      "epoch:46,batch:74 loss:1.098355770111084\n",
      "epoch:46,batch:75 loss:1.115543246269226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [10:35<00:41, 14.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46,batch:76 loss:1.1241371631622314\n",
      "epoch:46,batch:77 loss:1.0819495916366577\n",
      "epoch:46, train_loss:1.0819495916366577\n",
      "epoch:47,batch:0 loss:1.1077308654785156\n",
      "epoch:47,batch:1 loss:1.073355793952942\n",
      "epoch:47,batch:2 loss:1.1194496154785156\n",
      "epoch:47,batch:3 loss:1.0835120677947998\n",
      "epoch:47,batch:4 loss:1.1046059131622314\n",
      "epoch:47,batch:5 loss:1.1014808416366577\n",
      "epoch:47,batch:6 loss:1.0975745916366577\n",
      "epoch:47,batch:7 loss:1.1210120916366577\n",
      "epoch:47,batch:8 loss:1.0960121154785156\n",
      "epoch:47,batch:9 loss:1.0796058177947998\n",
      "epoch:47,batch:10 loss:1.085074543952942\n",
      "epoch:47,batch:11 loss:1.0858557224273682\n",
      "epoch:47,batch:12 loss:1.1210120916366577\n",
      "epoch:47,batch:13 loss:1.0960121154785156\n",
      "epoch:47,batch:14 loss:1.1077308654785156\n",
      "epoch:47,batch:15 loss:1.1046059131622314\n",
      "epoch:47,batch:16 loss:1.098355770111084\n",
      "epoch:47,batch:17 loss:1.0999183654785156\n",
      "epoch:47,batch:18 loss:1.1139808893203735\n",
      "epoch:47,batch:19 loss:1.0999183654785156\n",
      "epoch:47,batch:20 loss:1.1053870916366577\n",
      "epoch:47,batch:21 loss:1.1030433177947998\n",
      "epoch:47,batch:22 loss:1.1053872108459473\n",
      "epoch:47,batch:23 loss:1.096793293952942\n",
      "epoch:47,batch:24 loss:1.0866371393203735\n",
      "epoch:47,batch:25 loss:1.1014808416366577\n",
      "epoch:47,batch:26 loss:1.1092933416366577\n",
      "epoch:47,batch:27 loss:1.0881996154785156\n",
      "epoch:47,batch:28 loss:1.081168293952942\n",
      "epoch:47,batch:29 loss:1.099918246269226\n",
      "epoch:47,batch:30 loss:1.1022621393203735\n",
      "epoch:47,batch:31 loss:1.0913245677947998\n",
      "epoch:47,batch:32 loss:1.0913245677947998\n",
      "epoch:47,batch:33 loss:1.112418293952942\n",
      "epoch:47,batch:34 loss:1.098355770111084\n",
      "epoch:47,batch:35 loss:1.124137043952942\n",
      "epoch:47,batch:36 loss:1.116324543952942\n",
      "epoch:47,batch:37 loss:1.096793293952942\n",
      "epoch:47,batch:38 loss:1.1053872108459473\n",
      "epoch:47,batch:39 loss:1.1022621393203735\n",
      "epoch:47,batch:40 loss:1.0983558893203735\n",
      "epoch:47,batch:41 loss:1.0936683416366577\n",
      "epoch:47,batch:42 loss:1.1022621393203735\n",
      "epoch:47,batch:43 loss:1.0897620916366577\n",
      "epoch:47,batch:44 loss:1.1061683893203735\n",
      "epoch:47,batch:45 loss:1.094449520111084\n",
      "epoch:47,batch:46 loss:1.0999183654785156\n",
      "epoch:47,batch:47 loss:1.0944496393203735\n",
      "epoch:47,batch:48 loss:1.108512043952942\n",
      "epoch:47,batch:49 loss:1.1171058416366577\n",
      "epoch:47,batch:50 loss:1.1092933416366577\n",
      "epoch:47,batch:51 loss:1.0874183177947998\n",
      "epoch:47,batch:52 loss:1.104605793952942\n",
      "epoch:47,batch:53 loss:1.1116371154785156\n",
      "epoch:47,batch:54 loss:1.1178871393203735\n",
      "epoch:47,batch:55 loss:1.1288245916366577\n",
      "epoch:47,batch:56 loss:1.088980793952942\n",
      "epoch:47,batch:57 loss:1.1217933893203735\n",
      "epoch:47,batch:58 loss:1.1178871393203735\n",
      "epoch:47,batch:59 loss:1.0928871631622314\n",
      "epoch:47,batch:60 loss:1.1085121631622314\n",
      "epoch:47,batch:61 loss:1.0897619724273682\n",
      "epoch:47,batch:62 loss:1.088980793952942\n",
      "epoch:47,batch:63 loss:1.069449543952942\n",
      "epoch:47,batch:64 loss:1.0717933177947998\n",
      "epoch:47,batch:65 loss:1.1131994724273682\n",
      "epoch:47,batch:66 loss:1.0780432224273682\n",
      "epoch:47,batch:67 loss:1.0975745916366577\n",
      "epoch:47,batch:68 loss:1.108512043952942\n",
      "epoch:47,batch:69 loss:1.1085121631622314\n",
      "epoch:47,batch:70 loss:1.0717933177947998\n",
      "epoch:47,batch:71 loss:1.0960121154785156\n",
      "epoch:47,batch:72 loss:1.0944496393203735\n",
      "epoch:47,batch:73 loss:1.110074520111084\n",
      "epoch:47,batch:74 loss:1.1006996631622314\n",
      "epoch:47,batch:75 loss:1.0756995677947998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [10:49<00:27, 13.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47,batch:76 loss:1.1397621631622314\n",
      "epoch:47,batch:77 loss:1.1092932224273682\n",
      "epoch:47, train_loss:1.1092932224273682\n",
      "epoch:48,batch:0 loss:1.0897620916366577\n",
      "epoch:48,batch:1 loss:1.1124184131622314\n",
      "epoch:48,batch:2 loss:1.1038246154785156\n",
      "epoch:48,batch:3 loss:1.1014807224273682\n",
      "epoch:48,batch:4 loss:1.0921058654785156\n",
      "epoch:48,batch:5 loss:1.1350746154785156\n",
      "epoch:48,batch:6 loss:1.1202309131622314\n",
      "epoch:48,batch:7 loss:1.1116371154785156\n",
      "epoch:48,batch:8 loss:1.082730770111084\n",
      "epoch:48,batch:9 loss:1.0881996154785156\n",
      "epoch:48,batch:10 loss:1.092887043952942\n",
      "epoch:48,batch:11 loss:1.110074520111084\n",
      "epoch:48,batch:12 loss:1.1046059131622314\n",
      "epoch:48,batch:13 loss:1.1171057224273682\n",
      "epoch:48,batch:14 loss:1.1311683654785156\n",
      "epoch:48,batch:15 loss:1.0897620916366577\n",
      "epoch:48,batch:16 loss:1.1053870916366577\n",
      "epoch:48,batch:17 loss:1.1264808177947998\n",
      "epoch:48,batch:18 loss:1.1116371154785156\n",
      "epoch:48,batch:19 loss:1.0952308177947998\n",
      "epoch:48,batch:20 loss:1.1038246154785156\n",
      "epoch:48,batch:21 loss:1.1077308654785156\n",
      "epoch:48,batch:22 loss:1.0944496393203735\n",
      "epoch:48,batch:23 loss:1.086637020111084\n",
      "epoch:48,batch:24 loss:1.0913246870040894\n",
      "epoch:48,batch:25 loss:1.0936684608459473\n",
      "epoch:48,batch:26 loss:1.1053872108459473\n",
      "epoch:48,batch:27 loss:1.113980770111084\n",
      "epoch:48,batch:28 loss:1.0944496393203735\n",
      "epoch:48,batch:29 loss:1.125699520111084\n",
      "epoch:48,batch:30 loss:1.1108558177947998\n",
      "epoch:48,batch:31 loss:1.086637020111084\n",
      "epoch:48,batch:32 loss:1.090543270111084\n",
      "epoch:48,batch:33 loss:1.088980793952942\n",
      "epoch:48,batch:34 loss:1.103824496269226\n",
      "epoch:48,batch:35 loss:1.116324543952942\n",
      "epoch:48,batch:36 loss:1.0803871154785156\n",
      "epoch:48,batch:37 loss:1.106168270111084\n",
      "epoch:48,batch:38 loss:1.1077308654785156\n",
      "epoch:48,batch:39 loss:1.1061683893203735\n",
      "epoch:48,batch:40 loss:1.0788246393203735\n",
      "epoch:48,batch:41 loss:1.1178871393203735\n",
      "epoch:48,batch:42 loss:1.0999183654785156\n",
      "epoch:48,batch:43 loss:1.104605793952942\n",
      "epoch:48,batch:44 loss:1.124137043952942\n",
      "epoch:48,batch:45 loss:1.1077308654785156\n",
      "epoch:48,batch:46 loss:1.086637020111084\n",
      "epoch:48,batch:47 loss:1.0991370677947998\n",
      "epoch:48,batch:48 loss:1.0756995677947998\n",
      "epoch:48,batch:49 loss:1.0842933654785156\n",
      "epoch:48,batch:50 loss:1.0952308177947998\n",
      "epoch:48,batch:51 loss:1.0842933654785156\n",
      "epoch:48,batch:52 loss:1.0866371393203735\n",
      "epoch:48,batch:53 loss:1.1053870916366577\n",
      "epoch:48,batch:54 loss:1.0999183654785156\n",
      "epoch:48,batch:55 loss:1.0874183177947998\n",
      "epoch:48,batch:56 loss:1.1116371154785156\n",
      "epoch:48,batch:57 loss:1.102262020111084\n",
      "epoch:48,batch:58 loss:1.1210120916366577\n",
      "epoch:48,batch:59 loss:1.0881996154785156\n",
      "epoch:48,batch:60 loss:1.1022621393203735\n",
      "epoch:48,batch:61 loss:1.0881996154785156\n",
      "epoch:48,batch:62 loss:1.078824520111084\n",
      "epoch:48,batch:63 loss:1.088199496269226\n",
      "epoch:48,batch:64 loss:1.0717933177947998\n",
      "epoch:48,batch:65 loss:1.1264808177947998\n",
      "epoch:48,batch:66 loss:1.0975745916366577\n",
      "epoch:48,batch:67 loss:1.0850746631622314\n",
      "epoch:48,batch:68 loss:1.0827308893203735\n",
      "epoch:48,batch:69 loss:1.078824520111084\n",
      "epoch:48,batch:70 loss:1.1022621393203735\n",
      "epoch:48,batch:71 loss:1.0913244485855103\n",
      "epoch:48,batch:72 loss:1.102262020111084\n",
      "epoch:48,batch:73 loss:1.1131995916366577\n",
      "epoch:48,batch:74 loss:1.094449520111084\n",
      "epoch:48,batch:75 loss:1.1124184131622314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [11:02<00:13, 13.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48,batch:76 loss:1.1061683893203735\n",
      "epoch:48,batch:77 loss:1.1116371154785156\n",
      "epoch:48, train_loss:1.1116371154785156\n",
      "epoch:49,batch:0 loss:1.0952308177947998\n",
      "epoch:49,batch:1 loss:1.1014808416366577\n",
      "epoch:49,batch:2 loss:1.1249183416366577\n",
      "epoch:49,batch:3 loss:1.0678871870040894\n",
      "epoch:49,batch:4 loss:1.0975745916366577\n",
      "epoch:49,batch:5 loss:1.0944496393203735\n",
      "epoch:49,batch:6 loss:1.0881996154785156\n",
      "epoch:49,batch:7 loss:1.0991370677947998\n",
      "epoch:49,batch:8 loss:1.0788246393203735\n",
      "epoch:49,batch:9 loss:1.108512043952942\n",
      "epoch:49,batch:10 loss:1.1124184131622314\n",
      "epoch:49,batch:11 loss:1.1077308654785156\n",
      "epoch:49,batch:12 loss:1.0944496393203735\n",
      "epoch:49,batch:13 loss:1.1124184131622314\n",
      "epoch:49,batch:14 loss:1.0975747108459473\n",
      "epoch:49,batch:15 loss:1.103824496269226\n",
      "epoch:49,batch:16 loss:1.1178871393203735\n",
      "epoch:49,batch:17 loss:1.082730770111084\n",
      "epoch:49,batch:18 loss:1.1108558177947998\n",
      "epoch:49,batch:19 loss:1.096793293952942\n",
      "epoch:49,batch:20 loss:1.100699543952942\n",
      "epoch:49,batch:21 loss:1.0842933654785156\n",
      "epoch:49,batch:22 loss:1.0694496631622314\n",
      "epoch:49,batch:23 loss:1.1092933416366577\n",
      "epoch:49,batch:24 loss:1.1186683177947998\n",
      "epoch:49,batch:25 loss:1.099918246269226\n",
      "epoch:49,batch:26 loss:1.1288247108459473\n",
      "epoch:49,batch:27 loss:1.1124184131622314\n",
      "epoch:49,batch:28 loss:1.0881996154785156\n",
      "epoch:49,batch:29 loss:1.0960121154785156\n",
      "epoch:49,batch:30 loss:1.1155433654785156\n",
      "epoch:49,batch:31 loss:1.0897622108459473\n",
      "epoch:49,batch:32 loss:1.0842933654785156\n",
      "epoch:49,batch:33 loss:1.120230793952942\n",
      "epoch:49,batch:34 loss:1.0983558893203735\n",
      "epoch:49,batch:35 loss:1.125699520111084\n",
      "epoch:49,batch:36 loss:1.1077308654785156\n",
      "epoch:49,batch:37 loss:1.0921058654785156\n",
      "epoch:49,batch:38 loss:1.1053870916366577\n",
      "epoch:49,batch:39 loss:1.082730770111084\n",
      "epoch:49,batch:40 loss:1.1131995916366577\n",
      "epoch:49,batch:41 loss:1.110074520111084\n",
      "epoch:49,batch:42 loss:1.0952308177947998\n",
      "epoch:49,batch:43 loss:1.092105746269226\n",
      "epoch:49,batch:44 loss:1.0913244485855103\n",
      "epoch:49,batch:45 loss:1.0999183654785156\n",
      "epoch:49,batch:46 loss:1.0960121154785156\n",
      "epoch:49,batch:47 loss:1.1139808893203735\n",
      "epoch:49,batch:48 loss:1.1069494485855103\n",
      "epoch:49,batch:49 loss:1.098355770111084\n",
      "epoch:49,batch:50 loss:1.1053869724273682\n",
      "epoch:49,batch:51 loss:1.088199496269226\n",
      "epoch:49,batch:52 loss:1.098355770111084\n",
      "epoch:49,batch:53 loss:1.1155433654785156\n",
      "epoch:49,batch:54 loss:1.1038246154785156\n",
      "epoch:49,batch:55 loss:1.0921058654785156\n",
      "epoch:49,batch:56 loss:1.0897620916366577\n",
      "epoch:49,batch:57 loss:1.0913245677947998\n",
      "epoch:49,batch:58 loss:1.1022621393203735\n",
      "epoch:49,batch:59 loss:1.086637020111084\n",
      "epoch:49,batch:60 loss:1.073355793952942\n",
      "epoch:49,batch:61 loss:1.092887043952942\n",
      "epoch:49,batch:62 loss:1.1272621154785156\n",
      "epoch:49,batch:63 loss:1.1171058416366577\n",
      "epoch:49,batch:64 loss:1.1100746393203735\n",
      "epoch:49,batch:65 loss:1.1131995916366577\n",
      "epoch:49,batch:66 loss:1.090543270111084\n",
      "epoch:49,batch:67 loss:1.0960121154785156\n",
      "epoch:49,batch:68 loss:1.1069495677947998\n",
      "epoch:49,batch:69 loss:1.1116371154785156\n",
      "epoch:49,batch:70 loss:1.0975745916366577\n",
      "epoch:49,batch:71 loss:1.0858557224273682\n",
      "epoch:49,batch:72 loss:1.1053872108459473\n",
      "epoch:49,batch:73 loss:1.0975745916366577\n",
      "epoch:49,batch:74 loss:1.0897620916366577\n",
      "epoch:49,batch:75 loss:1.0928871631622314\n",
      "epoch:49,batch:76 loss:1.1077308654785156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:15<00:00, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49,batch:77 loss:1.1053870916366577\n",
      "epoch:49, train_loss:1.1053870916366577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建优化器\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=0.005, parameters=model.parameters())\n",
    "\n",
    "# 模型训练\n",
    "for current_epoch in tqdm(range(50)):\n",
    "    for batch_id, data in enumerate(train_reader()): \n",
    "        # 前向传播\n",
    "        data_x = data[0]\n",
    "        data_y = data[1]\n",
    "        predict = model(data_x)\n",
    "        # 放到tensor中\n",
    "        data_y = paddle.to_tensor(data_y.numpy(), dtype='int64')\n",
    "        # 交叉信息熵损失\n",
    "        avg_loss = F.cross_entropy(predict,data_y)\n",
    "        #print('predict=', predict)\n",
    "        #break\n",
    "        y_np = np.array(data_y.numpy()).reshape(-1, 1)\n",
    "        # 将结果转换为one_hot形式\n",
    "        y_onehot = onehot_encoder.transform(y_np)\n",
    "        # 得到avg_score\n",
    "        avg_score = mae_loss(predict.numpy(), y_onehot)\n",
    "        # 反向传播\n",
    "        avg_loss.backward()       \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        print(\"epoch:{},batch:{} loss:{}\".format(current_epoch, batch_id, np.mean(avg_loss.numpy())))\n",
    "\n",
    "    print(\"epoch:{}, train_loss:{}\".format(current_epoch, avg_loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:56:57.112955Z",
     "iopub.status.busy": "2022-04-15T06:56:57.112502Z",
     "iopub.status.idle": "2022-04-15T06:56:57.129499Z",
     "shell.execute_reply": "2022-04-15T06:56:57.128785Z",
     "shell.execute_reply.started": "2022-04-15T06:56:57.112926Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1053870916366577\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "paddle.save(model.state_dict(),'work/model/cnn.model')\n",
    "print(\"Loss: {}\".format(avg_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:56:57.130972Z",
     "iopub.status.busy": "2022-04-15T06:56:57.130540Z",
     "iopub.status.idle": "2022-04-15T06:56:57.149506Z",
     "shell.execute_reply": "2022-04-15T06:56:57.148856Z",
     "shell.execute_reply.started": "2022-04-15T06:56:57.130914Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 模型参数加载\n",
    "param_dict = paddle.load('work/model/cnn.model')\n",
    "model = CNNLSTMModel()\n",
    "# 加载参数\n",
    "model.load_dict(param_dict)   \n",
    "# 预测模式\n",
    "model.eval()   \n",
    "\n",
    "# 定义DataLoader\n",
    "data_reader = paddle.io.DataLoader(test_dataset,\n",
    "                        batch_size=1280,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:56:57.150787Z",
     "iopub.status.busy": "2022-04-15T06:56:57.150395Z",
     "iopub.status.idle": "2022-04-15T06:56:58.066767Z",
     "shell.execute_reply": "2022-04-15T06:56:58.066171Z",
     "shell.execute_reply.started": "2022-04-15T06:56:57.150760Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, data in enumerate(data_reader()):\n",
    "    x = data[0]\n",
    "    out = model(x)\n",
    "    if index == 0:\n",
    "        y_pred = out.numpy()\n",
    "    else:\n",
    "        y_pred = np.vstack((y_pred, out.numpy()))\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:56:58.068380Z",
     "iopub.status.busy": "2022-04-15T06:56:58.067696Z",
     "iopub.status.idle": "2022-04-15T06:56:58.186629Z",
     "shell.execute_reply": "2022-04-15T06:56:58.186004Z",
     "shell.execute_reply.started": "2022-04-15T06:56:58.068355Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>119995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>119996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>119997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>119998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>119999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label_0  label_1  label_2  label_3\n",
       "0      100000      1.0      0.0      0.0      0.0\n",
       "1      100001      1.0      0.0      0.0      0.0\n",
       "2      100002      1.0      0.0      0.0      0.0\n",
       "3      100003      1.0      0.0      0.0      0.0\n",
       "4      100004      1.0      0.0      0.0      0.0\n",
       "...       ...      ...      ...      ...      ...\n",
       "19995  119995      1.0      0.0      0.0      0.0\n",
       "19996  119996      1.0      0.0      0.0      0.0\n",
       "19997  119997      1.0      0.0      0.0      0.0\n",
       "19998  119998      1.0      0.0      0.0      0.0\n",
       "19999  119999      1.0      0.0      0.0      0.0\n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=test1[['id']]\n",
    "result['label_0'] = y_pred[:,0]\n",
    "result['label_1'] = y_pred[:, 1]\n",
    "result['label_2'] = y_pred[:, 2]\n",
    "result['label_3'] = y_pred[:, 3]\n",
    "result.to_csv('./predict5-paddle(CNN+LSTM)-aistudio.csv',index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:56:58.187951Z",
     "iopub.status.busy": "2022-04-15T06:56:58.187644Z",
     "iopub.status.idle": "2022-04-15T06:56:58.191113Z",
     "shell.execute_reply": "2022-04-15T06:56:58.190543Z",
     "shell.execute_reply.started": "2022-04-15T06:56:58.187923Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# score:14202.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
